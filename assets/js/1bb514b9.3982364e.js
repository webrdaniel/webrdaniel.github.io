"use strict";(self.webpackChunk=self.webpackChunk||[]).push([["33415"],{91903:function(e,t,n){n.r(t),n.d(t,{default:()=>h,frontMatter:()=>l,metadata:()=>r,assets:()=>c,toc:()=>u,contentTitle:()=>a});var r=JSON.parse('{"id":"experiments/experiments-request-locking","title":"Request Locking","description":"Parallelize crawlers with ease using request locking","source":"@site/../docs/experiments/request_locking.mdx","sourceDirName":"experiments","slug":"/experiments/experiments-request-locking","permalink":"/docs/next/experiments/experiments-request-locking","draft":false,"unlisted":false,"editUrl":"https://github.com/apify/crawlee/edit/master/website/../docs/experiments/request_locking.mdx","tags":[],"version":"current","lastUpdatedBy":"Vlad Frangu","lastUpdatedAt":1715677482000,"frontMatter":{"id":"experiments-request-locking","title":"Request Locking","description":"Parallelize crawlers with ease using request locking"},"sidebar":"docs","previous":{"title":"Experiments","permalink":"/docs/next/experiments"},"next":{"title":"System Infomation V2","permalink":"/docs/next/experiments/experiments-system-infomation-v2"}}'),s=n("85893"),i=n("50065"),o=n("47927");let l={id:"experiments-request-locking",title:"Request Locking",description:"Parallelize crawlers with ease using request locking"},a=void 0,c={},u=[{value:"How to enable the experiment",id:"how-to-enable-the-experiment",level:2},{value:"In crawlers",id:"in-crawlers",level:3},{value:"Outside crawlers (to setup your own request queue that supports locking)",id:"outside-crawlers-to-setup-your-own-request-queue-that-supports-locking",level:3},{value:"Using the new request queue in crawlers",id:"using-the-new-request-queue-in-crawlers",level:3},{value:"Other changes",id:"other-changes",level:2}];function d(e){let t={a:"a",admonition:"admonition",code:"code",h2:"h2",h3:"h3",hr:"hr",li:"li",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,i.a)(),...e.components};return(0,s.jsxs)(s.Fragment,{children:[(0,s.jsxs)(t.admonition,{title:"Release announcement",type:"tip",children:[(0,s.jsxs)(t.p,{children:["As of ",(0,s.jsx)(t.strong,{children:"May 2024"})," (",(0,s.jsx)(t.code,{children:"crawlee"})," version ",(0,s.jsx)(t.code,{children:"3.10.0"}),"), this experiment is now enabled by default! With that said, if you encounter issues you can:"]}),(0,s.jsxs)(t.ul,{children:["\n",(0,s.jsxs)(t.li,{children:["set ",(0,s.jsx)(t.code,{children:"requestLocking"})," to ",(0,s.jsx)(t.code,{children:"false"})," in the ",(0,s.jsx)(t.code,{children:"experiments"})," object of your crawler options"]}),"\n",(0,s.jsxs)(t.li,{children:["update all imports of ",(0,s.jsx)(t.code,{children:"RequestQueue"})," to ",(0,s.jsx)(t.code,{children:"RequestQueueV1"})]}),"\n",(0,s.jsxs)(t.li,{children:["open an issue on our ",(0,s.jsx)(t.a,{href:"https://github.com/apify/crawlee",target:"_blank",rel:"noopener",children:"GitHub repository"})]}),"\n"]}),(0,s.jsxs)(t.p,{children:["The content below is kept for documentation purposes.\nIf you're interested in the changes, you can read the ",(0,s.jsx)(t.a,{href:"https://blog.apify.com/new-apify-request-queue/",target:"_blank",rel:"noopener",children:"blog post about the new Request Queue storage system on the Apify blog"}),"."]})]}),"\n",(0,s.jsx)(t.hr,{}),"\n",(0,s.jsxs)(t.admonition,{type:"caution",children:[(0,s.jsx)(t.p,{children:"This is an experimental feature. While we welcome testers, keep in mind that it is currently not recommended to use this in production."}),(0,s.jsx)(t.p,{children:"The API is subject to change, and we might introduce breaking changes in the future."}),(0,s.jsxs)(t.p,{children:["Should you be using this, feel free to open issues on our ",(0,s.jsx)(t.a,{href:"https://github.com/apify/crawlee",target:"_blank",rel:"noopener",children:"GitHub repository"}),", and we'll take a look."]})]}),"\n",(0,s.jsxs)(t.p,{children:["Starting with ",(0,s.jsx)(t.code,{children:"crawlee"})," version ",(0,s.jsx)(t.code,{children:"3.5.5"}),", we have introduced a new crawler option that lets you enable using a new request locking\nAPI. With this API, you will be able to pass a ",(0,s.jsx)(t.code,{children:"RequestQueue"})," to multiple crawlers to parallelize the crawling process."]}),"\n",(0,s.jsx)(t.admonition,{title:"Keep in mind",type:"info",children:(0,s.jsxs)(t.p,{children:["The request queue that supports request locking is currently exported via the ",(0,s.jsx)(t.code,{children:"RequestQueueV2"})," class. Once the experiment is over, this class will replace\nthe current ",(0,s.jsx)(t.code,{children:"RequestQueue"})," class"]})}),"\n",(0,s.jsx)(t.h2,{id:"how-to-enable-the-experiment",children:"How to enable the experiment"}),"\n",(0,s.jsx)(t.h3,{id:"in-crawlers",children:"In crawlers"}),"\n",(0,s.jsx)(t.admonition,{type:"note",children:(0,s.jsxs)(t.p,{children:["This example shows how to enable the experiment in the ",(0,s.jsx)(o.Z,{to:"cheerio-crawler/class/CheerioCrawler",children:(0,s.jsx)(t.code,{children:"CheerioCrawler"})}),",\nbut you can apply this to any crawler type."]})}),"\n",(0,s.jsx)(t.pre,{children:(0,s.jsx)(t.code,{className:"language-ts",children:"import { CheerioCrawler } from 'crawlee';\n\nconst crawler = new CheerioCrawler({\n    // highlight-next-line\n    experiments: {\n        // highlight-next-line\n        requestLocking: true,\n        // highlight-next-line\n    },\n    async requestHandler({ $, request }) {\n        const title = $('title').text();\n        console.log(`The title of \"${request.url}\" is: ${title}.`);\n    },\n});\n\nawait crawler.run(['https://crawlee.dev']);\n"})}),"\n",(0,s.jsx)(t.h3,{id:"outside-crawlers-to-setup-your-own-request-queue-that-supports-locking",children:"Outside crawlers (to setup your own request queue that supports locking)"}),"\n",(0,s.jsxs)(t.p,{children:["Previously, you would import ",(0,s.jsx)(t.code,{children:"RequestQueue"})," from ",(0,s.jsx)(t.code,{children:"crawlee"}),". To switch to the queue that supports locking, you need to import ",(0,s.jsx)(t.code,{children:"RequestQueueV2"})," instead."]}),"\n",(0,s.jsx)(t.pre,{children:(0,s.jsx)(t.code,{className:"language-ts",children:"// highlight-next-line\nimport { RequestQueueV2 } from 'crawlee';\n\nconst queue = await RequestQueueV2.open('my-locking-queue');\nawait queue.addRequests([\n    { url: 'https://crawlee.dev' },\n    { url: 'https://crawlee.dev/docs' },\n    { url: 'https://crawlee.dev/api' },\n]);\n"})}),"\n",(0,s.jsx)(t.h3,{id:"using-the-new-request-queue-in-crawlers",children:"Using the new request queue in crawlers"}),"\n",(0,s.jsx)(t.p,{children:"If you make your own request queue that supports locking, you will also need to enable the experiment in your crawlers."}),"\n",(0,s.jsx)(t.admonition,{type:"danger",children:(0,s.jsx)(t.p,{children:"If you do not enable the experiment, you will receive a runtime error and the crawler will not start."})}),"\n",(0,s.jsx)(t.pre,{children:(0,s.jsx)(t.code,{className:"language-ts",children:"import { CheerioCrawler, RequestQueueV2 } from 'crawlee';\n\n// highlight-next-line\nconst queue = await RequestQueueV2.open('my-locking-queue');\n\nconst crawler = new CheerioCrawler({\n    // highlight-next-line\n    experiments: {\n        // highlight-next-line\n        requestLocking: true,\n        // highlight-next-line\n    },\n    // highlight-next-line\n    requestQueue: queue,\n    async requestHandler({ $, request }) {\n        const title = $('title').text();\n        console.log(`The title of \"${request.url}\" is: ${title}.`);\n    },\n});\n\nawait crawler.run();\n"})}),"\n",(0,s.jsx)(t.h2,{id:"other-changes",children:"Other changes"}),"\n",(0,s.jsx)(t.admonition,{type:"info",children:(0,s.jsx)(t.p,{children:"This section is only useful if you're a tinkerer and want to see what's going on under the hood."})}),"\n",(0,s.jsxs)(t.p,{children:["In order to facilitate the new request locking API, as well as keep both the current request queue logic and the new, locking based request queue\nlogic, we have implemented a common starting point called ",(0,s.jsx)(t.code,{children:"RequestProvider"}),"."]}),"\n",(0,s.jsxs)(t.p,{children:["This class implements almost all functions by default, but expects you, the developer, to implement the following methods:\n",(0,s.jsx)(t.code,{children:"fetchNextRequest"})," and ",(0,s.jsx)(t.code,{children:"ensureHeadIsNotEmpty"}),". These methods are responsible for loading and returning requests to process,\nand tell crawlers if there are more requests to process."]}),"\n",(0,s.jsx)(t.p,{children:"You can use this base class to implement your own request providers if you need to fetch requests from a different source."}),"\n",(0,s.jsx)(t.admonition,{type:"tip",children:(0,s.jsx)(t.p,{children:"We recommend you use TypeScript when implementing your own request provider, as it comes with suggestions for the abstract methods, as well as\ngiving you the exact types you need to return."})})]})}function h(e={}){let{wrapper:t}={...(0,i.a)(),...e.components};return t?(0,s.jsx)(t,{...e,children:(0,s.jsx)(d,{...e})}):d(e)}},47927:function(e,t,n){n.d(t,{Z:function(){return u}});var r=n(85893);n(67294);var s=n(53367),i=n(89873),o=n(87262);let[l,a]=n(99074).version.split("."),c=[l,a].join("."),u=e=>{let{to:t,children:n}=e,l=(0,i.E)(),{siteConfig:a}=(0,o.default)();return a.presets[0][1].docs.disableVersioning||l.version===c?(0,r.jsx)(s.default,{to:`/api/${t}`,children:n}):(0,r.jsx)(s.default,{to:`/api/${"current"===l.version?"next":l.version}/${t}`,children:n})}},50065:function(e,t,n){n.d(t,{Z:function(){return l},a:function(){return o}});var r=n(67294);let s={},i=r.createContext(s);function o(e){let t=r.useContext(i);return r.useMemo(function(){return"function"==typeof e?e(t):{...t,...e}},[t,e])}function l(e){let t;return t=e.disableParentContext?"function"==typeof e.components?e.components(s):e.components||s:o(e.components),r.createElement(i.Provider,{value:t},e.children)}},99074:function(e){e.exports=JSON.parse('{"name":"crawlee","version":"3.13.0","description":"The scalable web crawling and scraping library for JavaScript/Node.js. Enables development of data extraction and web automation jobs (not only) with headless Chrome and Puppeteer.","engines":{"node":">=16.0.0"},"bin":"./src/cli.ts","main":"./dist/index.js","module":"./dist/index.mjs","types":"./dist/index.d.ts","exports":{".":{"import":"./dist/index.mjs","require":"./dist/index.js","types":"./dist/index.d.ts"},"./package.json":"./package.json"},"keywords":["apify","headless","chrome","puppeteer","crawler","scraper"],"author":{"name":"Apify","email":"support@apify.com","url":"https://apify.com"},"contributors":["Jan Curn <jan@apify.com>","Marek Trunkat <marek@apify.com>","Ondra Urban <ondra@apify.com>"],"license":"Apache-2.0","repository":{"type":"git","url":"git+https://github.com/apify/crawlee"},"bugs":{"url":"https://github.com/apify/crawlee/issues"},"homepage":"https://crawlee.dev","scripts":{"build":"yarn clean && yarn compile && yarn copy","clean":"rimraf ./dist","compile":"tsc -p tsconfig.build.json && gen-esm-wrapper ./dist/index.js ./dist/index.mjs","copy":"tsx ../../scripts/copy.ts"},"publishConfig":{"access":"public"},"dependencies":{"@crawlee/basic":"3.13.0","@crawlee/browser":"3.13.0","@crawlee/browser-pool":"3.13.0","@crawlee/cheerio":"3.13.0","@crawlee/cli":"3.13.0","@crawlee/core":"3.13.0","@crawlee/http":"3.13.0","@crawlee/jsdom":"3.13.0","@crawlee/linkedom":"3.13.0","@crawlee/playwright":"3.13.0","@crawlee/puppeteer":"3.13.0","@crawlee/utils":"3.13.0","import-local":"^3.1.0","tslib":"^2.4.0"},"peerDependencies":{"playwright":"*","puppeteer":"*"},"peerDependenciesMeta":{"playwright":{"optional":true},"puppeteer":{"optional":true}}}')}}]);