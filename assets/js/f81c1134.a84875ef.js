"use strict";(self.webpackChunk=self.webpackChunk||[]).push([["35524"],{5865:function(e){e.exports=JSON.parse('{"archive":{"blogPosts":[{"id":"superscraper-with-crawlee","metadata":{"permalink":"/blog/superscraper-with-crawlee","source":"@site/blog/2025/03-05-superscraper/index.md","title":"Inside implementing SuperScraper with Crawlee","description":"This article explains how SuperScraper works, highlights its implementation details, and provides code snippets to demonstrate its core functionality.","date":"2025-03-05T00:00:00.000Z","tags":[],"readingTime":5.11,"hasTruncateMarker":true,"authors":[{"name":"Saurav Jain","title":"Developer Community Manager","url":"https://github.com/souravjain540","socials":{"x":"https://x.com/sauain","github":"https://github.com/souravjain540"},"imageURL":"https://avatars.githubusercontent.com/u/53312820?v=4","key":"SauravJ","page":null},{"name":"Radoslav Chudovsk\xfd","title":"Web Automation Engineer","url":"https://github.com/chudovskyr","socials":{"github":"https://github.com/chudovskyr"},"imageURL":"https://ca.slack-edge.com/T0KRMEKK6-U04MGU11VUK-7f59c4a9343b-512","key":"RadoC","page":null}],"frontMatter":{"slug":"superscraper-with-crawlee","title":"Inside implementing SuperScraper with Crawlee","description":"This article explains how SuperScraper works, highlights its implementation details, and provides code snippets to demonstrate its core functionality.","image":"./img/superscraper.webp","authors":["SauravJ","RadoC"]},"unlisted":false,"nextItem":{"title":"Crawlee for Python v0.5","permalink":"/blog/crawlee-for-python-v05"}},"content":"[SuperScraper](https://github.com/apify/super-scraper) is an open-source [Actor](https://docs.apify.com/platform/actors) that combines features from various web scraping services, including [ScrapingBee](https://www.scrapingbee.com/), [ScrapingAnt](https://scrapingant.com/), and [ScraperAPI](https://www.scraperapi.com/). \\n\\nA key capability is its standby mode, which runs the Actor as a persistent API server. This removes the usual start-up times - a common pain point in many systems - and lets users make direct API calls to interact with the system immediately.\\n\\nThis blog explains how SuperScraper works, highlights its implementation details, and provides code snippets to demonstrate its core functionality.\\n\\n![Google Maps Data Screenshot](./img/superscraper.webp)\\n\\n\x3c!-- truncate --\x3e\\n\\n### What is SuperScraper?\\n\\nSuperScraper transforms a traditional scraper into an API server. Instead of running with static inputs and waiting for completion, it starts only once, stays active, and listens for incoming requests. \\n\\n### How to enable standby mode\\n\\nTo activate standby mode, you must configure the settings so it listens for incoming requests.\\n\\n![Activating Actor standby mode](./img/actor-standby.webp)\\n\\n### Server setup\\n\\nThe project uses Node.js `http` module to create a server that listens on the desired port. After the server starts, a check ensures users are interacting with it correctly by sending requests instead of running it traditionally. This keeps SuperScraper operating as a persistent server.\\n\\n### Handling multiple crawlers\\n\\nSuperScraper processes user requests using multiple instances of Crawlee\u2019s [`PlaywrightCrawler`](https://crawlee.dev/api/playwright-crawler/class/PlaywrightCrawler). Since each `PlaywrightCrawler` instance can only handle one proxy configuration, a separate crawler is created for each unique proxy setting. \\n\\nFor example, if the user sends one request for \u201Cnormal\u201D proxies and one request with residential US proxies, a separate crawler needs to be created for each proxy configuration. Hence, to solve this, we store the crawlers in a key-value map, where the key is a stringified proxy configuration.\\n\\n```ts\\nconst crawlers = new Map<string, PlaywrightCrawler>();\\n```\\n\\nHere\u2019s a part of the code that gets executed when a new request from the user arrives; if the crawler for this proxy configuration exists in the map, it will be used. Otherwise, a new crawler gets created. Then, we add the request to the crawler\u2019s queue so it can be processed.\\n\\n```ts\\nconst key = JSON.stringify(crawlerOptions); \\nconst crawler = crawlers.has(key) ? crawlers.get(key)! : await createAndStartCrawler(crawlerOptions);\\n\\nawait crawler.addRequests([request]);\\n```\\n\\nThe function below initializes new crawlers with predefined settings and behaviors. Each crawler utilizes its own in-memory queue created with the `MemoryStorage` client. This approach is used for two key reasons:\\n\\n1. **Performance**: In-memory queues are faster, and there\'s no need to persist them when SuperScraper migrates.\\n2. **Isolation**: Using a separate queue prevents interference with the shared default queue of the SuperScraper Actor, avoiding potential bugs when multiple crawlers use it simultaneously.\\n\\n```ts\\nexport const createAndStartCrawler = async (crawlerOptions: CrawlerOptions = DEFAULT_CRAWLER_OPTIONS) => {\\n    const client = new MemoryStorage({ persistStorage: false });\\n    const queue = await RequestQueue.open(undefined, { storageClient: client });\\n\\n    const proxyConfig = await Actor.createProxyConfiguration(crawlerOptions.proxyConfigurationOptions);\\n\\n    const crawler = new PlaywrightCrawler({\\n        keepAlive: true,\\n        proxyConfiguration: proxyConfig,\\n        maxRequestRetries: 4,\\n        requestQueue: queue,\\n    });\\n};\\n```\\n\\nAt the end of the function, we start the crawler and log a message if it terminates for any reason. Next, we add the newly created crawler to the key-value map containing all crawlers, and finally, we return the crawler.\\n\\n```ts\\ncrawler.run().then(\\n    () => log.warning(`Crawler ended`, crawlerOptions),\\n    () => { }\\n);\\n\\ncrawlers.set(JSON.stringify(crawlerOptions), crawler);\\n\\nlog.info(\'Crawler ready \uD83D\uDE80\', crawlerOptions);\\n\\nreturn crawler;\\n```\\n\\n### Mapping standby HTTP requests to Crawlee requests\\n\\nWhen creating the server, it accepts a request listener function that takes two arguments: the user\u2019s request and a response object. The response object is used to send scraped data back to the user. These response objects are stored in a key-value map to so they can be accessed later in the code. The key is a randomly generated string shared between the request and its corresponding response object, it is used as `request.uniqueKey`.\\n\\n```ts\\nconst responses = new Map<string, ServerResponse>();\\n```\\n\\n**Saving response objects**\\n\\nThe following function stores a response object in the key-value map:\\n\\n```ts\\nexport function addResponse(responseId: string, response: ServerResponse) {\\n    responses.set(responseId, response);\\n}\\n```\\n\\n**Updating crawler logic to store responses**\\n\\nHere\u2019s the updated logic for fetching/creating the corresponding crawler for a given proxy configuration, with a call to store the response object:\\n\\n```ts\\nconst key = JSON.stringify(crawlerOptions); \\nconst crawler = crawlers.has(key) ? crawlers.get(key)! : await createAndStartCrawler(crawlerOptions);\\n\\naddResponse(request.uniqueKey!, res);\\n\\nawait crawler.requestQueue!.addRequest(request);\\n```\\n\\n**Sending scraped data back**\\n\\nOnce a crawler finishes processing a request, it retrieves the corresponding response object using the key and sends the scraped data back to the user:\\n\\n```ts\\nexport const sendSuccResponseById = (responseId: string, result: unknown, contentType: string) => {\\n    const res = responses.get(responseId);\\n    if (!res) {\\n        log.info(`Response for request ${responseId} not found`);\\n        return;\\n    }\\n\\n    res.writeHead(200, { \'Content-Type\': contentType });\\n    res.end(result);\\n    responses.delete(responseId);\\n};\\n```\\n\\n**Error handling**\\n\\nThere is similar logic to send a response back if an error occurs during scraping:\\n\\n```ts\\nexport const sendErrorResponseById = (responseId: string, result: string, statusCode: number = 500) => {\\n    const res = responses.get(responseId);\\n    if (!res) {\\n        log.info(`Response for request ${responseId} not found`);\\n        return;\\n    }\\n\\n    res.writeHead(statusCode, { \'Content-Type\': \'application/json\' });\\n    res.end(result);\\n    responses.delete(responseId);\\n};\\n```\\n\\n**Adding timeouts during migrations**\\n\\nDuring migration, SuperScraper adds timeouts to pending responses to handle termination cleanly.\\n\\n```ts\\nexport const addTimeoutToAllResponses = (timeoutInSeconds: number = 60) => {\\n    const migrationErrorMessage = {\\n        errorMessage: \'Actor had to migrate to another server. Please, retry your request.\',\\n    };\\n\\n    const responseKeys = Object.keys(responses);\\n\\n    for (const key of responseKeys) {\\n        setTimeout(() => {\\n            sendErrorResponseById(key, JSON.stringify(migrationErrorMessage));\\n        }, timeoutInSeconds * 1000);\\n    }\\n};\\n```\\n\\n### Managing migrations\\n\\nSuperScraper handles migrations by timing out active responses to prevent lingering requests during server transitions.\\n\\n```ts\\nActor.on(\'migrating\', ()=>{\\n    addTimeoutToAllResponses(60);\\n});\\n```\\n\\nUsers receive clear feedback during server migrations, maintaining stable operation.\\n\\n### Build your own\\n\\nThis guide showed how to build and manage a standby web scraper using Apify\u2019s platform and Crawlee. The implementation handles multiple proxy configurations through `PlaywrightCrawler` instances while managing request-response cycles efficiently to support diverse scraping needs.\\n\\nStandby mode transforms SuperScraper into a persistent API server, eliminating start-up delays. The migration handling system keeps operations stable during server transitions. You can build on this foundation to create web scraping tools tailored to your requirements.\\n\\nTo get started, explore the project on [GitHub](https://github.com/apify/super-scraper) or learn more about [Crawlee](https://crawlee.dev/) to build your own scalable web scraping tools."},{"id":"crawlee-for-python-v05","metadata":{"permalink":"/blog/crawlee-for-python-v05","source":"@site/blog/2025/01-10/index.md","title":"Crawlee for Python v0.5","description":"Announcing the Crawlee for Python v0.5 release.","date":"2025-01-10T00:00:00.000Z","tags":[],"readingTime":5.53,"hasTruncateMarker":true,"authors":[{"name":"Vlada Dusek","title":"Developer of Crawlee for Python","url":"https://github.com/vdusek","socials":{"github":"https://github.com/vdusek"},"imageURL":"https://avatars.githubusercontent.com/u/25082181?v=4","key":"VladaD","page":null}],"frontMatter":{"slug":"crawlee-for-python-v05","title":"Crawlee for Python v0.5","description":"Announcing the Crawlee for Python v0.5 release.","authors":["VladaD"]},"unlisted":false,"prevItem":{"title":"Inside implementing SuperScraper with Crawlee","permalink":"/blog/superscraper-with-crawlee"},"nextItem":{"title":"How to scrape Crunchbase using Python in 2024 (Easy Guide)","permalink":"/blog/scrape-crunchbase-python"}},"content":"Crawlee for Python v0.5 is now available! This is our biggest release to date, bringing new ported functionality from the [Crawlee for JavaScript](https://github.com/apify/crawlee), brand-new features that are exclusive to the Python library (for now), a new consolidated package structure, and a bunch of bug fixes and further improvements.\\n\\n\x3c!-- truncate --\x3e\\n\\n## Getting started\\n\\nYou can upgrade to the latest version straight from [PyPI](https://pypi.org/project/crawlee/):\\n\\n```shell\\npip install --upgrade crawlee\\n```\\n\\nCheck out the full changelog on our [website](https://www.crawlee.dev/python/docs/changelog#050-2025-01-02) to see all the details. If you are updating from an older version, make sure to follow our [Upgrading to v0.5](https://www.crawlee.dev/python/docs/upgrading/upgrading-to-v0x#upgrading-to-v05) guide for a smooth upgrade.\\n\\n## New package structure\\n\\nWe have introduced a new consolidated package structure. The goal is to streamline the development experience, help you find the crawlers you are looking for faster, and improve the IDE\'s code suggestions while importing.\\n\\n### Crawlers\\n\\nWe have grouped all crawler classes (and their corresponding crawling context classes) into a single sub-package called `crawlers`. Here is a quick example of how the imports have changed:\\n\\n```diff\\n- from crawlee.beautifulsoup_crawler import BeautifulSoupCrawler, BeautifulSoupCrawlingContext\\n+ from crawlee.crawlers import BeautifulSoupCrawler, BeautifulSoupCrawlingContext\\n```\\n\\nLook how you can see all the crawlers that we have, isn\'t that cool!\\n\\n![Import from crawlers subpackage.](./img/import_crawlers.webp)\\n\\n### Storage clients\\n\\nSimilarly, we have moved all storage client classes under `storage_clients` sub-package. For instance:\\n\\n```diff\\n- from crawlee.memory_storage_client import MemoryStorageClient\\n+ from crawlee.storage_clients import MemoryStorageClient\\n```\\n\\nThis consolidation makes it clearer where each class belongs and ensures that your IDE can provide better autocompletion when you are looking for the right crawler or storage client.\\n\\n## Continued parity with Crawlee JS\\n\\nWe are constantly working toward feature parity with our JavaScript library, [Crawlee JS](https://github.com/apify/crawlee). With v0.5, we have brought over more functionality:\\n\\n### HTML to text context helper\\n\\nThe `html_to_text` crawling context helper simplifies extracting text from an HTML page by automatically removing all tags and returning only the raw text content. It\'s available in the [`ParselCrawlingContext`](https://www.crawlee.dev/python/api/class/ParselCrawlingContext#html_to_text) and [`BeautifulSoupCrawlingContext`](https://www.crawlee.dev/python/api/class/BeautifulSoupCrawlingContext#html_to_text).\\n\\n```python\\nimport asyncio\\n\\nfrom crawlee.crawlers import ParselCrawler, ParselCrawlingContext\\n\\n\\nasync def main() -> None:\\n    crawler = ParselCrawler()\\n\\n    @crawler.router.default_handler\\n    async def handler(context: ParselCrawlingContext) -> None:\\n        context.log.info(\'Crawling: %s\', context.request.url)\\n        text = context.html_to_text()\\n        # Continue with the processing...\\n\\n    await crawler.run([\'https://crawlee.dev\'])\\n\\n\\nif __name__ == \'__main__\':\\n    asyncio.run(main())\\n```\\n\\nIn this example, we use a [`ParselCrawler`](https://www.crawlee.dev/python/api/class/ParselCrawler) to fetch a webpage, then invoke `context.html_to_text()` to extract clean text for further processing.\\n\\n### Use state\\n\\nThe [`use_state`](https://www.crawlee.dev/python/api/class/UseStateFunction) crawling context helper makes it simple to create and manage persistent state values within your crawler. It ensures that all state values are automatically persisted. It enables you to maintain data across different crawler runs, restarts, and failures. It acts as a convenient abstraction for interaction with [`KeyValueStore`](https://www.crawlee.dev/python/api/class/KeyValueStore).\\n\\n```python\\nimport asyncio\\n\\nfrom crawlee import Request\\nfrom crawlee.configuration import Configuration\\nfrom crawlee.crawlers import ParselCrawler, ParselCrawlingContext\\n\\n\\nasync def main() -> None:\\n    # Create a crawler with purge_on_start disabled to retain state across runs.\\n    crawler = ParselCrawler(\\n        configuration=Configuration(purge_on_start=False),\\n    )\\n\\n    @crawler.router.default_handler\\n    async def handler(context: ParselCrawlingContext) -> None:\\n        context.log.info(f\'Crawling {context.request.url}\')\\n\\n        # Retrieve or initialize the state with a default value.\\n        state = await context.use_state(\'state\', default_value={\'runs\': 0})\\n\\n        # Increment the run count.\\n        state[\'runs\'] += 1\\n\\n    # Create a request with always_enqueue enabled to bypass deduplication and ensure it is processed.\\n    request = Request.from_url(\'https://crawlee.dev/\', always_enqueue=True)\\n\\n    # Run the crawler with the start request.\\n    await crawler.run([request])\\n\\n    # Fetch the persisted state from the key-value store.\\n    kvs = await crawler.get_key_value_store()\\n    state = await kvs.get_auto_saved_value(\'state\')\\n    crawler.log.info(f\'Final state after run: {state}\')\\n\\n\\nif __name__ == \'__main__\':\\n    asyncio.run(main())\\n```\\n\\nPlease note that the `use_state` is an experimental feature. Its behavior and interface may evolve in future versions.\\n\\n## Brand new features\\n\\nIn addition to porting features from JS, we are introducing new, Python-first functionalities that will eventually make their way into Crawlee JS in the coming months.\\n\\n### Crawler\'s stop method\\n\\nThe [`BasicCrawler`](https://www.crawlee.dev/python/api/class/BasicCrawler), and by extension, all crawlers that inherit from it, now has a [`stop`](https://www.crawlee.dev/python/api/class/BasicCrawler#stop) method. This makes it easy to halt the crawling when a specific condition is met, for instance, if you have found the data you were looking for.\\n\\n```python\\nimport asyncio\\n\\nfrom crawlee.crawlers import ParselCrawler, ParselCrawlingContext\\n\\n\\nasync def main() -> None:\\n    crawler = ParselCrawler()\\n\\n    @crawler.router.default_handler\\n    async def handler(context: ParselCrawlingContext) -> None:\\n        context.log.info(\'Crawling: %s\', context.request.url)\\n\\n        # Extract and enqueue links from the page.\\n        await context.enqueue_links()\\n\\n        title = context.selector.css(\'title::text\').get()\\n\\n        # Condition when you want to stop the crawler, e.g. you\\n        # have found what you were looking for.\\n        if \'Crawlee for Python\' in title:\\n            context.log.info(\'Condition met, stopping the crawler.\')\\n            await crawler.stop()\\n\\n    await crawler.run([\'https://crawlee.dev\'])\\n\\n\\nif __name__ == \'__main__\':\\n    asyncio.run(main())\\n```\\n\\n### Request loaders\\n\\nThere are new classes [`RequestLoader`](https://www.crawlee.dev/python/api/class/RequestLoader), [`RequestManager`](https://www.crawlee.dev/python/api/class/RequestManager) and [`RequestManagerTandem`](https://www.crawlee.dev/python/api/class/RequestManagerTandem) that manage how Crawlee accesses and stores requests. They allow you to use other component (service) as a source for requests and optionally you can combine it with a [`RequestQueue`](https://www.crawlee.dev/python/api/class/RequestQueue). They let you plug in any request source, and combine the external data sources with Crawlee\'s standard `RequestQueue`.\\n\\nYou can learn more about these new features in the [Request loaders guide](https://www.crawlee.dev/python/docs/guides/request-loaders).\\n\\n```python\\nimport asyncio\\n\\nfrom crawlee.crawlers import ParselCrawler, ParselCrawlingContext\\nfrom crawlee.request_loaders import RequestList, RequestManagerTandem\\nfrom crawlee.storages import RequestQueue\\n\\n\\nasync def main() -> None:\\n    rl = RequestList(\\n        [\\n            \'https://crawlee.dev\',\\n            \'https://apify.com\',\\n            # Long list of URLs...\\n        ],\\n    )\\n\\n    rq = await RequestQueue.open()\\n\\n    # Combine them into a single request source.\\n    tandem = RequestManagerTandem(rl, rq)\\n\\n    crawler = ParselCrawler(request_manager=tandem)\\n\\n    @crawler.router.default_handler\\n    async def handler(context: ParselCrawlingContext) -> None:\\n        context.log.info(f\'Crawling {context.request.url}\')\\n        # ...\\n\\n    await crawler.run()\\n\\n\\nif __name__ == \'__main__\':\\n    asyncio.run(main())\\n```\\n\\nIn this example we combine a [`RequestList`](https://www.crawlee.dev/python/api/class/RequestList) with a [`RequestQueue`](https://www.crawlee.dev/python/api/class/RequestQueue). However, instead of the `RequestList` you can use any other class that implements the [`RequestLoader`](https://www.crawlee.dev/python/api/class/RequestLoader) interface to suit your specific requirements.\\n\\n### Service locator\\n\\nThe [`ServiceLocator`](https://www.crawlee.dev/python/api/class/ServiceLocator) is primarily an internal mechanism for managing the services that Crawlee depends on. Specifically, the [`Configuration`](https://www.crawlee.dev/python/api/class/ServiceLocator), [`StorageClient`](https://www.crawlee.dev/python/api/class/ServiceLocator), and [`EventManager`](https://www.crawlee.dev/python/api/class/ServiceLocator). By swapping out these components, you can adapt Crawlee to suit different runtime environments.\\n\\nYou can use the service locator explicitly:\\n\\n```python\\nimport asyncio\\n\\nfrom crawlee import service_locator\\nfrom crawlee.configuration import Configuration\\nfrom crawlee.crawlers import ParselCrawler, ParselCrawlingContext\\nfrom crawlee.events import LocalEventManager\\nfrom crawlee.storage_clients import MemoryStorageClient\\n\\n\\nasync def main() -> None:\\n    service_locator.set_configuration(Configuration())\\n    service_locator.set_storage_client(MemoryStorageClient())\\n    service_locator.set_event_manager(LocalEventManager())\\n\\n    crawler = ParselCrawler()\\n\\n    # ...\\n\\n\\nif __name__ == \'__main__\':\\n    asyncio.run(main())\\n```\\n\\nOr pass the services directly to the crawler instance, and they will be set under the hood:\\n\\n```python\\nimport asyncio\\n\\nfrom crawlee.configuration import Configuration\\nfrom crawlee.crawlers import ParselCrawler, ParselCrawlingContext\\nfrom crawlee.events import LocalEventManager\\nfrom crawlee.storage_clients import MemoryStorageClient\\n\\n\\nasync def main() -> None:\\n    crawler = ParselCrawler(\\n        configuration=Configuration(),\\n        storage_client=MemoryStorageClient(),\\n        event_manager=LocalEventManager(),\\n    )\\n\\n    # ...\\n\\n\\nif __name__ == \'__main__\':\\n    asyncio.run(main())\\n```\\n\\n## Conclusion\\n\\nWe are excited to share that Crawlee v0.5 is here. If you have any questions or feedback, please open a [GitHub discussion](https://github.com/apify/crawlee-python/discussions). If you encounter any bugs, or have an idea for a new feature, please open a [GitHub issue](https://github.com/apify/crawlee-python/issues)."},{"id":"scrape-crunchbase-python","metadata":{"permalink":"/blog/scrape-crunchbase-python","source":"@site/blog/2025/01-03-scrape-crunchbase/index.md","title":"How to scrape Crunchbase using Python in 2024 (Easy Guide)","description":"Learn how to scrape Crunchbase using Crawlee for Python","date":"2025-01-03T00:00:00.000Z","tags":[{"inline":true,"label":"community","permalink":"/blog/tags/community"}],"readingTime":10.79,"hasTruncateMarker":true,"authors":[{"name":"Max","title":"Community Member of Crawlee and web scraping expert","url":"https://github.com/Mantisus","socials":{"github":"https://github.com/Mantisus"},"imageURL":"https://avatars.githubusercontent.com/u/34358312?v=4","key":"MaxB","page":null}],"frontMatter":{"slug":"scrape-crunchbase-python","title":"How to scrape Crunchbase using Python in 2024 (Easy Guide)","tags":["community"],"description":"Learn how to scrape Crunchbase using Crawlee for Python","image":"./img/scrape_crunchbase.webp","authors":["MaxB"]},"unlisted":false,"prevItem":{"title":"Crawlee for Python v0.5","permalink":"/blog/crawlee-for-python-v05"},"nextItem":{"title":"How to scrape Google Maps data using Python","permalink":"/blog/scrape-google-maps"}},"content":"Python developers know the drill: you need reliable company data, and Crunchbase has it. This guide shows you how to build an effective [Crunchbase](https://www.crunchbase.com/) scraper in Python that gets you the data you need.\\n\\nCrunchbase tracks details that matter: locations, business focus, founders, and investment histories. Manual extraction from such a large dataset isn\'t practical -automation is essential for transforming this information into an analyzable format.\\n\\nBy the end of this blog, we\'ll explore three different ways to extract data from Crunchbase using [`Crawlee for Python`](https://github.com/apify/crawlee-python). We\'ll fully implement two of them and discuss the specifics and challenges of the third. This will help us better understand how important it is to properly [choose the right data source](https://www.crawlee.dev/blog/web-scraping-tips#1-choosing-a-data-source-for-the-project).\\n\\n:::note\\n\\nThis guide comes from a developer in our growing community. Have you built interesting projects with Crawlee? Join us on [Discord](https://discord.com/invite/jyEM2PRvMU) to share your experiences and blog ideas - we value these contributions from developers like you.\\n\\n:::\\n\\n![How to Scrape Crunchbase Using Python](./img/scrape_crunchbase.webp)\\n\\nKey steps we\'ll cover:\\n\\n1. Project setup\\n2. Choosing the data source\\n3. Implementing sitemap-based crawler\\n4. Analysis of search-based approach and its limitations\\n5. Implementing the official API crawler\\n6. Conclusion and repository access\\n\\n\x3c!-- truncate --\x3e\\n\\n## Prerequisites\\n\\n- Python 3.9 or higher\\n- Familiarity with web scraping concepts\\n- Crawlee for Python `v0.5.0`\\n- poetry `v2.0` or higher\\n\\n### Project setup\\n\\nBefore we start scraping, we need to set up our project. In this guide, we won\'t be using crawler templates (`Playwright` and `Beautifulsoup`), so we\'ll set up the project manually.\\n\\n1. Install [`Poetry`](https://python-poetry.org/)\\n\\n    ```bash\\n    pipx install poetry\\n    ```\\n\\n2. Create and navigate to the project folder.\\n\\n    ```bash\\n    mkdir crunchbase-crawlee && cd crunchbase-crawlee\\n    ```\\n\\n3. Initialize the project using Poetry, leaving all fields empty.\\n\\n    ```bash\\n    poetry init\\n    ```\\n\\n    When prompted:\\n    - For \\"Compatible Python versions\\", enter: `>={your Python version},<4.0`\\n    (For example, if you\'re using Python 3.10, enter: `>=3.10,<4.0`)\\n    - Leave all other fields empty by pressing Enter\\n    - Confirm the generation by typing \\"yes\\"\\n\\n4. Add and install Crawlee with necessary dependencies to your project using `Poetry.`\\n\\n    ```bash\\n    poetry add crawlee[parsel,curl-impersonate]\\n    ```\\n\\n5. Complete the project setup by creating the standard file structure for `Crawlee for Python` projects.\\n\\n    ```bash\\n    mkdir crunchbase-crawlee && touch crunchbase-crawlee/{__init__.py,__main__.py,main.py,routes.py}\\n    ```\\n\\nAfter setting up the basic project structure, we can explore different methods of obtaining data from Crunchbase.\\n\\n### Choosing the data source\\n\\nWhile we can extract target data directly from the [company page](https://www.crunchbase.com/organization/apify), we need to choose the best way to navigate the site.\\n\\nA careful examination of Crunchbase\'s structure shows that we have three main options for obtaining data:\\n\\n1. [`Sitemap`](https://www.crunchbase.com/www-sitemaps/sitemap-index.xml) - for complete site traversal.\\n2. [`Search`](https://www.crunchbase.com/discover/organization.companies) - for targeted data collection.\\n3. [Official API](https://data.crunchbase.com/v4-legacy/docs/crunchbase-basic-getting-started) - recommended method.\\n\\nLet\'s examine each of these approaches in detail.\\n\\n## Scraping Crunchbase using sitemap and Crawlee for Python\\n\\n`Sitemap` is a standard way of site navigation used by crawlers like [`Google`](https://google.com/), [`Ahrefs`](https://ahrefs.com/), and other search engines. All crawlers must follow the rules described in [`robots.txt`](https://www.crunchbase.com/robots.txt).\\n\\nLet\'s look at the structure of Crunchbase\'s Sitemap:\\n\\n![Sitemap first lvl](./img/sitemap_lvl_one.webp)\\n\\nAs you can see, links to organization pages are located inside second-level `Sitemap` files, which are compressed using `gzip`.\\n\\nThe structure of one of these files looks like this:\\n\\n![Sitemap second lvl](./img/sitemap_lvl_two.webp)\\n\\nThe `lastmod` field is particularly important here. It allows tracking which companies have updated their information since the previous data collection. This is especially useful for regular data updates.\\n\\n### 1. Configuring the crawler for scraping\\n\\nTo work with the site, we\'ll use [`CurlImpersonateHttpClient`](https://www.crawlee.dev/python/api/class/CurlImpersonateHttpClient), which impersonates a `Safari` browser. While this choice might seem unexpected for working with a sitemap, it\'s necessitated by Crunchbase\'s protection features.\\n\\nThe reason is that Crunchbase uses [Cloudflare](https://www.cloudflare.com/) to protect against automated access. This is clearly visible when analyzing traffic on a company page:\\n\\n![Cloudflare Link](./img/cloudflare_link.webp)\\n\\nAn interesting feature is that `challenges.cloudflare` is executed after loading the document with data. This means we receive the data first, and only then JavaScript checks if we\'re a bot. If our HTTP client\'s fingerprint is sufficiently similar to a real browser, we\'ll successfully receive the data.\\n\\nCloudflare [also analyzes traffic at the sitemap level](https://developers.cloudflare.com/waf/custom-rules/use-cases/allow-traffic-from-verified-bots/). If our crawler doesn\'t look legitimate, access will be blocked. That\'s why we impersonate a real browser.\\n\\nTo prevent blocks due to overly aggressive crawling, we\'ll configure [`ConcurrencySettings`](https://www.crawlee.dev/python/api/class/ConcurrencySettings).\\n\\nWhen scaling this approach, you\'ll likely need proxies. Detailed information about proxy setup can be found in the [documentation](https://www.crawlee.dev/python/docs/guides/proxy-management).\\n\\nWe\'ll save our scraping results in `JSON` format. Here\'s how the basic crawler configuration looks:\\n\\n```python\\n# main.py\\n\\nfrom crawlee import ConcurrencySettings, HttpHeaders\\nfrom crawlee.crawlers import ParselCrawler\\nfrom crawlee.http_clients import CurlImpersonateHttpClient\\n\\nfrom .routes import router\\n\\n\\nasync def main() -> None:\\n    \\"\\"\\"The crawler entry point.\\"\\"\\"\\n    concurrency_settings = ConcurrencySettings(max_concurrency=1, max_tasks_per_minute=50)\\n\\n    http_client = CurlImpersonateHttpClient(\\n        impersonate=\'safari17_0\',\\n        headers=HttpHeaders(\\n            {\\n                \'accept-language\': \'en\',\\n                \'accept-encoding\': \'gzip, deflate, br, zstd\',\\n            }\\n        ),\\n    )\\n    crawler = ParselCrawler(\\n        request_handler=router,\\n        max_request_retries=1,\\n        concurrency_settings=concurrency_settings,\\n        http_client=http_client,\\n        max_requests_per_crawl=30,\\n    )\\n\\n    await crawler.run([\'https://www.crunchbase.com/www-sitemaps/sitemap-index.xml\'])\\n\\n    await crawler.export_data_json(\'crunchbase_data.json\')\\n```\\n\\n### 2. Implementing sitemap navigation\\n\\nSitemap navigation happens in two stages. In the first stage, we need to get a list of all files containing organization information:\\n\\n```python\\n# routes.py\\n\\nfrom crawlee.crawlers import ParselCrawlingContext\\nfrom crawlee.router import Router\\nfrom crawlee import Request\\n\\nrouter = Router[ParselCrawlingContext]()\\n\\n\\n@router.default_handler\\nasync def default_handler(context: ParselCrawlingContext) -> None:\\n    \\"\\"\\"Default request handler.\\"\\"\\"\\n    context.log.info(f\'default_handler processing {context.request} ...\')\\n\\n    requests = [\\n        Request.from_url(url, label=\'sitemap\')\\n        for url in context.selector.xpath(\'//loc[contains(., \\"sitemap-organizations\\")]/text()\').getall()\\n    ]\\n\\n    # Since this is a tutorial, I don\'t want to upload more than one sitemap link\\n    await context.add_requests(requests, limit=1)\\n```\\n\\nIn the second stage, we process second-level sitemap files stored in `gzip` format. This requires a special approach as the data needs to be decompressed first:\\n\\n```python\\n# routes.py\\n\\nfrom gzip import decompress\\nfrom parsel import Selector\\n\\n\\n@router.handler(\'sitemap\')\\nasync def sitemap_handler(context: ParselCrawlingContext) -> None:\\n    \\"\\"\\"Sitemap gzip request handler.\\"\\"\\"\\n    context.log.info(f\'sitemap_handler processing {context.request.url} ...\')\\n\\n    data = context.http_response.read()\\n    data = decompress(data)\\n\\n    selector = Selector(data.decode())\\n\\n    requests = [Request.from_url(url, label=\'company\') for url in selector.xpath(\'//loc/text()\').getall()]\\n\\n    await context.add_requests(requests)\\n```\\n\\n### 3. Extracting and saving data\\n\\nEach company page contains a large amount of information. For demonstration purposes, we\'ll focus on the main fields: `Company Name`, `Short Description`, `Website`, and `Location`.\\n\\nOne of Crunchbase\'s advantages is that all data is stored in `JSON` format within the page:\\n\\n![Company Data](./img/data_json.webp)\\n\\nThis significantly simplifies data extraction - we only need to use one `Xpath` selector to get the `JSON`, and then apply [`jmespath`](https://jmespath.org/) to extract the needed fields:\\n\\n```python\\n# routes.py\\n\\n@router.handler(\'company\')\\nasync def company_handler(context: ParselCrawlingContext) -> None:\\n    \\"\\"\\"Company request handler.\\"\\"\\"\\n    context.log.info(f\'company_handler processing {context.request.url} ...\')\\n\\n    json_selector = context.selector.xpath(\'//*[@id=\\"ng-state\\"]/text()\')\\n\\n    await context.push_data(\\n        {\\n            \'Company Name\': json_selector.jmespath(\'HttpState.*.data[].properties.identifier.value\').get(),\\n            \'Short Description\': json_selector.jmespath(\'HttpState.*.data[].properties.short_description\').get(),\\n            \'Website\': json_selector.jmespath(\'HttpState.*.data[].cards.company_about_fields2.website.value\').get(),\\n            \'Location\': \'; \'.join(\\n                json_selector.jmespath(\\n                    \'HttpState.*.data[].cards.company_about_fields2.location_identifiers[].value\'\\n                ).getall()\\n            ),\\n        }\\n    )\\n```\\n\\nThe collected data is saved in `Crawlee for Python`\'s internal storage using the `context.push_data` method. When the crawler finishes, we export all collected data to a JSON file:\\n\\n```python\\n# main.py\\n\\nawait crawler.export_data_json(\'crunchbase_data.json\')\\n```\\n\\n### 4. Running the project\\n\\nWith all components in place, we need to create an entry point for our crawler:\\n\\n```python\\n# __main__.py\\nimport asyncio\\n\\nfrom .main import main\\n\\nif __name__ == \'__main__\':\\n    asyncio.run(main())\\n```\\n\\nExecute the crawler using Poetry:\\n\\n```bash\\npoetry run python -m crunchbase-crawlee\\n```\\n\\n### 5. Finally, characteristics of using the sitemap crawler\\n\\nThe sitemap approach has its distinct advantages and limitations. It\'s ideal in the following cases:\\n\\n- When you need to collect data about all companies on the platform\\n- When there are no specific company selection criteria\\n- If you have sufficient time and computational resources\\n\\nHowever, there are significant limitations to consider:\\n\\n- Almost no ability to filter data during collection\\n- Requires constant monitoring of Cloudflare blocks\\n- Scaling the solution requires proxy servers, which increases project costs\\n\\n## Using search for scraping Crunchbase\\n\\nThe limitations of the sitemap approach might point to search as the next solution. However, Crunchbase applies tighter security measures to its search functionality compared to its public pages.\\n\\nThe key difference lies in how Cloudflare protection works. While we receive data before the `challenges.cloudflare` check when accessing a company page, the search API requires valid `cookies` that have passed this check.\\n\\nLet\'s verify this in practice. Open the following link in Incognito mode:\\n\\n```plaintext\\n<https://www.crunchbase.com/v4/data/autocompletes?query=Ap&collection_ids=organizations&limit=25&source=topSearch>\\n```\\n\\nWhen analyzing the traffic, we\'ll see the following pattern:\\n\\n![Search Protect](./img/search_protect.webp)\\n\\nThe sequence of events here is:\\n\\n1. First, the page is blocked with code `403`\\n2. Then the `challenges.cloudflare` check is performed\\n3. Only after successfully passing the check do we receive data with code `200`\\n\\nAutomating this process would require a `headless` browser capable of bypassing [`Cloudflare Turnstile`](https://www.cloudflare.com/application-services/products/turnstile/). The current version of `Crawlee for Python` (v0.5.0) doesn\'t provide this functionality, although it\'s planned for future development.\\n\\nYou can extend the capabilities of Crawlee for Python by integrating [`Camoufox`](https://camoufox.com/) following this [example.](https://www.crawlee.dev/python/docs/examples/playwright-crawler-with-camoufox)\\n\\n## Working with the official Crunchbase API\\n\\nCrunchbase provides a [free API](https://data.crunchbase.com/v4-legacy/docs/crunchbase-basic-using-api) with basic functionality. Paid subscription users get expanded data access. Complete documentation for available endpoints can be found in the [official API specification](https://app.swaggerhub.com/apis-docs/Crunchbase/crunchbase-enterprise_api).\\n\\n### 1. Setting up API access\\n\\nTo start working with the API, follow these steps:\\n\\n1. [Create a Crunchbase account](https://www.crunchbase.com/register)\\n2. Go to the Integrations section\\n3. Create a Crunchbase Basic API key\\n\\nAlthough the documentation states that key activation may take up to an hour, it usually starts working immediately after creation.\\n\\n### 2. Configuring the crawler for API work\\n\\nAn important API feature is the limit - no more than 200 requests per minute, but in the free version, this number is significantly lower. Taking this into account, let\'s configure [`ConcurrencySettings`](https://www.crawlee.dev/python/api/class/ConcurrencySettings). Since we\'re working with the official API, we don\'t need to mask our HTTP client. We\'ll use the standard [\'HttpxHttpClient\'](https://www.crawlee.dev/python/api/class/HttpxHttpClient) with preset headers.\\n\\nFirst, let\'s save the API key in an environment variable:\\n\\n```bash\\nexport CRUNCHBASE_TOKEN={YOUR KEY}\\n```\\n\\nHere\'s how the crawler configuration for working with the API looks:\\n\\n```python\\n# main.py\\n\\nimport os\\n\\nfrom crawlee.crawlers import HttpCrawler\\nfrom crawlee.http_clients import HttpxHttpClient\\nfrom crawlee import ConcurrencySettings, HttpHeaders\\n\\nfrom .routes import router\\n\\nCRUNCHBASE_TOKEN = os.getenv(\'CRUNCHBASE_TOKEN\', \'\')\\n\\n\\nasync def main() -> None:\\n    \\"\\"\\"The crawler entry point.\\"\\"\\"\\n\\n    concurrency_settings = ConcurrencySettings(max_tasks_per_minute=60)\\n\\n    http_client = HttpxHttpClient(\\n        headers=HttpHeaders({\'accept-encoding\': \'gzip, deflate, br, zstd\', \'X-cb-user-key\': CRUNCHBASE_TOKEN})\\n    )\\n    crawler = HttpCrawler(\\n        request_handler=router,\\n        concurrency_settings=concurrency_settings,\\n        http_client=http_client,\\n        max_requests_per_crawl=30,\\n    )\\n\\n    await crawler.run(\\n        [\'https://api.crunchbase.com/api/v4/autocompletes?query=apify&collection_ids=organizations&limit=25\']\\n    )\\n\\n    await crawler.export_data_json(\'crunchbase_data.json\')\\n```\\n\\n### 3. Processing search results\\n\\nFor working with the API, we\'ll need two main endpoints:\\n\\n1. [get_autocompletes](https://app.swaggerhub.com/apis-docs/Crunchbase/crunchbase-enterprise_api/1.0.3#/Autocomplete/get_autocompletes) - for searching\\n2. [get_entities_organizations__entity_id](https://app.swaggerhub.com/apis-docs/Crunchbase/crunchbase-enterprise_api/1.0.3#/Entity/get_entities_organizations__entity_id_) - for getting data\\n\\nFirst, let\'s implement search results processing:\\n\\n```python\\nimport json\\n\\nfrom crawlee.crawlers import HttpCrawler\\nfrom crawlee.router import Router\\nfrom crawlee import Request\\n\\nrouter = Router[HttpCrawlingContext]()\\n\\n\\n@router.default_handler\\nasync def default_handler(context: HttpCrawlingContext) -> None:\\n    \\"\\"\\"Default request handler.\\"\\"\\"\\n    context.log.info(f\'default_handler processing {context.request.url} ...\')\\n\\n    data = json.loads(context.http_response.read())\\n\\n    requests = []\\n\\n    for entity in data[\'entities\']:\\n        permalink = entity[\'identifier\'][\'permalink\']\\n        requests.append(\\n            Request.from_url(\\n                url=f\'https://api.crunchbase.com/api/v4/entities/organizations/{permalink}?field_ids=short_description%2Clocation_identifiers%2Cwebsite_url\',\\n                label=\'company\',\\n            )\\n        )\\n\\n    await context.add_requests(requests)\\n```\\n\\n### 4. Extracting company data\\n\\nAfter getting the list of companies, we extract detailed information about each one:\\n\\n```python\\n@router.handler(\'company\')\\nasync def company_handler(context: HttpCrawlingContext) -> None:\\n    \\"\\"\\"Company request handler.\\"\\"\\"\\n    context.log.info(f\'company_handler processing {context.request.url} ...\')\\n\\n    data = json.loads(context.http_response.read())\\n\\n    await context.push_data(\\n        {\\n            \'Company Name\': data[\'properties\'][\'identifier\'][\'value\'],\\n            \'Short Description\': data[\'properties\'][\'short_description\'],\\n            \'Website\': data[\'properties\'].get(\'website_url\'),\\n            \'Location\': \'; \'.join([item[\'value\'] for item in data[\'properties\'].get(\'location_identifiers\', [])]),\\n        }\\n    )\\n```\\n\\n### 5. Advanced location-based search\\n\\nIf you need more flexible search capabilities, the API provides a special [`search`](https://app.swaggerhub.com/apis-docs/Crunchbase/crunchbase-enterprise_api/1.0.3#/Search/post_searches_organizations) endpoint. Here\'s an example of searching for all companies in Prague:\\n\\n```python\\npayload = {\\n    \'field_ids\': [\'identifier\', \'location_identifiers\', \'short_description\', \'website_url\'],\\n    \'limit\': 200,\\n    \'order\': [{\'field_id\': \'rank_org\', \'sort\': \'asc\'}],\\n    \'query\': [\\n        {\\n            \'field_id\': \'location_identifiers\',\\n            \'operator_id\': \'includes\',\\n            \'type\': \'predicate\',\\n            \'values\': [\'e0b951dc-f710-8754-ddde-5ef04dddd9f8\'],\\n        },\\n        {\'field_id\': \'facet_ids\', \'operator_id\': \'includes\', \'type\': \'predicate\', \'values\': [\'company\']},\\n    ],\\n}\\n\\nserialiazed_payload = json.dumps(payload)\\nawait crawler.run(\\n    [\\n        Request.from_url(\\n            url=\'https://api.crunchbase.com/api/v4/searches/organizations\',\\n            method=\'POST\',\\n            payload=serialiazed_payload,\\n            use_extended_unique_key=True,\\n            headers=HttpHeaders({\'Content-Type\': \'application/json\'}),\\n            label=\'search\',\\n        )\\n    ]\\n)\\n```\\n\\nFor processing search results and pagination, we use the following handler:\\n\\n```python\\n@router.handler(\'search\')\\nasync def search_handler(context: HttpCrawlingContext) -> None:\\n    \\"\\"\\"Search results handler with pagination support.\\"\\"\\"\\n    context.log.info(f\'search_handler processing {context.request.url} ...\')\\n\\n    data = json.loads(context.http_response.read())\\n\\n    last_entity = None\\n    results = []\\n\\n    for entity in data[\'entities\']:\\n        last_entity = entity[\'uuid\']\\n        results.append(\\n            {\\n                \'Company Name\': entity[\'properties\'][\'identifier\'][\'value\'],\\n                \'Short Description\': entity[\'properties\'][\'short_description\'],\\n                \'Website\': entity[\'properties\'].get(\'website_url\'),\\n                \'Location\': \'; \'.join([item[\'value\'] for item in entity[\'properties\'].get(\'location_identifiers\', [])]),\\n            }\\n        )\\n\\n    if results:\\n        await context.push_data(results)\\n\\n    if last_entity:\\n        payload = json.loads(context.request.payload)\\n        payload[\'after_id\'] = last_entity\\n        payload = json.dumps(payload)\\n\\n        await context.add_requests(\\n            [\\n                Request.from_url(\\n                    url=\'https://api.crunchbase.com/api/v4/searches/organizations\',\\n                    method=\'POST\',\\n                    payload=payload,\\n                    use_extended_unique_key=True,\\n                    headers=HttpHeaders({\'Content-Type\': \'application/json\'}),\\n                    label=\'search\',\\n                )\\n            ]\\n        )\\n```\\n\\n### 6. Finally, free API limitations\\n\\nThe free version of the API has significant limitations:\\n\\n- Limited set of available endpoints\\n- Autocompletes function only works for company searches\\n- Not all data fields are accessible\\n- Limited search filtering capabilities\\n\\nConsider a paid subscription for production-level work. The API provides the most reliable way to access Crunchbase data, even with its rate constraints.\\n\\n## What\u2019s your best path forward?\\n\\nWe\'ve explored three different approaches to obtaining data from Crunchbase:\\n\\n1. **Sitemap** - for large-scale data collection\\n2. **Search** - difficult to automate due to Cloudflare protection\\n3. **Official API** - the most reliable solution for commercial projects\\n\\nEach method has its advantages, but for most projects, I recommend using the official API despite its limitations in the free version.\\n\\nThe complete source code is available in my [repository](https://github.com/Mantisus/crunchbase-crawlee). Have questions or want to discuss implementation details? Join our [Discord](https://discord.com/invite/jyEM2PRvMU) - our community of developers is there to help."},{"id":"scrape-google-maps","metadata":{"permalink":"/blog/scrape-google-maps","source":"@site/blog/2024/12-13-scrape-google-maps-using-python/index.md","title":"How to scrape Google Maps data using Python","description":"Learn how to scrape google maps data using Crawlee for Python","date":"2024-12-13T00:00:00.000Z","tags":[{"inline":true,"label":"community","permalink":"/blog/tags/community"}],"readingTime":10.865,"hasTruncateMarker":true,"authors":[{"name":"Satyam Tripathi","title":"Community Member of Crawlee","url":"https://github.com/triposat","socials":{"github":"https://github.com/triposat"},"imageURL":"https://avatars.githubusercontent.com/u/69134468?v=4","key":"SatyamT","page":null}],"frontMatter":{"slug":"scrape-google-maps","title":"How to scrape Google Maps data using Python","tags":["community"],"description":"Learn how to scrape google maps data using Crawlee for Python","image":"./img/google-maps.webp","authors":["SatyamT"]},"unlisted":false,"prevItem":{"title":"How to scrape Crunchbase using Python in 2024 (Easy Guide)","permalink":"/blog/scrape-crunchbase-python"},"nextItem":{"title":"How to scrape Google search results with Python","permalink":"/blog/scrape-google-search"}},"content":"Millions of people use Google Maps daily, leaving behind a goldmine of data just waiting to be analyzed. In this guide, I\'ll show you how to build a reliable scraper using Crawlee and Python to extract locations, ratings, and reviews from Google Maps, all while handling its dynamic content challenges.\\n\\n:::note\\n\\nOne of our community members wrote this blog as a contribution to the Crawlee Blog. If you would like to contribute blogs like these to Crawlee Blog, please reach out to us on our [discord channel](https://apify.com/discord).\\n\\n:::\\n\\n## What data will we extract from Google Maps?\\n\\nWe\u2019ll collect information about hotels in a specific city. You can also customize your search to meet your requirements. For example, you might search for \\"hotels near me\\", \\"5-star hotels in Bombay\\", or other similar queries.\\n\\n![Google Maps Data Screenshot](./img/scrape-google-maps-with-crawlee-screenshot-data-to-scrape.webp)\\n\\nWe\u2019ll extract important data, including the hotel name, rating, review count, price, a link to the hotel page on Google Maps, and all available amenities. Here\u2019s an example of what the extracted data will look like:\\n\\n```json\\n{\\n    \\"name\\": \\"Vividus Hotels, Bangalore\\",\\n    \\"rating\\": \\"4.3\\",\\n    \\"reviews\\": \\"633\\",\\n    \\"price\\": \\"\u20B93,667\\",\\n    \\"amenities\\": [\\n        \\"Pool available\\",\\n        \\"Free breakfast available\\",\\n        \\"Free Wi-Fi available\\",\\n        \\"Free parking available\\"\\n    ],\\n    \\"link\\": \\"https://www.google.com/maps/place/Vividus+Hotels+,+Bangalore/...\\"\\n}\\n```\\n\x3c!-- truncate --\x3e\\n\\n## Building a Google Maps scraper\\n\\nLet\'s build a Google Maps scraper step-by-step.\\n\\n:::note\\n\\nCrawlee requires Python 3.9 or later.\\n\\n:::\\n\\n### 1. Setting up your environment\\n\\nFirst, let\'s set up everything you\u2019ll need to run the scraper. Open your terminal and run these commands:\\n\\n```bash\\n# Create and activate a virtual environment\\npython -m venv google-maps-scraper\\n\\n# Windows:\\n.\\\\google-maps-scraper\\\\Scripts\\\\activate\\n\\n# Mac/Linux:\\nsource google-maps-scraper/bin/activate\\n\\n# We plan to use Playwright with Crawlee, so we need to install both:\\npip install crawlee \\"crawlee[playwright]\\"\\nplaywright install\\n```\\n\\n*If you\'re new to **Crawlee**, check out its easy-to-follow documentation. It\u2019s available for both [Node.js](https://www.crawlee.dev/docs/quick-start) and [Python](https://www.crawlee.dev/python/docs/quick-start).*\\n\\n:::note\\n\\nBefore going ahead with the project, I\'d like to ask you to star Crawlee for Python on [GitHub](https://github.com/apify/crawlee-python/), it helps us to spread the word to fellow scraper developers. \\n\\n:::\\n\\n### 2. Connecting to Google Maps\\n\\nLet\'s see the steps to connect to Google Maps.\\n\\n**Step 1: Setting up the crawler**\\n\\nThe first step is to configure the crawler. We\'re using [`PlaywrightCrawler`](https://www.crawlee.dev/python/api/class/PlaywrightCrawler) from Crawlee, which gives us powerful tools for automated browsing. We set `headless=False` to make the browser visible during scraping and allow 5 minutes for the pages to load.\\n\\n```python\\nfrom crawlee.playwright_crawler import PlaywrightCrawler\\nfrom datetime import timedelta\\n\\n# Initialize crawler with browser visibility and timeout settings\\ncrawler = PlaywrightCrawler(\\n    headless=False,  # Shows the browser window while scraping\\n    request_handler_timeout=timedelta(\\n        minutes=5\\n    ),  # Allows plenty of time for page loading\\n)\\n```\\n\\n**Step 2: Handling each page**\\n\\nThis function defines how each page is handled when the crawler visits it. It uses `context.page` to navigate to the target URL.\\n\\n```python\\nasync def scrape_google_maps(context):\\n    \\"\\"\\"\\n    Establishes connection to Google Maps and handles the initial page load\\n    \\"\\"\\"\\n    page = context.page\\n    await page.goto(context.request.url)\\n    context.log.info(f\\"Processing: {context.request.url}\\")\\n```\\n\\n**Step 3: Launching the crawler**\\n\\nFinally, the main function brings everything together. It creates a search URL, sets up the crawler, and starts the scraping process.\\n\\n```python\\nimport asyncio\\n\\nasync def main():\\n    # Prepare the search URL\\n    search_query = \\"hotels in bengaluru\\"\\n    start_url = f\\"https://www.google.com/maps/search/{search_query.replace(\' \', \'+\')}\\"\\n\\n    # Tell the crawler how to handle each page it visits\\n    crawler.router.default_handler(scrape_google_maps)\\n\\n    # Start the scraping process\\n    await crawler.run([start_url])\\n\\nif __name__ == \\"__main__\\":\\n    asyncio.run(main())\\n```\\n\\nLet\u2019s combine the above code snippets and save them in a file named `gmap_scraper.py`:\\n\\n```python\\nfrom crawlee.playwright_crawler import PlaywrightCrawler\\nfrom datetime import timedelta\\nimport asyncio\\n\\nasync def scrape_google_maps(context):\\n    \\"\\"\\"\\n    Establishes connection to Google Maps and handles the initial page load\\n    \\"\\"\\"\\n    page = context.page\\n    await page.goto(context.request.url)\\n    context.log.info(f\\"Processing: {context.request.url}\\")\\n\\nasync def main():\\n    \\"\\"\\"\\n    Configures and launches the crawler with custom settings\\n    \\"\\"\\"\\n    # Initialize crawler with browser visibility and timeout settings\\n    crawler = PlaywrightCrawler(\\n        headless=False,  # Shows the browser window while scraping\\n        request_handler_timeout=timedelta(\\n            minutes=5\\n        ),  # Allows plenty of time for page loading\\n    )\\n\\n    # Tell the crawler how to handle each page it visits\\n    crawler.router.default_handler(scrape_google_maps)\\n\\n    # Prepare the search URL\\n    search_query = \\"hotels in bengaluru\\"\\n    start_url = f\\"https://www.google.com/maps/search/{search_query.replace(\' \', \'+\')}\\"\\n\\n    # Start the scraping process\\n    await crawler.run([start_url])\\n\\nif __name__ == \\"__main__\\":\\n    asyncio.run(main())\\n```\\n\\nRun the code using:\\n\\n```bash\\n$ python3 gmap_scraper.py\\n```\\n\\nWhen everything works correctly, you\'ll see the output like this:\\n\\n![Connect to page](./img/scrape-google-maps-with-crawlee-screenshot-connect-to-page.png)\\n\\n### 3. Import dependencies and defining Scraper Class\\n\\nLet\'s start with the basic structure and necessary imports:\\n\\n```python\\nimport asyncio\\nfrom datetime import timedelta\\nfrom typing import Dict, Optional, Set\\nfrom crawlee.playwright_crawler import PlaywrightCrawler\\nfrom playwright.async_api import Page, ElementHandle\\n```\\n\\nThe `GoogleMapsScraper` class serves as the main scraper engine:\\n\\n```python\\nclass GoogleMapsScraper:\\n    def __init__(self, headless: bool = True, timeout_minutes: int = 5):\\n        self.crawler = PlaywrightCrawler(\\n            headless=headless,\\n            request_handler_timeout=timedelta(minutes=timeout_minutes),\\n        )\\n        self.processed_names: Set[str] = set()\\n\\n    async def setup_crawler(self) -> None:\\n        self.crawler.router.default_handler(self._scrape_listings)\\n```\\n\\nThis initialization code sets up two crucial components:\\n\\n1. A `PlaywrightCrawler` instance configured to run either headlessly (without a visible browser window) or with a visible browser\\n2. A set to track processed business names, preventing duplicate entries\\n\\nThe `setup_crawler` method configures the crawler to use our main scraping function as the default handler for all requests.\\n\\n### 4. Understanding Google Maps internal code structure\\n\\nBefore we dive into scraping, let\'s understand exactly what elements we need to target. When you search for hotels in Bengaluru, Google Maps organizes hotel information in a specific structure. Here\'s a detailed breakdown of how to locate each piece of information.\\n\\n**Hotel name:**\\n\\n![Hotel name](./img/scrape-google-maps-with-crawlee-screenshot-name.webp)\\n\\n**Hotel rating:**\\n\\n![Hotel rating](./img/scrape-google-maps-with-crawlee-screenshot-ratings.webp)\\n\\n**Hotel review count:**\\n\\n![Hotel Review Count](./img/scrape-google-maps-with-crawlee-screenshot-reviews.webp)\\n\\n**Hotel URL:**\\n\\n![Hotel URL](./img/scrape-google-maps-with-crawlee-screenshot-url.webp)\\n\\n**Hotel Price:**\\n\\n![Hotel Price](./img/scrape-google-maps-with-crawlee-screenshot-price.webp)\\n\\n**Hotel amenities:**\\n\\nThis returns multiple elements as each hotel has several amenities. We\'ll need to iterate through these.\\n\\n![Hotel amenities](./img/scrape-google-maps-with-crawlee-screenshot-amenities.webp)\\n\\n**Quick tips:**\\n\\n- Always verify these selectors before scraping, as Google might update them.\\n- Use Chrome DevTools (F12) to inspect elements and confirm selectors.\\n- Some elements might not be present for all hotels (like prices during the off-season).\\n\\n### 5. Scraping Google Maps data using identified selectors\\n\\nLet\'s build a scraper to extract detailed hotel information from Google Maps. First, create the core scraping function to handle data extraction.\\n\\n*gmap_scraper.py:*\\n\\n```python\\nasync def _extract_listing_data(self, listing: ElementHandle) -> Optional[Dict]:\\n    \\"\\"\\"Extract structured data from a single listing element.\\"\\"\\"\\n    try:\\n        name_el = await listing.query_selector(\\".qBF1Pd\\")\\n        if not name_el:\\n            return None\\n        name = await name_el.inner_text()\\n        if name in self.processed_names:\\n            return None\\n\\n        elements = {\\n            \\"rating\\": await listing.query_selector(\\".MW4etd\\"),\\n            \\"reviews\\": await listing.query_selector(\\".UY7F9\\"),\\n            \\"price\\": await listing.query_selector(\\".wcldff\\"),\\n            \\"link\\": await listing.query_selector(\\"a.hfpxzc\\"),\\n            \\"address\\": await listing.query_selector(\\".W4Efsd:nth-child(2)\\"),\\n            \\"category\\": await listing.query_selector(\\".W4Efsd:nth-child(1)\\"),\\n        }\\n\\n        amenities = []\\n        amenities_els = await listing.query_selector_all(\\".dc6iWb\\")\\n        for amenity in amenities_els:\\n            amenity_text = await amenity.get_attribute(\\"aria-label\\")\\n            if amenity_text:\\n                amenities.append(amenity_text)\\n\\n        place_data = {\\n            \\"name\\": name,\\n            \\"rating\\": await elements[\\"rating\\"].inner_text() if elements[\\"rating\\"] else None,\\n            \\"reviews\\": (await elements[\\"reviews\\"].inner_text()).strip(\\"()\\") if elements[\\"reviews\\"] else None,\\n            \\"price\\": await elements[\\"price\\"].inner_text() if elements[\\"price\\"] else None,\\n            \\"address\\": await elements[\\"address\\"].inner_text() if elements[\\"address\\"] else None,\\n            \\"category\\": await elements[\\"category\\"].inner_text() if elements[\\"category\\"] else None,\\n            \\"amenities\\": amenities if amenities else None,\\n            \\"link\\": await elements[\\"link\\"].get_attribute(\\"href\\") if elements[\\"link\\"] else None,\\n        }\\n\\n        self.processed_names.add(name)\\n        return place_data\\n    except Exception as e:\\n        context.log.exception(\\"Error extracting listing data\\")\\n        return None\\n```\\n\\nIn the code:\\n\\n- `query_selector`: Returns first DOM element matching CSS selector, useful for single items like a name or rating\\n- `query_selector_all`: Returns all matching elements, ideal for multiple items like amenities\\n- `inner_text()`: Extracts text content\\n- Some hotels might not have all the information available - we handle this with \'N/A\u2019\\n\\nWhen you run this script, you\'ll see output similar to this:\\n\\n```json\\n{\\n    \\"name\\": \\"GRAND KALINGA HOTEL\\",\\n    \\"rating\\": \\"4.2\\",\\n    \\"reviews\\": \\"1,171\\",\\n    \\"price\\": \\"\\\\u20b91,760\\",\\n    \\"link\\": \\"https://www.google.com/maps/place/GRAND+KALINGA+HOTEL/data=!4m10!3m9!1s0x3bae160e0ce07789:0xb15bf736f4238e6a!5m2!4m1!1i2!8m2!3d12.9762259!4d77.5786043!16s%2Fg%2F11sp32pz28!19sChIJiXfgDA4WrjsRao4j9Db3W7E?authuser=0&hl=en&rclk=1\\",\\n    \\"amenities\\": [\\n        \\"Pool available\\",\\n        \\"Free breakfast available\\",\\n        \\"Free Wi-Fi available\\",\\n        \\"Free parking available\\"\\n    ]\\n}\\n```\\n\\n### 6. Managing Infinite Scrolling\\n\\nGoogle Maps uses infinite scrolling to load more results as users scroll down. We handle this with a dedicated method:\\n\\nFirst, we need a function that can handle the scrolling and detect when we\'ve hit the bottom. Copy-paste this new function in the `gmap_scraper.py` file:\\n\\n```python\\nasync def _load_more_items(self, page: Page) -> bool:\\n        \\"\\"\\"Scroll down to load more items.\\"\\"\\"\\n        try:\\n            feed = await page.query_selector(\'div[role=\\"feed\\"]\')\\n            if not feed:\\n                return False\\n            prev_scroll = await feed.evaluate(\\"(element) => element.scrollTop\\")\\n            await feed.evaluate(\\"(element) => element.scrollTop += 800\\")\\n            await page.wait_for_timeout(2000)\\n\\n            new_scroll = await feed.evaluate(\\"(element) => element.scrollTop\\")\\n            if new_scroll <= prev_scroll:\\n                return False\\n            await page.wait_for_timeout(1000)\\n            return True\\n        except Exception as e:\\n            context.log.exception(\\"Error during scroll\\")\\n            return False\\n```\\n\\nRun this code using:\\n\\n```bash\\n$ python3 gmap_scraper.py\\n```\\n\\nYou should see an output like this:\\n\\n![scrape-google-maps-with-crawlee-screenshot-handle-pagination](./img/scrape-google-maps-with-crawlee-screenshot-handle-pagination.webp)\\n\\n### 7. Scrape Listings\\n\\nThe main scraping function ties everything together. It scrapes listings from the page by repeatedly extracting data and scrolling.\\n\\n```python\\nasync def _scrape_listings(self, context) -> None:\\n    \\"\\"\\"Main scraping function to process all listings\\"\\"\\"\\n    try:\\n        page = context.page\\n        print(f\\"\\\\nProcessing URL: {context.request.url}\\\\n\\")\\n\\n        await page.wait_for_selector(\\".Nv2PK\\", timeout=30000)\\n        await page.wait_for_timeout(2000)\\n\\n        while True:\\n            listings = await page.query_selector_all(\\".Nv2PK\\")\\n            new_items = 0\\n\\n            for listing in listings:\\n                place_data = await self._extract_listing_data(listing)\\n                if place_data:\\n                    await context.push_data(place_data)\\n                    new_items += 1\\n                    print(f\\"Processed: {place_data[\'name\']}\\")\\n\\n            if new_items == 0 and not await self._load_more_items(page):\\n                break\\n            if new_items > 0:\\n                await self._load_more_items(page)\\n                \\n        print(f\\"\\\\nFinished processing! Total items: {len(self.processed_names)}\\")\\n    except Exception as e:\\n        print(f\\"Error in scraping: {str(e)}\\")\\n```\\n\\nThe scraper uses Crawlee\'s built-in storage system to manage scraped data. When you run the scraper, it creates a `storage` directory in your project with several key components:\\n\\n- `datasets/`: Contains the scraped results in JSON format\\n- `key_value_stores/`: Stores crawler state and metadata\\n- `request_queues/`: Manages URLs to be processed\\n\\nThe `push_data()` method we use in our scraper sends the data to Crawlee\'s dataset storage as you can see below:\\n\\n![Crawlee push_data](./img/How-to-scrape-Google-Maps-data-using-Python-and-Crawlee-metadata.webp)\\n\\n### 8. Running the Scraper\\n\\nFinally, we need functions to execute our scraper:\\n\\n```python\\nasync def run(self, search_query: str) -> None:\\n    \\"\\"\\"Execute the scraper with a search query\\"\\"\\"\\n    try:\\n        await self.setup_crawler()\\n        start_url = f\\"https://www.google.com/maps/search/{search_query.replace(\' \', \'+\')}\\"\\n        await self.crawler.run([start_url])\\n        await self.crawler.export_data_json(\'gmap_data.json\')\\n    except Exception as e:\\n        print(f\\"Error running scraper: {str(e)}\\")\\n\\nasync def main():\\n    \\"\\"\\"Entry point of the script\\"\\"\\"\\n    scraper = GoogleMapsScraper(headless=True)\\n    search_query = \\"hotels in bengaluru\\"\\n    await scraper.run(search_query)\\n\\nif __name__ == \\"__main__\\":\\n    asyncio.run(main())\\n```\\n\\nThis data is automatically stored and can later be exported to a JSON file using:\\n\\n```python\\nawait self.crawler.export_data_json(\'gmap_data.json\')\\n```\\n\\nHere\'s what your exported JSON file will look like:\\n\\n```json\\n[\\n  {\\n    \\"name\\": \\"Vividus Hotels, Bangalore\\",\\n    \\"rating\\": \\"4.3\\",\\n    \\"reviews\\": \\"633\\",\\n    \\"price\\": \\"\u20B93,667\\",\\n    \\"amenities\\": [\\n      \\"Pool available\\",\\n      \\"Free breakfast available\\",\\n      \\"Free Wi-Fi available\\",\\n      \\"Free parking available\\"\\n    ],\\n    \\"link\\": \\"https://www.google.com/maps/place/Vividus+Hotels+,+Bangalore/...\\"\\n  }\\n]\\n```\\n\\n### 9. Using proxies for Google Maps scraping\\n\\nWhen scraping Google Maps at scale, using proxies is very helpful. Here are a few key reasons why:\\n\\n1. **Avoid IP blocks**: Google Maps can detect and block IP addresses that make an excessive number of requests in a short time. Using proxies helps you stay under the radar.\\n2. **Bypass rate limits**: Google implements strict limits on the number of requests per IP address. By rotating through multiple IPs, you can maintain a consistent scraping pace without hitting these limits.\\n3. **Access location-specific data**: Different regions may display different data on Google Maps. Proxies allow you to view listings as if you are browsing from any specific location.\\n\\nHere\'s a simple implementation using Crawlee\'s built-in proxy management. Update your previous code with this to use proxy settings.\\n\\n```python\\nfrom crawlee.playwright_crawler import PlaywrightCrawler\\nfrom crawlee.proxy_configuration import ProxyConfiguration\\n\\n# Configure your proxy settings\\nproxy_configuration = ProxyConfiguration(\\n    proxy_urls=[\\n        \\"http://username:password@proxy.provider.com:12345\\",\\n        # Add more proxy URLs as needed\\n    ]\\n)\\n\\n# Initialize crawler with proxy support\\ncrawler = PlaywrightCrawler(\\n    headless=True,\\n    request_handler_timeout=timedelta(minutes=5),\\n    proxy_configuration=proxy_configuration,\\n)\\n```\\n\\nHere, I use a proxy to scrape hotel data in New York City.\\n\\n![Using a proxy](./img/scrape-google-maps-with-crawlee-screenshot-proxies.webp)\\n\\nHere\'s an example of data scraped from New York City hotels using proxies:\\n\\n```json\\n{\\n  \\"name\\": \\"The Manhattan at Times Square Hotel\\",\\n  \\"rating\\": \\"3.1\\",\\n  \\"reviews\\": \\"8,591\\",\\n  \\"price\\": \\"$120\\",\\n  \\"amenities\\": [\\n    \\"Free parking available\\",\\n    \\"Free Wi-Fi available\\",\\n    \\"Air-conditioned available\\",\\n    \\"Breakfast available\\"\\n  ],\\n  \\"link\\": \\"https://www.google.com/maps/place/...\\"\\n}\\n```\\n\\n### 10. Project: Interactive hotel analysis dashboard\\n\\nAfter scraping hotel data from Google Maps, you can build an interactive dashboard that helps analyze hotel trends. Here\u2019s a preview of how the dashboard works:\\n\\n![Final dashboard](./img/scrape-google-maps-with-crawlee-screenshot-hotel-analysis-dashboard.webp)\\n\\nFind the complete info for this dashboard on GitHub: [Hotel Analysis Dashboard](https://github.com/triposat/Hotel-Analytics-Dashboard).\\n\\n### 11. Now you\u2019re ready to put everything into action!\\n\\nTake a look at the complete scripts in my GitHub Gist:\\n\\n- [Basic Scraper](https://gist.github.com/triposat/9a6fb03130f3c4332bab71b72a973940)\\n- [Code with Proxy Integration](https://gist.github.com/triposat/6c554b13c787a55348b48b6bfc5459c0)\\n- [Hotel Analysis Dashboard](https://gist.github.com/triposat/13ce4b05c36512e69b5602833e781a6c)\\n\\nTo make it all work:\\n\\n1. **Run the basic scraper or proxy-integrated scraper**: This will collect the hotel data and store it in a JSON file.\\n2. **Run the dashboard script**: Load your JSON data and view it interactively in the dashboard.\\n\\n## Wrapping up and next steps\\n\\nYou\'ve successfully built a comprehensive Google Maps scraper that collects and processes hotel data, presenting it through an interactive dashboard. Now you\u2019ve learned about:\\n\\n- Using Crawlee with Playwright to navigate and extract data from Google Maps\\n- Using proxies to scale up scraping without getting blocked\\n- Storing the extracted data in JSON format\\n- Creating an interactive dashboard to analyze hotel data\\n\\nWe\u2019ve handpicked some great resources to help you further explore web scraping:\\n\\n- [Scrapy vs. Crawlee: Choosing the right tool](https://www.crawlee.dev/blog/scrapy-vs-crawlee)\\n- [Mastering proxy management with Crawlee](https://wwww.crawlee.dev/blog/proxy-management-in-crawlee)\\n- [Think like a web scraping expert: 12 pro tips](https://www.crawlee.dev/blog/web-scraping-tips)\\n- [Building a LinkedIn job scraper](https://www.crawlee.dev/blog/linkedin-job-scraper-python)"},{"id":"scrape-google-search","metadata":{"permalink":"/blog/scrape-google-search","source":"@site/blog/2024/12-02-scrape-google-search/index.md","title":"How to scrape Google search results with Python","description":"Learn how to scrape google search results using Crawlee for Python","date":"2024-12-02T00:00:00.000Z","tags":[{"inline":true,"label":"community","permalink":"/blog/tags/community"}],"readingTime":6.115,"hasTruncateMarker":true,"authors":[{"name":"Max","title":"Community Member of Crawlee and web scraping expert","url":"https://github.com/Mantisus","socials":{"github":"https://github.com/Mantisus"},"imageURL":"https://avatars.githubusercontent.com/u/34358312?v=4","key":"MaxB","page":null}],"frontMatter":{"slug":"scrape-google-search","title":"How to scrape Google search results with Python","tags":["community"],"description":"Learn how to scrape google search results using Crawlee for Python","image":"./img/google-search.webp","authors":["MaxB"]},"unlisted":false,"prevItem":{"title":"How to scrape Google Maps data using Python","permalink":"/blog/scrape-google-maps"},"nextItem":{"title":"Reverse engineering GraphQL persistedQuery extension","permalink":"/blog/graphql-persisted-query"}},"content":"Scraping `Google Search` delivers essential `SERP analysis`, SEO optimization, and data collection capabilities. Modern scraping tools make this process faster and more reliable.\\n\\n:::note\\n\\nOne of our community members wrote this blog as a contribution to the Crawlee Blog. If you would like to contribute blogs like these to Crawlee Blog, please reach out to us on our [discord channel](https://apify.com/discord).\\n\\n:::\\n\\nIn this guide, we\'ll create a Google Search scraper using [`Crawlee for Python`](https://github.com/apify/crawlee-python) that can handle result ranking and pagination.\\n\\nWe\'ll create a scraper that:\\n\\n- Extracts titles, URLs, and descriptions from search results\\n- Handles multiple search queries\\n- Tracks ranking positions\\n- Processes multiple result pages\\n- Saves data in a structured format\\n\\n![How to scrape Google search results with Python](./img/google-search.webp)\\n\\n\x3c!-- truncate --\x3e\\n\\n## Prerequisites\\n\\n- Python 3.7 or higher\\n- Basic understanding of HTML and CSS selectors\\n- Familiarity with web scraping concepts\\n- Crawlee for Python v0.4.2 or higher\\n\\n### Project setup\\n\\n1. Install Crawlee with required dependencies:\\n\\n    ```bash\\n    pipx install crawlee[beautifulsoup,curl-impersonate]\\n    ```\\n\\n2. Create a new project using Crawlee CLI:\\n\\n    ```bash\\n    pipx run crawlee create crawlee-google-search\\n    ```\\n\\n3. When prompted, select `Beautifulsoup` as your template type.\\n4. Navigate to the project directory and complete installation:\\n\\n    ```bash\\n    cd crawlee-google-search\\n    poetry install\\n    ```\\n\\n## Development of the Google Search scraper in Python\\n\\n### 1. Defining data for extraction\\n\\nFirst, let\'s define our extraction scope. Google\'s search results now include maps, notable people, company details, videos, common questions, and many other elements. We\'ll focus on analyzing standard search results with rankings.\\n\\nHere\'s what we\'ll be extracting:\\n\\n![Search Example](./img/search_example.webp)\\n\\nLet\'s verify whether we can extract the necessary data from the page\'s HTML code, or if we need deeper analysis or `JS` rendering. Note that this verification is sensitive to HTML tags:\\n\\n![Check Html](./img/check_html.webp)\\n\\nBased on the data obtained from the page, all necessary information is present in the HTML code. Therefore, we can use [`beautifulsoup_crawler`](https://www.crawlee.dev/python/docs/examples/beautifulsoup-crawler).\\n\\nThe fields we\'ll extract:\\n\\n- Search result titles\\n- URLs\\n- Description text\\n- Ranking positions\\n\\n### 2. Configure the crawler\\n\\nFirst, let\'s create the crawler configuration.\\n\\nWe\'ll use [`CurlImpersonateHttpClient`](https://www.crawlee.dev/python/api/class/CurlImpersonateHttpClient) as our `http_client` with preset `headers` and `impersonate` relevant to the [`Chrome`](https://www.google.com/intl/en/chrome/) browser.\\n\\nWe\'ll also configure [`ConcurrencySettings`](https://www.crawlee.dev/python/api/class/ConcurrencySettings) to control scraping aggressiveness. This is crucial to avoid getting blocked by Google.\\n\\nIf you need to extract data more intensively, consider setting up [`ProxyConfiguration`](https://www.crawlee.dev/python/api/class/ProxyConfiguration).\\n\\n```python\\nfrom crawlee.beautifulsoup_crawler import BeautifulSoupCrawler\\nfrom crawlee.http_clients.curl_impersonate import CurlImpersonateHttpClient\\nfrom crawlee import ConcurrencySettings, HttpHeaders\\n\\nasync def main() -> None:\\n    concurrency_settings = ConcurrencySettings(max_concurrency=5, max_tasks_per_minute=200)\\n\\n    http_client = CurlImpersonateHttpClient(impersonate=\\"chrome124\\",\\n                                            headers=HttpHeaders({\\"referer\\": \\"https://www.google.com/\\",\\n                                                     \\"accept-language\\": \\"en\\",\\n                                                     \\"accept-encoding\\": \\"gzip, deflate, br, zstd\\",\\n                                                     \\"user-agent\\": \\"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/131.0.0.0 Safari/537.36\\"\\n                                            }))\\n\\n    crawler = BeautifulSoupCrawler(\\n        max_request_retries=1,\\n        concurrency_settings=concurrency_settings,\\n        http_client=http_client,\\n        max_requests_per_crawl=10,\\n        max_crawl_depth=5\\n    )\\n\\n    await crawler.run([\'https://www.google.com/search?q=Apify\'])\\n```\\n\\n### 3. Implementing data extraction\\n\\nFirst, let\'s analyze the HTML code of the elements we need to extract:\\n\\n![Check Html](./img/html_example.webp)\\n\\nThere\'s an obvious distinction between *readable* ID attributes and *generated* class names and other attributes. When creating selectors for data extraction, you should ignore any generated attributes. Even if you\'ve read that Google has been using a particular generated tag for N years, you shouldn\'t rely on it - this reflects your experience in writing robust code.\\n\\nNow that we understand the HTML structure, let\'s implement the extraction. As our crawler deals with only one type of page, we can use `router.default_handler` for processing it. Within the handler, we\'ll use `BeautifulSoup` to iterate through each search result, extracting data such as `title`, `url`, and `text_widget` while saving the results.\\n\\n```python\\n@crawler.router.default_handler\\nasync def default_handler(context: BeautifulSoupCrawlingContext) -> None:\\n    \\"\\"\\"Default request handler.\\"\\"\\"\\n    context.log.info(f\'Processing {context.request} ...\')\\n\\n    for item in context.soup.select(\\"div#search div#rso div[data-hveid][lang]\\"):\\n        data = {\\n            \'title\': item.select_one(\\"h3\\").get_text(),\\n            \\"url\\": item.select_one(\\"a\\").get(\\"href\\"),\\n            \\"text_widget\\": item.select_one(\\"div[style*=\'line\']\\").get_text(),\\n        }\\n        await context.push_data(data)\\n```\\n\\n### 4. Handling pagination\\n\\nSince Google results depend on the IP geolocation of the search request, we can\'t rely on link text for pagination. We need to create a more sophisticated CSS selector that works regardless of geolocation and language settings.\\n\\nThe `max_crawl_depth` parameter controls how many pages our crawler should scan. Once we have our robust selector, we simply need to get the next page link and add it to the crawler\'s queue.\\n\\nTo write more efficient selectors, learn the basics of [CSS](https://www.w3schools.com/cssref/css_selectors.php) and [XPath](https://www.w3schools.com/xml/xpath_syntax.asp) syntax.\\n\\n```python\\n    await context.enqueue_links(selector=\\"div[role=\'navigation\'] td[role=\'heading\']:last-of-type > a\\")\\n```\\n\\n### 5. Exporting data to CSV format\\n\\nSince we want to save all search result data in a convenient tabular format like CSV, we can simply add the export_data method call right after running the crawler:\\n\\n```python\\nawait crawler.export_data_csv(\\"google_search.csv\\")\\n```\\n\\n### 6. Finalizing the Google Search scraper\\n\\nWhile our core crawler logic works, you might have noticed that our results currently lack ranking position information. To complete our scraper, we need to implement proper ranking position tracking by passing data between requests using `user_data` in [`Request`](https://www.crawlee.dev/python/api/class/Request).\\n\\nLet\'s modify the script to handle multiple queries and track ranking positions for search results analysis. We\'ll also set the crawling depth as a top-level variable. Let\'s move the `router.default_handler` to `routes.py` to match the project structure:\\n\\n```python\\n# crawlee-google-search.main\\n\\nfrom crawlee.beautifulsoup_crawler import BeautifulSoupCrawler, BeautifulSoupCrawlingContext\\nfrom crawlee.http_clients.curl_impersonate import CurlImpersonateHttpClient\\nfrom crawlee import Request, ConcurrencySettings, HttpHeaders\\n\\nfrom .routes import router\\n\\nQUERIES = [\\"Apify\\", \\"Crawlee\\"]\\n\\nCRAWL_DEPTH = 2\\n\\n\\nasync def main() -> None:\\n    \\"\\"\\"The crawler entry point.\\"\\"\\"\\n\\n    concurrency_settings = ConcurrencySettings(max_concurrency=5, max_tasks_per_minute=200)\\n\\n    http_client = CurlImpersonateHttpClient(impersonate=\\"chrome124\\",\\n                                            headers=HttpHeaders({\\"referer\\": \\"https://www.google.com/\\",\\n                                                     \\"accept-language\\": \\"en\\",\\n                                                     \\"accept-encoding\\": \\"gzip, deflate, br, zstd\\",\\n                                                     \\"user-agent\\": \\"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/131.0.0.0 Safari/537.36\\"\\n                                            }))\\n    crawler = BeautifulSoupCrawler(\\n        request_handler=router,\\n        max_request_retries=1,\\n        concurrency_settings=concurrency_settings,\\n        http_client=http_client,\\n        max_requests_per_crawl=100,\\n        max_crawl_depth=CRAWL_DEPTH\\n    )\\n\\n    requests_lists = [Request.from_url(f\\"https://www.google.com/search?q={query}\\", user_data = {\\"query\\": query}) for query in QUERIES]\\n\\n    await crawler.run(requests_lists)\\n\\n    await crawler.export_data_csv(\\"google_ranked.csv\\")\\n```\\n\\nLet\'s also modify the handler to add `query` and `order_no` fields and basic error handling:\\n\\n```python\\n# crawlee-google-search.routes\\n\\nfrom crawlee.beautifulsoup_crawler import BeautifulSoupCrawlingContext\\nfrom crawlee.router import Router\\n\\nrouter = Router[BeautifulSoupCrawlingContext]()\\n\\n\\n@router.default_handler\\nasync def default_handler(context: BeautifulSoupCrawlingContext) -> None:\\n    \\"\\"\\"Default request handler.\\"\\"\\"\\n    context.log.info(f\'Processing {context.request.url} ...\')\\n\\n    order = context.request.user_data.get(\\"last_order\\", 1)\\n    query = context.request.user_data.get(\\"query\\")\\n    for item in context.soup.select(\\"div#search div#rso div[data-hveid][lang]\\"):\\n        try:\\n            data = {\\n                \\"query\\": query,\\n                \\"order_no\\": order,\\n                \'title\': item.select_one(\\"h3\\").get_text(),\\n                \\"url\\": item.select_one(\\"a\\").get(\\"href\\"),\\n                \\"text_widget\\": item.select_one(\\"div[style*=\'line\']\\").get_text(),\\n            }\\n            await context.push_data(data)\\n            order += 1\\n        except AttributeError as e:\\n            context.log.warning(f\'Attribute error for query \\"{query}\\": {str(e)}\')\\n        except Exception as e:\\n            context.log.error(f\'Unexpected error for query \\"{query}\\": {str(e)}\')\\n\\n    await context.enqueue_links(selector=\\"div[role=\'navigation\'] td[role=\'heading\']:last-of-type > a\\",\\n                                user_data={\\"last_order\\": order, \\"query\\": query})\\n```\\n\\nAnd we\'re done!\\n\\nOur Google Search crawler is ready. Let\'s look at the results in the `google_ranked.csv` file:\\n\\n![Results CSV](./img/results.webp)\\n\\nThe code repository is available on [`GitHub`](https://github.com/Mantisus/crawlee-google-search)\\n\\n## Scrape Google Search results with Apify\\n\\nIf you\'re working on a large-scale project requiring millions of data points, like the project featured in this [article about Google ranking analysis](https://backlinko.com/search-engine-ranking) - you might need a ready-made solution.\\n\\nConsider using [`Google Search Results Scraper`](https://www.apify.com/apify/google-search-scraper) by the Apify team.\\n\\nIt offers important features such as:\\n\\n- Proxy support\\n- Scalability for large-scale data extraction\\n- Geolocation control\\n- Integration with external services like [`Zapier`](https://zapier.com/), [`Make`](https://www.make.com/), [`Airbyte`](https://airbyte.com/), [`LangChain`](https://www.langchain.com/) and others\\n\\nYou can learn more in the Apify [blog](https://blog.apify.com/unofficial-google-search-api-from-apify-22a20537a951/)\\n\\n## What will you scrape?\\n\\nIn this blog, we\'ve explored step-by-step how to create a Google Search crawler that collects ranking data. How you analyze this dataset is up to you!\\n\\nAs a reminder, you can find the full project code on [`GitHub`](https://github.com/Mantisus/crawlee-google-search).\\n\\nI\'d like to think that in 5 years I\'ll need to write an article on \\"How to extract data from the best search engine for LLMs\\", but I suspect that in 5 years this article will still be relevant."},{"id":"graphql-persisted-query","metadata":{"permalink":"/blog/graphql-persisted-query","source":"@site/blog/2024/11-15-graphql-persisted-query/index.md","title":"Reverse engineering GraphQL persistedQuery extension","description":"Learn how to do reverse engineering on persistedQuery extension by GraphQL and reveal the query hash needed for scraping.","date":"2024-11-15T00:00:00.000Z","tags":[],"readingTime":4.67,"hasTruncateMarker":true,"authors":[{"name":"Saurav Jain","title":"Developer Community Manager","url":"https://github.com/souravjain540","socials":{"x":"https://x.com/sauain","github":"https://github.com/souravjain540"},"imageURL":"https://avatars.githubusercontent.com/u/53312820?v=4","key":"SauravJ","page":null},{"name":"Mat\u011Bj Volf","title":"Web Automation Engineer","url":"https://github.com/mvolfik","socials":{"github":"https://github.com/mvolfik"},"imageURL":"https://avatars.githubusercontent.com/u/31281386?v=4","key":"MatejV","page":null}],"frontMatter":{"slug":"graphql-persisted-query","title":"Reverse engineering GraphQL persistedQuery extension","description":"Learn how to do reverse engineering on persistedQuery extension by GraphQL and reveal the query hash needed for scraping.","image":"./img/graphql.webp","authors":["SauravJ","MatejV"]},"unlisted":false,"prevItem":{"title":"How to scrape Google search results with Python","permalink":"/blog/scrape-google-search"},"nextItem":{"title":"12 tips on how to think like a web scraping expert","permalink":"/blog/web-scraping-tips"}},"content":"GraphQL is a query language for getting deeply nested structured data from a website\'s backend, similar to MongoDB queries.\\n\\nThe request is usually a POST to some general `/graphql` endpoint with a body like this:\\n\\n![GraphQL Query](./img/graphql.webp)\\n\\nWhen scraping data from websites using GraphQL, it\u2019s common to inspect the network requests in developer tools to find the exact queries being used. However, on some websites, you might notice that the GraphQL query itself isn\u2019t visible in the request. Instead, you only see a cryptic hash value. This can be confusing and makes it harder to understand how data is being requested from the server.\\n\\nThis is because some websites use a feature called [\\"persisted queries.](https://www.apollographql.com/docs/apollo-server/performance/apq/) It\'s a performance optimization that reduces the amount of data sent with each request by replacing the full query text with a precomputed hash. While this improves website speed and efficiency, it introduces challenges for scraping because the query text isn\u2019t readily available.\\n\\n\\n![Persisted Query Reverse Engineering](./img/graphql-persisted-query.webp)\\n\\n\x3c!-- truncate --\x3e\\n\\nTLDR: the client computes the sha256 hash of the `query` text and only sends that hash. In addition, you can possibly fit all of this into the query string of a GET request, making it easily cachable. Below is an example request from Zillow\\n\\n![Request from Zillow](./img/zillow.webp)\\n\\nAs you can see, it\u2019s just some metadata about the persistedQuery extension, the hash of the query, and variables to be embedded in the query.\\n\\nHere\u2019s another request from expedia.com, sent as a POST, but with the same extension:\\n\\n![Expedia Query](./img/expedia.webp)\\n\\n\\nThis primarily optimizes website performance, but it creates several challenges for web scraping:\\n\\n- GET requests are usually more prone to being blocked.\\n- Hidden Query Parameters: We don\u2019t know the full query, so if the website responds with a \u201CPersisted query not found\u201D error (asking us to send the query in full, not just the hash), we can\u2019t send it.\\n- Once the website changes even a little bit and the clients start asking for a new query - even though the old one might still work, the server will very soon forget its ID/hash, and your request with this hash will never work again, since you can\u2019t \u201Cremind\u201D the server of the full query text.\\n\\nFor various reasons, you might need to extract the entire GraphQL query text, but this can be tricky. While you could inspect the website\u2019s JavaScript to find the query text, it\u2019s often dynamically constructed from multiple fragments, making it hard to piece together.\\n\\nInstead, we\u2019ll take a more direct approach: tricking the client application (e.g., the browser) into revealing the full query. When the client uses a hash that the server doesn\'t recognize, the server typically responds with an error message like `PersistedQueryNotFound`. This prompts the client to resend the full query in a subsequent request. By intercepting and modifying the original request to include an invalid hash, we can trigger this behavior and capture the complete query text. This method avoids digging through JavaScript and relies on the natural error-handling flow of the client-server interaction.\\n\\nFor exactly this use case, a perfect tool exists: [mitmproxy](https://mitmproxy.org/), an open-source Python library that intercepts requests made by your own devices, websites, or apps and allows you to modify them with simple Python scripts.\\n\\nDownload `mitmproxy`, and prepare a Python script like this:\\n\\n```python\\nimport json\\n\\ndef request(flow):\\n    try:\\n        dat = json.loads(flow.request.text)\\n        dat[0][\\"extensions\\"][\\"persistedQuery\\"][\\"sha256Hash\\"] = \\"0d9e\\" # any bogus hex string here\\n        flow.request.text = json.dumps(dat)\\n    except:\\n        pass\\n```\\n\\nThis defines a hook that `mitmproxy` will run on every request: it tries to load the request\'s JSON body, modifies the hash to an arbitrary value, and writes the updated JSON as a new body of the request.\\n\\nWe also need to make sure we reroute our browser requests to `mitmproxy`. For this purpose we are going to use a browser extension called [FoxyProxy](https://chromewebstore.google.com/detail/foxyproxy/gcknhkkoolaabfmlnjonogaaifnjlfnp?hl=en). It is available in both Firefox and Chrome.\\n\\nJust add a route with these settings:\\n\\n![mitmproxy settings](./img/mitmprpxy.webp)\\n\\nNow we can run `mitmproxy` with this script: `mitmweb -s script.py`\\n\\nThis will open a browser tab where you can watch all the intercepted requests in real-time.\\n\\n![Browser tab](./img/browser.webp)\\n\\nIf you go to the particular path and see the query in the request section, you will see some garbage value has replaced the hash.\\n\\n![Replaced hash](./img/request.webp)\\n\\nNow, if you visit Zillow and open that particular path that we tried for the extension, and go to the response section, the client-side receives the PersistedQueryNotFound error.\\n\\n![Persisted query error](./img/error.webp)\\n\\nThe front end of Zillow reacts with sending the whole query as a POST request.\\n\\n![POST request](./img/query.webp)\\n\\nWe extract the query and hash directly from this POST request. To ensure that the Zillow server does not forget about this hash, we periodically run this POST request with the exact same query and hash. This will ensure that the scraper continues to work even when the server\'s cache is cleaned or reset or the website changes.\\n\\n## Conclusion\\n\\nPersisted queries are a powerful optimization tool for GraphQL APIs, enhancing website performance by minimizing payload sizes and enabling GET request caching. However, they also pose significant challenges for web scraping, primarily due to the reliance on server-stored hashes and the potential for those hashes to become invalid.\\n\\nUsing `mitmproxy` to intercept and manipulate GraphQL requests gives an efficient approach to reveal  the full query text without delving into complex client-side JavaScript. By forcing the server to respond with a `PersistedQueryNotFound` error, we can capture the full query payload and utilize it for scraping purposes. Periodically running the extracted query ensures the scraper remains functional, even when server-side cache resets occur or the website evolves."},{"id":"web-scraping-tips","metadata":{"permalink":"/blog/web-scraping-tips","source":"@site/blog/2024/11-10-web-scraping-tips/index.md","title":"12 tips on how to think like a web scraping expert","description":"Learn how to think and scrape like a web scraping expert.","date":"2024-11-10T00:00:00.000Z","tags":[{"inline":true,"label":"community","permalink":"/blog/tags/community"}],"readingTime":11.67,"hasTruncateMarker":true,"authors":[{"name":"Max","title":"Community Member of Crawlee and web scraping expert","url":"https://github.com/Mantisus","socials":{"github":"https://github.com/Mantisus"},"imageURL":"https://avatars.githubusercontent.com/u/34358312?v=4","key":"MaxB","page":null}],"frontMatter":{"slug":"web-scraping-tips","title":"12 tips on how to think like a web scraping expert","tags":["community"],"description":"Learn how to think and scrape like a web scraping expert.","image":"./img/scraping-tips.webp","authors":["MaxB"]},"unlisted":false,"prevItem":{"title":"Reverse engineering GraphQL persistedQuery extension","permalink":"/blog/graphql-persisted-query"},"nextItem":{"title":"How to create a LinkedIn job scraper in Python with Crawlee","permalink":"/blog/linkedin-job-scraper-python"}},"content":"Typically, tutorials focus on the technical aspects, on what you can replicate: \\"Start here, follow this path, and you\'ll end up here.\\" This is great for learning a particular technology, but it\'s sometimes difficult to understand why the author decided to do things a certain way or what guides their development process.\\n\\n:::note\\n\\nOne of our community members wrote this blog as a contribution to Crawlee Blog. If you want to contribute blogs like these to Crawlee Blog, please reach out to us on our [discord channel](https://apify.com/discord).\\n\\n:::\\n\\nIn this blog, I\'ll discuss the general rules and principles that guide me when I work on web scraping projects and allow me to achieve great results.\\n\\nSo, let\'s explore the mindset of a web scraping developer.\\n\\n![How to think like a web scraping expert](./img/scraping-tips.webp)\\n\\n\x3c!-- truncate --\x3e\\n\\n## 1. Choosing a data source for the project\\n\\nWhen you start working on a project, you likely have a target site from which you need to extract specific data. Check what possibilities this site or application provides for data extraction. Here are some possible options:\\n\\n- `Official API` - the site may provide a free official API through which you can get all the necessary data. This is the best option for you. For example, you can consider this approach if you need to extract data from [`Yelp`](https://docs.developer.yelp.com/docs/fusion-intro)\\n- `Website` - in this case, we study the website, its structure, as well as the ways the frontend and backend interact\\n- `Mobile Application` - in some cases, there\'s no website or API at all, or the mobile application provides more data, in which case, don\'t forget about the [`man-in-the-middle`](https://blog.apify.com/using-a-man-in-the-middle-proxy-to-scrape-data-from-a-mobile-app-api-e954915f979d/) approach\\n\\nIf one data source fails, try accessing another available source.\\n\\nFor example, for `Yelp`, all three options are available, and if the `Official API` doesn\'t suit you for some reason, you can try the other two.\\n\\n## 2. Check [`robots.txt`](https://developers.google.com/search/docs/crawling-indexing/robots/intro) and [`sitemap`](https://developers.google.com/search/docs/crawling-indexing/sitemaps/build-sitemap)\\n\\nI think everyone knows about `robots.txt` and `sitemap` one way or another, but I regularly see people simply forgetting about them. If you\'re hearing about these for the first time, here\'s a quick explanation:\\n\\n- `robots` is the established name for crawlers in SEO. Usually, this refers to crawlers of major search engines like Google and Bing, or services like Ahrefs and ChatGPT.\\n- `robots.txt` is a file describing the allowed behavior for robots. It includes permitted crawler user-agents, wait time between page scans, patterns of pages forbidden for scanning, and more. These rules are typically based on which pages should be indexed by search engines and which should not.\\n- `sitemap` describes the site structure to make it easier for robots to navigate. It also helps in scanning only the content that needs updating, without creating unnecessary load on the site\\n\\nSince you\'re not [`Google`](http://google.com/) or any other popular search engine, the robot rules in `robots.txt` will likely be against you. But combined with the `sitemap`, this is a good place to study the site structure, expected interaction with robots, and non-browser user-agents. In some situations, it simplifies data extraction from the site.\\n\\nFor example, using the [`sitemap`](https://www.crawlee.dev/sitemap.xml) for [Crawlee website](http://www.crawlee.dev/), you can easily get direct links to posts both for the entire lifespan of the blog and for a specific period. One simple check, and you don\'t need to implement pagination logic.\\n\\n## 3. Don\'t neglect site analysis\\n\\nThorough site analysis is an important prerequisite for creating an effective web scraper, especially if you\'re not planning to use browser automation. However, such analysis takes time, sometimes a lot of it.\\n\\nIt\'s also worth noting that the time spent on analysis and searching for a more optimal crawling solution doesn\'t always pay off - you might spend hours only to discover that the most obvious approach was the best all along.\\n\\nTherefore, it\'s wise to set limits on your initial site analysis. If you don\'t see a better path within the allocated time, revert to simpler approaches. As you gain more experience, you\'ll more often be able to tell early on, based on the technologies used on the site, whether it\'s worth dedicating more time to analysis or not.\\n\\nAlso, in projects where you need to extract data from a site just once, thorough site analysis can sometimes eliminate the need to write scraper code altogether. Here\'s an example of such a site - `https://ricebyrice.com/nl/pages/find-store`.\\n\\n![Ricebyrice](./img/ricebyrice_base.webp)\\n\\nBy analyzing it, you\'ll easily discover that all the data can be obtained with a single request. You simply need to copy this data from your browser into a JSON file, and your task is complete.\\n\\n![Ricebyrice Response](./img/ricebyrice_response.webp)\\n\\n## 4. Maximum interactivity\\n\\nWhen analyzing a site, switch sorts, pages, interact with various elements of the site, while watching the `Network` tab in your browser\'s [Dev Tools](https://developer.chrome.com/docs/devtools). This will allow you to better understand how the site interacts with the backend, what framework it\'s built on, and what behavior can be expected from it.\\n\\n## 5. Data doesn\'t appear out of thin air\\n\\nThis is obvious, but it\'s important to keep in mind while working on a project. If you see some data or request parameters, it means they were obtained somewhere earlier, possibly in another request, possibly they may have already been on the website page, possibly they were formed using JS from other parameters. But they are always somewhere.\\n\\nIf you don\'t understand where the data on the page comes from, or the data used in a request, follow these steps:\\n\\n1. Sequentially, check all requests the site made before this point.\\n2. Examine their responses, headers, and cookies.\\n3. Use your intuition: Could this parameter be a timestamp? Could it be another parameter in a modified form?\\n4. Does it resemble any standard hashes or encodings?\\n\\nPractice makes perfect here. As you become familiar with different technologies, various frameworks, and their expected behaviors, and as you encounter a wide range of technologies, you\'ll find it easier to understand how things work and how data is transferred. This accumulated knowledge will significantly improve your ability to trace and understand data flow in web applications.\\n\\n## 6. Data is cached\\n\\nYou may notice that when opening the same page several times, the requests transmitted to the server differ: possibly something was cached and is already stored on your computer. Therefore, it\'s recommended to analyze the site in incognito mode, as well as switch browsers.\\n\\nThis situation is especially relevant for mobile applications, which may store some data in storage on the device. Therefore, when analyzing mobile applications, you may need to clear the cache and storage.\\n\\n## 7. Learn more about the framework\\n\\nIf during the analysis you discover that the site uses a framework you haven\'t encountered before, take some time to learn about it and its features. For example, if you notice a site is built with Next.js, understanding how it handles routing and data fetching could be crucial for your scraping strategy.\\n\\nYou can learn about these frameworks through official documentation or by using LLMs like [`ChatGPT`](https://openai.com/chatgpt/) or [`Claude`](https://claude.ai/). These AI assistants are excellent at explaining framework-specific concepts. Here\'s an example of how you might query an LLM about Next.js:\\n\\n```\\nI am in the process of optimizing my website using Next.js. Are there any files passed to the browser that describe all internal routing and how links are formed?\\n\\nRestrictions:\\n- Accompany your answers with code samples\\n- Use this message as the main message for all subsequent responses\\n- Reference only those elements that are available on the client side, without access to the project code base\\n\\n```\\n\\nYou can create similar queries for backend frameworks as well. For instance, with GraphQL, you might ask about available fields and query structures. These insights can help you understand how to better interact with the site\'s API and what data is potentially available.\\n\\nFor effective work with LLM, I recommend at least basically studying the basics of [`prompt engineering`](https://parlance-labs.com/education/prompt_eng/berryman.html).\\n\\n## 8. Reverse engineering\\n\\nWeb scraping goes hand in hand with reverse engineering. You study the interactions of the frontend and backend, you may need to study the code to better understand how certain parameters are formed.\\n\\nBut in some cases, reverse engineering may require more knowledge, effort, time, or have a high degree of complexity. At this point, you need to decide whether you need to delve into it or it\'s better to change the data source, or, for example, technologies. Most likely, this will be the moment when you decide to abandon HTTP web scraping and switch to a headless browser.\\n\\nThe main principle of most web scraping protections is not to make web scraping impossible, but to make it expensive.\\n\\nLet\'s just look at what the response to a search on [`zoopla`](https://www.zoopla.co.uk/) looks like\\n\\n![Zoopla Search Response](./img/zoopla_response.webp)\\n\\n## 9. Testing requests to endpoints\\n\\nAfter identifying the endpoints you need to extract the target data, make sure you get a correct response when making a request. If you get a response from the server other than 200, or data different from expected, then you need to figure out why. Here are some possible reasons:\\n\\n- You need to pass some parameters, for example cookies, or specific technical headers\\n- The site requires that when accessing this endpoint, there is a corresponding `Referrer` header\\n- The site expects that the headers will follow a certain order. I\'ve encountered this only a couple of times, but I have encountered it\\n- The site uses protection against web scraping, for example with `TLS fingerprint`\\n\\nAnd many other possible reasons, each of which requires separate analysis.\\n\\n## 10. Experiment with request parameters\\n\\nExplore what results you get when changing request parameters, if any. Some parameters may be missing but supported on the server side. For example, `order`, `sort`, `per_page`, `limit`, and others. Try adding them and see if the behavior changes.\\n\\nThis is especially relevant for sites using [`graphql`](https://graphql.org/)\\n\\nLet\'s consider this [`example`](https://restoran.ua/en/posts?subsection=0)\\n\\nIf you analyze the site, you\'ll see a request that can be reproduced with the following code, I\'ve formatted it a bit to improve readability:\\n\\n```python\\nimport requests\\n\\nurl = \\"https://restoran.ua/graphql\\"\\n\\ndata = {\\n    \\"operationName\\": \\"Posts_PostsForView\\",\\n    \\"variables\\": {\\"sort\\": {\\"sortBy\\": [\\"startAt_DESC\\"]}},\\n    \\"query\\": \\"\\"\\"query Posts_PostsForView(\\n    $where: PostForViewWhereInput,\\n    $sort: PostForViewSortInput,\\n    $pagination: PaginationInput,\\n    $search: String,\\n    $token: String,\\n    $coordinates_slice: SliceInput)\\n    {\\n        PostsForView(\\n                where: $where\\n                sort: $sort\\n                pagination: $pagination\\n                search: $search\\n                token: $token\\n                ) {\\n                        id\\n                        title: ukTitle\\n                        summary: ukSummary\\n                        slug\\n                        startAt\\n                        endAt\\n                        newsFeed\\n                        events\\n                        journal\\n                        toProfessionals\\n                        photoHeader {\\n                            address: mobile\\n                            __typename\\n                            }\\n                        coordinates(slice: $coordinates_slice) {\\n                            lng\\n                            lat\\n                            __typename\\n                            }\\n                        __typename\\n                    }\\n    }\\"\\"\\"\\n}\\n\\nresponse = requests.post(url, json=data)\\n\\nprint(response.json())\\n```\\n\\nNow I\'ll update it to get results in 2 languages at once, and most importantly, along with the internal text of the publications:\\n\\n```python\\nimport requests\\n\\nurl = \\"https://restoran.ua/graphql\\"\\n\\ndata = {\\n    \\"operationName\\": \\"Posts_PostsForView\\",\\n    \\"variables\\": {\\"sort\\": {\\"sortBy\\": [\\"startAt_DESC\\"]}},\\n    \\"query\\": \\"\\"\\"query Posts_PostsForView(\\n    $where: PostForViewWhereInput,\\n    $sort: PostForViewSortInput,\\n    $pagination: PaginationInput,\\n    $search: String,\\n    $token: String,\\n    $coordinates_slice: SliceInput)\\n    {\\n        PostsForView(\\n                where: $where\\n                sort: $sort\\n                pagination: $pagination\\n                search: $search\\n                token: $token\\n                ) {\\n                        id\\n                        # highlight-start\\n                        uk_title: ukTitle\\n                        en_title: enTitle\\n                        # highlight-end\\n                        summary: ukSummary\\n                        slug\\n                        startAt\\n                        endAt\\n                        newsFeed\\n                        events\\n                        journal\\n                        toProfessionals\\n                        photoHeader {\\n                            address: mobile\\n                            __typename\\n                            }\\n                        # highlight-start\\n                        mixedBlocks {\\n                            index\\n                            en_text: enText\\n                            uk_text: ukText\\n                            __typename\\n                            }\\n                        # highlight-end\\n                        coordinates(slice: $coordinates_slice) {\\n                            lng\\n                            lat\\n                            __typename\\n                            }\\n                        __typename\\n                    }\\n    }\\"\\"\\"\\n}\\n\\nresponse = requests.post(url, json=data)\\nprint(response.json())\\n```\\n\\nAs you can see, a small update of the request parameters allows me not to worry about visiting the internal page of each publication. You have no idea how many times this trick has saved me.\\n\\nIf you see `graphql` in front of you and don\'t know where to start, then my advice about documentation and LLM works here too.\\n\\n## 11. Don\'t be afraid of new technologies\\n\\nI know how easy it is to master a few tools and just use them because it works. I\'ve fallen into this trap more than once myself.\\n\\nBut modern sites use modern technologies that have a significant impact on web scraping, and in response, new tools for web scraping are emerging. Learning these may greatly simplify your next project, and may even solve some problems that were insurmountable for you. I wrote about some tools [`earlier`](https://www.crawlee.dev/blog/common-problems-in-web-scraping).\\n\\nI especially recommend paying attention to [`curl_cffi`](https://curl-cffi.readthedocs.io/en/latest/) and frameworks\\n[`botasaurus`](https://www.omkar.cloud/botasaurus/) and [`Crawlee for Python`](https://www.crawlee.dev/python/).\\n\\n## 12. Help open-source libraries\\n\\nPersonally, I only recently came to realize the importance of this. All the tools I use for my work are either open-source developments or based on open-source. Web scraping literally lives thanks to open-source, and this is especially noticeable if you\'re a `Python` developer and have realized that on pure `Python` everything is quite sad when you need to deal with `TLS fingerprint`, and again, open-source saved us here.\\n\\nAnd it seems to me that the least we could do is invest a little of our knowledge and skills in supporting open-source.\\n\\nI chose to support [`Crawlee for Python`](https://www.crawlee.dev/python/), and no, not because they allowed me to write in their blog, but because it shows excellent development dynamics and is aimed at making life easier for web crawler developers. It allows for faster crawler development by taking care of and hiding under the hood such critical aspects as session management, session rotation when blocked, managing concurrency of asynchronous tasks (if you write asynchronous code, you know what a pain this can be), and much more.\\n\\n:::tip\\nIf you like the blog so far, please consider [giving Crawlee a star on GitHub](https://github.com/apify/crawlee), it helps us to reach and help more developers.\\n:::\\n\\nAnd what choice will you make?\\n\\n## Conclusion\\n\\nI think some things in the article were obvious to you, some things you follow yourself, but I hope you learned something new too. If most of them were new, then try using these rules as a checklist in your next project.\\n\\nI would be happy to discuss the article. Feel free to comment here, in the article, or contact me in the [Crawlee developer community](https://apify.com/discord) on Discord.\\n\\nYou can also find me on the following platforms: [Github](https://github.com/Mantisus), [Linkedin](https://www.linkedin.com/in/max-bohomolov/), [Apify](https://apify.com/mantisus), [Upwork](https://www.upwork.com/freelancers/mantisus), [Contra](https://contra.com/mantisus).\\n\\nThank you for your attention :)"},{"id":"linkedin-job-scraper-python","metadata":{"permalink":"/blog/linkedin-job-scraper-python","source":"@site/blog/2024/10-14-linkedin-job-scraper-python/index.md","title":"How to create a LinkedIn job scraper in Python with Crawlee","description":"Learn how to scrape LinkedIn jobs and save it into a CSV file using Python.","date":"2024-10-14T00:00:00.000Z","tags":[],"readingTime":6.265,"hasTruncateMarker":true,"authors":[{"name":"Arindam Majumder","title":"Community Member of Crawlee","url":"https://github.com/Arindam200","socials":{"x":"https://x.com/Arindam_1729","github":"https://github.com/Arindam200"},"imageURL":"https://avatars.githubusercontent.com/u/109217591?v=4","key":"ArindamM","page":null}],"frontMatter":{"slug":"linkedin-job-scraper-python","title":"How to create a LinkedIn job scraper in Python with Crawlee","description":"Learn how to scrape LinkedIn jobs and save it into a CSV file using Python.","image":"./img/linkedin-job-scraper.webp","authors":["ArindamM"]},"unlisted":false,"prevItem":{"title":"12 tips on how to think like a web scraping expert","permalink":"/blog/web-scraping-tips"},"nextItem":{"title":"Optimizing web scraping: Scraping auth data using JSDOM","permalink":"/blog/scrape-using-jsdom"}},"content":"## Introduction\\n\\nIn this article, we will build a web application that scrapes LinkedIn for job postings using Crawlee and Streamlit.\\n\\nWe will create a LinkedIn job scraper in Python using Crawlee for Python to extract the company name, job title, time of posting, and link to the job posting from dynamically received user input through the web application.\\n\\n:::note\\n\\nOne of our community members wrote this blog as a contribution to Crawlee Blog. If you want to contribute blogs like these to Crawlee Blog, please reach out to us on our [discord channel](https://apify.com/discord).\\n\\n:::\\n\\nBy the end of this tutorial, you\u2019ll have a fully functional web application that you can use to scrape job postings from LinkedIn.\\n\\n![Linkedin Job Scraper](./img/linkedin-job-scraper.webp)\\n\\nLet\'s begin.\\n\\n\x3c!-- truncate --\x3e\\n\\n## Prerequisites\\n\\nLet\'s start by creating a new Crawlee for Python project with this command:\\n\\n```bash\\npipx run crawlee create linkedin-scraper\\n```\\n\\nSelect `PlaywrightCrawler` in the terminal when Crawlee asks for it.\\n\\nAfter installation, Crawlee for Python will create boilerplate code for you. You can change the directory(`cd`) to the project folder and run this command to install dependencies.\\n\\n```bash\\npoetry install\\n```\\n\\nWe are going to begin editing the files provided to us by Crawlee so we can build our scraper.\\n\\n:::note\\n\\nBefore going ahead if you like reading this blog, we would be really happy if you gave [Crawlee for Python a star on GitHub](https://github.com/apify/crawlee-python/)!\\n\\n:::\\n\\n## Building the LinkedIn job Scraper in Python with Crawlee\\n\\nIn this section, we will be building the scraper using the Crawlee for Python package. To learn more about Crawlee, check out their [documentation](https://www.crawlee.dev/python/docs/quick-start).\\n\\n### 1. Inspecting the LinkedIn job Search Page\\n\\nOpen LinkedIn in your web browser and sign out from the website (if you already have an account logged in). You should see an interface like this.\\n\\n![LinkedIn Homepage](./img/linkedin-homepage.webp)\\n\\nNavigate to the jobs section, search for a job and location of your choice, and copy the URL.\\n\\n![LinkedIn Jobs Page](./img/linkedin-jobs.webp)\\n\\nYou should have something like this:\\n\\n`https://www.linkedin.com/jobs/search?keywords=Backend%20Developer&location=Canada&geoId=101174742&trk=public_jobs_jobs-search-bar_search-submit&position=1&pageNum=0`\\n\\nWe\'re going to focus on the search parameters, which is the part that goes after \'?\'. The keyword and location parameters are the most important ones for us.\\n\\nThe job title the user supplies will be input to the keyword parameter, while the location the user supplies will go into the location parameter. Lastly, the `geoId` parameter will be removed while we keep the other parameters constant.\\n\\nWe are going to be making changes to our `main.py` file. Copy and paste the code below in your `main.py` file.\\n\\n```py\\nfrom crawlee.playwright_crawler import PlaywrightCrawler\\nfrom .routes import router\\nimport urllib.parse\\n\\nasync def main(title: str, location: str, data_name: str) -> None:\\n    base_url = \\"https://www.linkedin.com/jobs/search\\"\\n\\n    # URL encode the parameters\\n    params = {\\n        \\"keywords\\": title,\\n        \\"location\\": location,\\n        \\"trk\\": \\"public_jobs_jobs-search-bar_search-submit\\",\\n        \\"position\\": \\"1\\",\\n        \\"pageNum\\": \\"0\\"\\n    }\\n\\n    encoded_params = urlencode(params)\\n\\n    # Encode parameters into a query string\\n    query_string = \'?\' + encoded_params\\n\\n    # Combine base URL with the encoded query string\\n    encoded_url = urljoin(base_url, \\"\\") + query_string\\n\\n    # Initialize the crawler\\n    crawler = PlaywrightCrawler(\\n        request_handler=router,\\n    )\\n\\n    # Run the crawler with the initial list of URLs\\n    await crawler.run([encoded_url])\\n\\n    # Save the data in a CSV file\\n    output_file = f\\"{data_name}.csv\\"\\n    await crawler.export_data(output_file)\\n```\\n\\nNow that we have encoded the URL, the next step for us is to adjust the generated router to handle LinkedIn job postings.\\n\\n### 2. Routing your crawler\\n\\nWe will be making use of two handlers for your application:\\n\\n* **Default handler**\\n\\nThe `default_handler` handles the start URL\\n\\n* **Job listing**\\n\\nThe `job_listing` handler extracts the individual job details.\\n\\nPlaywright crawler is going to crawl through the job posting page and extract the links to all job postings on the page.\\n\\n![Identifying elements](./img/elements.webp)\\n\\nWhen you examine the job postings, you will discover that the job posting links are inside an ordered list with a class named `jobs-search__results-list`. We will then extract the links using the Playwright locator object and add them to the `job_listing` route for processing.\\n\\n```py\\nrouter = Router[PlaywrightCrawlingContext]()\\n\\n@router.default_handler\\nasync def default_handler(context: PlaywrightCrawlingContext) -> None:\\n    \\"\\"\\"Default request handler.\\"\\"\\"\\n\\n    #select all the links for the job posting on the page\\n    hrefs = await context.page.locator(\'ul.jobs-search__results-list a\').evaluate_all(\\"links => links.map(link => link.href)\\")\\n\\n    #add all the links to the job listing route\\n    await context.add_requests(\\n            [Request.from_url(rec, label=\'job_listing\') for rec in hrefs]\\n        )\\n```\\n\\nNow that we have the job listings, the next step is to scrape their details.\\n\\nWe\'ll extract each job\u2019s title, company\'s name, time of posting, and the link to the job post. Open your dev tools to extract each element using its CSS selector.\\n\\n![Inspecting elements](./img/inspect.webp)\\n\\nAfter scraping each of the listings, we\'ll remove special characters from the text to make it clean and push the data to local storage using the `context.push_data` function.\\n\\n```py\\n@router.handler(\'job_listing\')\\nasync def listing_handler(context: PlaywrightCrawlingContext) -> None:\\n    \\"\\"\\"Handler for job listings.\\"\\"\\"\\n\\n    await context.page.wait_for_load_state(\'load\')\\n\\n    job_title = await context.page.locator(\'div.top-card-layout__entity-info h1.top-card-layout__title\').text_content()\\n\\n    company_name  = await context.page.locator(\'span.topcard__flavor a\').text_content()\\n\\n    time_of_posting= await context.page.locator(\'div.topcard__flavor-row span.posted-time-ago__text\').text_content()\\n\\n\\n    await context.push_data(\\n        {\\n            # we are making use of regex to remove special characters for the extracted texts\\n\\n            \'title\': re.sub(r\'[\\\\s\\\\n]+\', \'\', job_title),\\n            \'Company name\': re.sub(r\'[\\\\s\\\\n]+\', \'\', company_name),\\n            \'Time of posting\': re.sub(r\'[\\\\s\\\\n]+\', \'\', time_of_posting),\\n            \'url\': context.request.loaded_url,\\n        }\\n    )\\n```\\n\\n## 3. Creating your application\\n\\nFor this project, we will be using Streamlit for the web application. Before we proceed, we are going to create a new file named `app.py` in your project directory. In addition, ensure you have  [Streamlit](https://docs.streamlit.io/get-started/installation)  installed in your global Python environment before proceeding with this section.\\n\\n```py\\nimport streamlit as st\\nimport subprocess\\n\\n# Streamlit form for inputs\\nst.title(\\"LinkedIn Job Scraper\\")\\n\\nwith st.form(\\"scraper_form\\"):\\n    title = st.text_input(\\"Job Title\\", value=\\"backend developer\\")\\n    location = st.text_input(\\"Job Location\\", value=\\"newyork\\")\\n    data_name = st.text_input(\\"Output File Name\\", value=\\"backend_jobs\\")\\n\\n    submit_button = st.form_submit_button(\\"Run Scraper\\")\\n\\nif submit_button:\\n\\n    # Run the scraping script with the form inputs\\n    command = f\\"\\"\\"poetry run python -m linkedin-scraper --title \\"{title}\\"  --location \\"{location}\\" --data_name \\"{data_name}\\" \\"\\"\\"\\n\\n    with st.spinner(\\"Crawling in progress...\\"):\\n         # Execute the command and display the results\\n        result = subprocess.run(command, shell=True, capture_output=True, text=True)\\n\\n        st.write(\\"Script Output:\\")\\n        st.text(result.stdout)\\n\\n        if result.returncode == 0:\\n            st.success(f\\"Data successfully saved in {data_name}.csv\\")\\n        else:\\n            st.error(f\\"Error: {result.stderr}\\")\\n```\\n\\nThe Streamlit web application takes in the user\'s input and uses the Python Subprocess package to run the Crawlee scraping script.\\n\\n\\n## 4. Testing your app\\n\\nBefore we test the application, we need to make a little modification to the `__main__` file in order for it to accommodate the command line arguments.\\n\\n```py\\nimport asyncio\\nimport argparse\\n\\nfrom .main import main\\n\\ndef get_args():\\n    # ArgumentParser object to capture command-line arguments\\n    parser = argparse.ArgumentParser(description=\\"Crawl LinkedIn job listings\\")\\n\\n\\n    # Define the arguments\\n    parser.add_argument(\\"--title\\", type=str, required=True, help=\\"Job title\\")\\n    parser.add_argument(\\"--location\\", type=str, required=True, help=\\"Job location\\")\\n    parser.add_argument(\\"--data_name\\", type=str, required=True, help=\\"Name for the output CSV file\\")\\n\\n\\n    # Parse the arguments\\n    return parser.parse_args()\\n\\nif __name__ == \'__main__\':\\n    args = get_args()\\n    # Run the main function with the parsed command-line arguments\\n    asyncio.run(main(args.title, args.location, args.data_name))\\n```\\n\\nWe will start the Streamlit application by running this code in the terminal:\\n\\n```bash\\nstreamlit run app.py\\n```\\n\\nThis is what your application what the application should look like on the browser:\\n\\n![Running scraper](./img/running.webp)\\n\\nYou will get this interface showing you that the scraping has been completed:\\n\\n![Filling input form](./img/form.webp)\\n\\nTo access the scraped data, go over to your project directory and open the CSV file.\\n\\n![CSV file with all scraped LinkedIn jobs](./img/excel.webp)\\n\\nYou should have something like this as the output of your CSV file.\\n\\n\\n## Conclusion\\n\\nIn this tutorial, we have learned how to build an application that can scrape job posting data from LinkedIn using Crawlee. Have fun building great scraping applications with Crawlee.\\n\\nYou can find the complete working Crawler code here on the [GitHub repository.](https://github.com/Arindam200/LinkedIn_Scraping)"},{"id":"scrape-using-jsdom","metadata":{"permalink":"/blog/scrape-using-jsdom","source":"@site/blog/2024/09-30-jsdom-based-scraping/index.md","title":"Optimizing web scraping: Scraping auth data using JSDOM","description":"Learn how to scrape using JSDOM, alternative to Cheerio and browser based scraping.","date":"2024-09-30T00:00:00.000Z","tags":[],"readingTime":7.285,"hasTruncateMarker":true,"authors":[{"name":"Saurav Jain","title":"Developer Community Manager","url":"https://github.com/souravjain540","socials":{"x":"https://x.com/sauain","github":"https://github.com/souravjain540"},"imageURL":"https://avatars.githubusercontent.com/u/53312820?v=4","key":"SauravJ","page":null}],"frontMatter":{"slug":"scrape-using-jsdom","title":"Optimizing web scraping: Scraping auth data using JSDOM","description":"Learn how to scrape using JSDOM, alternative to Cheerio and browser based scraping.","image":"./img/jsdom.webp","authors":["SauravJ"]},"unlisted":false,"prevItem":{"title":"How to create a LinkedIn job scraper in Python with Crawlee","permalink":"/blog/linkedin-job-scraper-python"},"nextItem":{"title":"Web scraping of a dynamic website using Python with HTTP Client","permalink":"/blog/scraping-dynamic-websites-using-python"}},"content":"As scraping developers, we sometimes need to extract authentication data like temporary keys to perform our tasks. However, it is not as simple as that. Usually, it is in HTML or XHR network requests, but sometimes, the auth data is computed. In that case, we can either reverse-engineer the computation, which takes a lot of time to deobfuscate scripts or run the JavaScript that computes it. Normally, we use a browser, but that is expensive. Crawlee provides support for running browser scraper and Cheerio Scraper in parallel, but that is very complex and expensive in terms of compute resource usage. JSDOM helps us run page JavaScript with fewer resources than a browser and slightly higher than Cheerio.\\n\\nThis article will discuss a new approach that we use in one of our Actors to obtain the authentication data from TikTok ads creative center generated by browser web applications without actually running the browser but instead of it, using JSDOM.\\n\\n![JSDOM based approach from scraping](./img/jsdom.webp)\\n\\n\x3c!-- truncate --\x3e\\n\\n## Analyzing the website\\n\\nWhen you visit this URL:\\n\\n`https://ads.tiktok.com/business/creativecenter/inspiration/popular/hashtag/pc/en`\\n\\nYou will see a list of hashtags with their live ranking, the number of posts they have, trend chart, creators, and analytics. You can also notice that we can filter the industry, set the time period, and use a check box to filter if the trend is new to the top 100 or not.\\n\\n![tiktok-trends](./img/tiktok-trends.webp)\\n\\nOur goal here is to extract the top 100 hashtags from the list with the given filters.\\n\\nThe two possible approaches are to use [`CheerioCrawler`](https://crawlee.dev/docs/guides/cheerio-crawler-guide), and the second one will be browser-based scraping. Cheerio gives results faster but does not work with JavaScript-rendered websites.\\n\\nCheerio is not the best option here as the [Creative Center](https://ads.tiktok.com/business/creativecenter/inspiration/popular/hashtag/pad/en) is a web application, and the data source is API, so we can only get the hashtags initially present in the HTML structure but not each of the 100 as we require.\\n\\nThe second approach can be using libraries like Puppeteer, Playwright, etc, to do browser-based scraping and using automation to scrape all of the hashtags, but with previous experiences, it takes a lot of time for such a small task.\\n\\nNow comes the new approach that we developed to make this process a lot better than browser based and very close to CheerioCrawler based crawling.\\n\\n## JSDOM Approach\\n\\n:::note\\n\\nBefore diving deep into this approach, I would like to give credit to [Alexey Udovydchenko](https://apify.com/alexey), Web Automation Engineer at Apify, for developing this approach. Kudos to him!\\n\\n:::\\n\\nIn this approach, we are going to make API calls to `https://ads.tiktok.com/creative_radar_api/v1/popular_trend/hashtag/list` to get the required data.\\n\\nBefore making calls to this API, we will need few required headers (auth data), so we will first make the call to `https://ads.tiktok.com/business/creativecenter/inspiration/popular/hashtag/pad/en`.\\n\\nWe will start this approach by creating a function that will create the URL for the API call for us and, make the call and get the data.\\n\\n```js\\nexport const createStartUrls = (input) => {\\n    const {\\n        days = \'7\',\\n        country = \'\',\\n        resultsLimit = 100,\\n        industry = \'\',\\n        isNewToTop100,\\n    } = input;\\n\\n    const filterBy = isNewToTop100 ? \'new_on_board\' : \'\';\\n    return [\\n        {\\n            url: `https://ads.tiktok.com/creative_radar_api/v1/popular_trend/hashtag/list?page=1&limit=50&period=${days}&country_code=${country}&filter_by=${filterBy}&sort_by=popular&industry_id=${industry}`,\\n            headers: {\\n                // required headers\\n            },\\n            userData: { resultsLimit },\\n        },\\n    ];\\n};\\n```\\n\\nIn the above function, we create the start url for the API call that include various parameters as we talked about earlier. After creating the URL according to the parameters it will call the `creative_radar_api` and fetch all the results.\\n\\nBut it won\u2019t work until we get the headers. So, let\u2019s create a function that will first create a session using `sessionPool` and `proxyConfiguration`.\\n\\n```js\\nexport const createSessionFunction = async (\\n    sessionPool,\\n    proxyConfiguration,\\n) => {\\n    const proxyUrl = await proxyConfiguration.newUrl(Math.random().toString());\\n    const url =\\n        \'https://ads.tiktok.com/business/creativecenter/inspiration/popular/hashtag/pad/en\';\\n    // need url with data to generate token\\n    const response = await gotScraping({ url, proxyUrl });\\n    const headers = await getApiUrlWithVerificationToken(\\n        response.body.toString(),\\n        url,\\n    );\\n    if (!headers) {\\n        throw new Error(`Token generation blocked`);\\n    }\\n    log.info(`Generated API verification headers`, Object.values(headers));\\n    return new Session({\\n        userData: {\\n            headers,\\n        },\\n        sessionPool,\\n    });\\n};\\n```\\n\\nIn this function, the main goal is to call `https://ads.tiktok.com/business/creativecenter/inspiration/popular/hashtag/pad/en` and get headers in return. To get the headers we are using `getApiUrlWithVerificationToken` function.\\n\\n:::note\\n\\nBefore going ahead, I want to mention that Crawlee natively supports JSDOM using the [JSDOM Crawler](https://crawlee.dev/api/jsdom-crawler). It gives a framework for the parallel crawling of web pages using plain HTTP requests and jsdom DOM implementation. It uses raw HTTP requests to download web pages, it is very fast and efficient on data bandwidth.\\n\\n:::\\n\\nLet\u2019s see how we are going to create the `getApiUrlWithVerificationToken` function:\\n\\n```js\\nconst getApiUrlWithVerificationToken = async (body, url) => {\\n    log.info(`Getting API session`);\\n    const virtualConsole = new VirtualConsole();\\n    const { window } = new JSDOM(body, {\\n        url,\\n        contentType: \'text/html\',\\n        runScripts: \'dangerously\',\\n        resources: \'usable\' || new CustomResourceLoader(),\\n        // ^ \'usable\' faster than custom and works without canvas\\n        pretendToBeVisual: false,\\n        virtualConsole,\\n    });\\n    virtualConsole.on(\'error\', () => {\\n        // ignore errors cause by fake XMLHttpRequest\\n    });\\n\\n    const apiHeaderKeys = [\'anonymous-user-id\', \'timestamp\', \'user-sign\'];\\n    const apiValues = {};\\n    let retries = 10;\\n    // api calls made outside of fetch, hack below is to get URL without actual call\\n    window.XMLHttpRequest.prototype.setRequestHeader = (name, value) => {\\n        if (apiHeaderKeys.includes(name)) {\\n            apiValues[name] = value;\\n        }\\n        if (Object.values(apiValues).length === apiHeaderKeys.length) {\\n            retries = 0;\\n        }\\n    };\\n    window.XMLHttpRequest.prototype.open = (method, urlToOpen) => {\\n        if (\\n            [\'static\', \'scontent\'].find((x) =>\\n                urlToOpen.startsWith(`https://${x}`),\\n            )\\n        )\\n        log.debug(\'urlToOpen\', urlToOpen);\\n    };\\n    do {\\n        await sleep(4000);\\n        retries--;\\n    } while (retries > 0);\\n\\n    await window.close();\\n    return apiValues;\\n};\\n```\\n\\nIn this function, we are creating a virtual console that uses `CustomResourceLoader` to run the background process and replace the browser with JSDOM.\\n\\nFor this particular example, we need three mandatory headers to make the API call, and those are `anonymous-user-id,` `timestamp,` and `user-sign.`\\n\\nUsing `XMLHttpRequest.prototype.setRequestHeader`, we are checking if the mentioned headers are in the response or not, if yeas, we take the value of those headers, and repeat the retries until we get all the headers.\\n\\nThen, the most important part is that we use `XMLHttpRequest.prototype.open` to extract the auth data and make calls without actually using browsers or exposing the bot activity.\\n\\nAt the end of `createSessionFunction`, it returns a session with the required headers.\\n\\nNow coming to our main code, we will use CheerioCrawler and will use `prenavigationHooks` to inject the headers that we got from the earlier function into the `requestHandler`.\\n\\n```js\\nconst crawler = new CheerioCrawler({\\n    sessionPoolOptions: {\\n        maxPoolSize: 1,\\n        createSessionFunction: async (sessionPool) =>\\n            createSessionFunction(sessionPool, proxyConfiguration),\\n    },\\n    preNavigationHooks: [\\n        (crawlingContext) => {\\n            const { request, session } = crawlingContext;\\n            request.headers = {\\n                ...request.headers,\\n                ...session.userData?.headers,\\n            };\\n        },\\n    ],\\n    proxyConfiguration,\\n});\\n```\\n\\nFinally in the request handler we make the call using the headers and make sure how many calls are needed to fetch all the data handling pagination.\\n\\n```js\\nasync requestHandler(context) {\\n    const { log, request, json } = context;\\n    const { userData } = request;\\n    const { itemsCounter = 0, resultsLimit = 0 } = userData;\\n    if (!json.data) {\\n        throw new Error(\'BLOCKED\');\\n    }\\n    const { data } = json;\\n    const items = data.list;\\n    const counter = itemsCounter + items.length;\\n    const dataItems = items.slice(\\n        0,\\n        resultsLimit && counter > resultsLimit\\n            ? resultsLimit - itemsCounter\\n            : undefined,\\n    );\\n    await context.pushData(dataItems);\\n    const {\\n        pagination: { page, total },\\n    } = data;\\n    log.info(\\n        `Scraped ${dataItems.length} results out of ${total} from search page ${page}`,\\n    );\\n    const isResultsLimitNotReached =\\n        counter < Math.min(total, resultsLimit);\\n    if (isResultsLimitNotReached && data.pagination.has_more) {\\n        const nextUrl = new URL(request.url);\\n        nextUrl.searchParams.set(\'page\', page + 1);\\n        await crawler.addRequests([\\n            {\\n                url: nextUrl.toString(),\\n                headers: request.headers,\\n                userData: {\\n                    ...request.userData,\\n                    itemsCounter: itemsCounter + dataItems.length,\\n                },\\n            },\\n        ]);\\n    }\\n}\\n```\\n\\nOne important thing to note here is that we are making this code in a way that we can make any numbers of API calls.\\n\\nIn this particular example we just made one request and a single session, but you can make more if you need. When the first API call will be completed, it will create the second API call. Again, you can make more calls if needed, but we stopped at two.\\n\\nTo make things more clear, here is how code flow looks:\\n\\n![code flow](./img/code-flow.webp)\\n\\n## Conclusion\\n\\nThis approach helps us to get a third way to extract the authentication data without actually using a browser and pass the data to CheerioCrawler. This significantly improves the performance and reduces the RAM requirement by 50%, and while browser-based scraping performance is ten times slower than pure Cheerio, JSDOM does it just 3-4 times slower, which makes it 2-3 times faster than browser-based scraping.\\n\\nThe project\'s codebase is already [uploaded here](https://github.com/souravjain540/tiktok-trends). The code is written as an Apify Actor; you can find more about it [here](https://docs.apify.com/academy/getting-started/creating-actors), but you can also run it without using Apify SDK.\\n\\nIf you have any doubts or questions about this approach, reach out to us on our [Discord server](https://apify.com/discord)."},{"id":"scraping-dynamic-websites-using-python","metadata":{"permalink":"/blog/scraping-dynamic-websites-using-python","source":"@site/blog/2024/09-12-finding-students-accommodations/index.md","title":"Web scraping of a dynamic website using Python with HTTP Client","description":"Learn how to scrape dynamic websites using Crawlee for Python with HTTP client.","date":"2024-09-12T00:00:00.000Z","tags":[{"inline":true,"label":"community","permalink":"/blog/tags/community"}],"readingTime":12.66,"hasTruncateMarker":true,"authors":[{"name":"Max","title":"Community Member of Crawlee and web scraping expert","url":"https://github.com/Mantisus","socials":{"github":"https://github.com/Mantisus"},"imageURL":"https://avatars.githubusercontent.com/u/34358312?v=4","key":"MaxB","page":null}],"frontMatter":{"slug":"scraping-dynamic-websites-using-python","title":"Web scraping of a dynamic website using Python with HTTP Client","tags":["community"],"description":"Learn how to scrape dynamic websites using Crawlee for Python with HTTP client.","image":"./img/dynamic-websites.webp","authors":["MaxB"]},"unlisted":false,"prevItem":{"title":"Optimizing web scraping: Scraping auth data using JSDOM","permalink":"/blog/scrape-using-jsdom"},"nextItem":{"title":"How to scrape infinite scrolling webpages with Python","permalink":"/blog/infinite-scroll-using-python"}},"content":"Dynamic websites that use JavaScript for content rendering and backend interaction often create challenges for web scraping. The traditional approach to solving this problem is browser emulation, but it\'s not very efficient in terms of resource consumption.\\n\\n:::note\\n\\nOne of our community members wrote this blog as a contribution to Crawlee Blog. If you want to contribute blogs like these to Crawlee Blog, please reach out to us on our [discord channel](https://apify.com/discord).\\n\\n:::\\n\\nIn this article, we\'ll explore an alternative method based on in-depth site analysis and the use of an HTTP client. We\'ll go through the entire process from analyzing a dynamic website to implementing an efficient web crawler using the [`Crawlee for Python`](https://www.crawlee.dev/python/) framework.\\n\\n![How to scrape dynamic websites in Python](./img/dynamic-websites.webp)\\n\\n\x3c!-- truncate --\x3e\\n\\n## What you\'ll learn in this tutorial\\n\\nOur subject of study is the  [Accommodation for Students](https://www.accommodationforstudents.com)  website. Using this example, we\'ll examine the specifics of analyzing sites built with the Next.js framework and implement a crawler capable of efficiently extracting data without using browser emulation.\\n\\nBy the end of this article, you will have:\\n- A clear understanding of how to analyze sites with dynamic content rendered using JavaScript.\\n- How to implement a crawler based on Crawlee for Python.\\n- Insight into some of the details of working with sites that use [`Next.js`](https://nextjs.org/).\\n- A link to a GitHub repository with the full crawler implementation code.\\n\\n## Website analysis\\n\\nTo track all requests, open your Dev Tools and the `network` tab before entering the site. Some data may be transmitted only once the site is first opened.\\n\\nAs the site is intended for students in the UK, let\'s go to London. We\'ll start the analysis from the [search page](https://www.accommodationforstudents.com/search-results?location=London&beds=0&occupancy=min&minPrice=0&maxPrice=500&latitude=51.509865&longitude=-0.118092&geo=false&page=1)\\n\\nInteracting with elements on the site page, you\'ll quickly notice a request of this type:\\n\\n```plaintext\\nhttps://www.accommodationforstudents.com/search?limit=22&skip=0&random=false&mode=text&numberOfBedrooms=0&occupancy=min&countryCode=gb&location=London&sortBy=price&order=asc\\n```\\n\\n![Request type](./img/request.webp)\\n\\nIf we look at the format of the received response, we\'ll immediately notice that it comes in [`JSON`](https://www.json.org/json-en.html) format.\\n\\n![JSON reposonse](./img/json.webp)\\n\\nGreat, we\'re getting data in a structured format that\'s very convenient to work with. We see the total number of results links to listings are in the `url` attribute for each `properties` element\\n\\nLet\'s also take a look at the server response headers.\\n\\n![server response](./img/server.webp)\\n\\n- `content-type: application/json; charset=utf-8` - It tells us that the server response comes in JSON format, which we\'ve already confirmed visually.\\n- `content-encoding: gzip` - It tells us that the response was compressed using [`gzip`](https://www.gnu.org/software/gzip/), and therefore we should use appropriate decompression in our crawler.\\n- `server: cloudflare` - The site is hosted on [\u0421loudflare](https://www.cloudflare.com/) servers and uses their protection. We should consider this when creating our crawler.\\n\\nGreat, let\'s also look at the parameters used in the search API request and make hypotheses about what they\'re responsible for:\\n\\n- `limit: 22` - The number of elements we get per request.\\n- `skip: 0` - The element from which we\'ll start getting important data for pagination.\\n- `random: false` - We don\'t change the random sorting as we benefit from strict sorting.\\n- `mode: text` - An unusual parameter. If you decide to conduct several experiments, you\'ll find that it can take the following values: text, fallback, geo. - Interestingly, the geo parameter completely changes the output, returning about 5400 options. I assume it\'s necessary to search by coordinates, and if we don\'t pass any coordinates, we get all the available results.\\n- `numberOfBedrooms: 0 `- filter by bedrooms.\\n- `occupancy: min` - filter by occupancy.\\n- `countryCode: gb` - country code, in our case it\'s Great Britain\\n- `location: London` - search location\\n- `sortBy: price` - the field by which sorting is performed\\n- `order: asc` - type of sorting\\n\\nBut there\'s another important point to pay attention to. Let\'s look at our link in the browser bar, which looks like this:\\n\\n```plaintext\\nhttps://www.accommodationforstudents.com/search-results?location=London&beds=0&occupancy=min&minPrice=0&maxPrice=500&latitude=51.509865&longitude=-0.118092&geo=false&page=1\\n```\\n\\nIn it, we see the coordinate parameters `latitude` and `longitude`, which don\'t participate in any way when interacting with the backend, and the `geo` parameter with a false value. This also confirms our hypothesis regarding the mode parameter. This is quite useful if you want to extract all data from the site.\\n\\nGreat. We can get the site\'s search data in a convenient JSON format. We also have flexible parameters to guarantee data extraction, whether all are available on the site or for a specific city.\\n\\nLet\'s move on to analyzing the property page.\\n\\nSince after clicking on the listing it opens in a new window, make sure you have `Auto-open DevTools for popups` option set in Dev Tools\\n\\nUnfortunately, we don\'t see any interesting interaction with the backend after analyzing all requests. All listing data is obtained in one request containing HTML code and JSON elements.\\n\\n![Listing data contained in HTML code and JSON elements](./img/listing.webp)\\n\\nAfter carefully studying the page\'s source code, we can say that all the data we\'re interested in is in the JSON located in the `script` tag, which has an `id` attribute with the value `__NEXT_DATA__`. We can easily extract this JSON using a regular expression or HTML parser.\\n\\nWe already have everything necessary to build the crawler at this analysis stage. We know how to get data from the search, how pagination works, how to go from the search to the listing page, and where to extract the data we\'re interested in on the listing page.\\n\\nBut there\'s one obvious inconvenience: we get search data in JSON, and listing data we get in HTML inside, which is JSON. This isn\'t a problem but rather an inconvenience and higher traffic consumption, as such an HTML page will weigh much more than just JSON.\\n\\nLet\'s continue our analysis.\\n\\nThe data in `__NEXT_DATA__` signals that the site uses the Next.js framework. Each framework has its own established internal patterns, parameters, and features.\\n\\nLet\'s analyze the listing page again by refreshing it and analyzing the `.js` files we receive.\\n\\n![Javascript files](./img/javascript.webp)\\n\\nWe\'re interested in the file containing `_buildManifest.js` in its name, the link to it will regularly change, so I\'ll provide an example:\\n\\n```plaintext\\nhttps://www.accommodationforstudents.com/_next/static/B5yLvSqNOvFysuIu10hQ5/_buildManifest.js\\n```\\n\\nThis file contains all possible routes available on the site. After careful study, we can see a link format like `/property/[id]`, which is clearly related to the property page. After reading more about Next.js, we can get the final link\u2014`https://www.accommodationforstudents.com/_next/data/[build_id]/property/[id].json`.\\n\\nThis link has two variables:\\n\\n1. `build_id` - the current build of the `Next.js` application, it can be obtained from `__NEXT_DATA__` on any application page. In the example link for `_buildManifest.js`, its value is `B5yLvSqNOvFysuIu10hQ5`\\n2. `id` - the identifier for the property object whose data we\'re interested in.\\n\\nLet\'s form a link and study the result in the browser.\\n\\n![Study the result in browser](./img/result.webp)\\n\\n\\nAs you can see, now we get the listing results in JSON format. But after all, `Next.js` works for search, so let\'s get a link for it, so that our future crawler interacts with only one API. It transforms from the link you see in the browser bar and will look like this:\\n\\n```plaintext\\nhttps://www.accommodationforstudents.com/_next/data/[build_id]/search-results.json?location=[location]&page=[page]\\n```\\n\\nI think you immediately noticed that I excluded part of the search parameters, I did this because we simply don\'t need them. Coordinates aren\'t used in basic interaction with the backend. I plan that the crawler will search by location, so I keep the location and pagination parameters.\\n\\nLet\'s summarize our analysis.\\n\\n1. For search pages, we\'ll use links of the format - `https://www.accommodationforstudents.com/_next/data/[build_id]/search-results.json?location=[location]&page=[page]`\\n2. For listing pages, we\'ll use links of the format - `https://www.accommodationforstudents.com/_next/data/[build_id]/property/[id].json`\\n3. We need to get the `build_id`, let\'s use the main page of the site and a simple regular expression for this.\\n4. We need an HTTP client that allows bypassing Cloudflare, and we don\'t need any HTML parsers, as we\'ll get all target data from JSON.\\n\\n## Crawler implementation\\n\\nI\'m using Crawlee for Python version `0.3.5`, this is important, as the library is developing actively and will have more capabilities in higher versions. But this is an ideal moment to show how we can work with it for complex projects.\\n\\nThe library already has support for an HTTP client that allows bypassing Cloudflare - [`CurlImpersonateHttpClient`](https://github.com/apify/crawlee-python/blob/v0.3.6/src/crawlee/http_clients/curl_impersonate.py). Since we have to work with JSON responses we could use [`parsel_crawler`](https://github.com/apify/crawlee-python/tree/v0.3.5/src/crawlee/parsel_crawler) added in version `0.3.0`, but I think this is excessive for such tasks, besides I like the high speed of [`orjson`](https://github.com/ijl/orjson).. Therefore, we\'ll need to implement our crawler rather than using one of the ready-made ones.\\n\\nAs a sample crawler, we\'ll use [beautifulsoup_crawler](https://github.com/apify/crawlee-python/tree/v0.3.5/src/crawlee/beautifulsoup_crawler)\\n\\nLet\'s install the necessary dependencies.\\n\\n```Bash\\npip install crawlee[curl-impersonate]==0.3.5\\npip install orjson>=3.10.7,<4.0.0\\"\\n```\\n\\nI\'m using [`orjson`](https://pypi.org/project/orjson/) instead of the standard [`json`](https://docs.python.org/3/library/json.html) module due to its high performance, which is especially noticeable in asynchronous applications.\\n\\nWell, let\'s implement our custom_crawler.\\nLet\'s define the `CustomContext` class with the necessary attributes.\\n\\n```python\\n# custom_context.py\\n\\nfrom __future__ import annotations\\n\\nfrom dataclasses import dataclass\\nfrom typing import TYPE_CHECKING\\n\\nfrom crawlee.basic_crawler import BasicCrawlingContext\\nfrom crawlee.http_crawler import HttpCrawlingResult\\n\\nif TYPE_CHECKING:\\n\\n    from collections.abc import Callable\\n\\n\\n@dataclass(frozen=True)\\nclass CustomContext(HttpCrawlingResult, BasicCrawlingContext):\\n    \\"\\"\\"Crawling context used by CustomCrawler.\\"\\"\\"\\n\\n    page_data: dict | None\\n    # not `EnqueueLinksFunction`` because we are breaking protocol since we are not working with HTML\\n    # and we are not using selectors\\n    enqueue_links: Callable\\n\\n```\\n\\nNote that in my context, `enqueue_links` is just `Callable`, not [`EnqueueLinksFunction`](https://github.com/apify/crawlee-python/blob/v0.3.5/src/crawlee/_types.py#L162). This is because we won\'t be using selectors and extracting links from HTML, which violate the agreed protocol. Still, I want the syntax in my crawler to be as close to standardized as possible.\\n\\nLet\'s move on to the crawler functionality in the `CustomCrawler` class.\\n\\n```python\\n# custom_crawler.py\\n\\nfrom __future__ import annotations\\n\\nimport logging\\nfrom re import search\\nfrom typing import TYPE_CHECKING, Any, Unpack\\n\\nfrom crawlee import Request\\nfrom crawlee.basic_crawler import (\\n    BasicCrawler,\\n    BasicCrawlerOptions,\\n    BasicCrawlingContext,\\n    ContextPipeline,\\n)\\nfrom crawlee.errors import SessionError\\nfrom crawlee.http_clients.curl_impersonate import CurlImpersonateHttpClient\\nfrom crawlee.http_crawler import HttpCrawlingContext\\nfrom orjson import loads\\n\\nfrom afs_crawlee.constants import BASE_TEMPLATE, HEADERS\\n\\nfrom .custom_context import CustomContext\\n\\nif TYPE_CHECKING:\\n    from collections.abc import AsyncGenerator, Iterable\\n\\n\\nclass CustomCrawler(BasicCrawler[CustomContext]):\\n    \\"\\"\\"A crawler that fetches the request URL using `curl_impersonate` and parses the result with `orjson` and `re`.\\"\\"\\"\\n\\n    def __init__(\\n        self,\\n        *,\\n        impersonate: str = \'chrome124\',\\n        additional_http_error_status_codes: Iterable[int] = (),\\n        ignore_http_error_status_codes: Iterable[int] = (),\\n        **kwargs: Unpack[BasicCrawlerOptions[CustomContext]],\\n    ) -> None:\\n\\n        self._build_id = None\\n        self._base_url = BASE_TEMPLATE\\n\\n        kwargs[\'_context_pipeline\'] = (\\n            ContextPipeline()\\n            .compose(self._make_http_request)\\n            .compose(self._handle_blocked_request)\\n            .compose(self._parse_http_response)\\n        )\\n\\n        # Initialize curl_impersonate http client using TLS preset and necessary headers\\n        kwargs.setdefault(\\n            \'http_client\',\\n            CurlImpersonateHttpClient(\\n                additional_http_error_status_codes=additional_http_error_status_codes,\\n                ignore_http_error_status_codes=ignore_http_error_status_codes,\\n                impersonate=impersonate,\\n                headers=HEADERS,\\n            ),\\n        )\\n\\n        kwargs.setdefault(\'_logger\', logging.getLogger(__name__))\\n\\n        super().__init__(**kwargs)\\n```\\n\\nIn `__init__`, we define that we\'ll use `CurlImpersonateHttpClient` as the `http_client`. Another important element is `_context_pipeline`, which defines the sequence of methods through which our context passes.\\n\\n`_make_http_request` - is completely identical to `BeautifulSoupCrawler`\\n`_handle_blocked_request` - since we get all data through the API, only the server response status will signal about blocking.\\n\\n```python\\n    async def _handle_blocked_request(self, crawling_context: CustomContext) -> AsyncGenerator[CustomContext, None]:\\n        if self._retry_on_blocked:\\n            status_code = crawling_context.http_response.status_code\\n\\n            if crawling_context.session and crawling_context.session.is_blocked_status_code(status_code=status_code):\\n                raise SessionError(f\'Assuming the session is blocked based on HTTP status code {status_code}\')\\n\\n        yield crawling_context\\n```\\n\\n`_parse_http_response` - a function that encapsulates the main logic of parsing responses\\n\\n```python\\n    async def _parse_http_response(self, context: HttpCrawlingContext) -> AsyncGenerator[CustomContext, None]:\\n\\n        page_data = None\\n\\n        if context.http_response.headers[\'content-type\'] == \'text/html; charset=utf-8\':\\n            # Get Build ID for Next js from the start page of the site, form a link to next.js endpoints\\n            build_id = search(rb\'\\"buildId\\":\\"(.{21})\\"\', context.http_response.read()).group(1)\\n            self._build_id = build_id.decode(\'UTF-8\')\\n            self._base_url = self._base_url.format(build_id=self._build_id)\\n        else:\\n            # Convert json to python dictionary\\n            page_data = context.http_response.read()\\n            page_data = page_data.decode(\'ISO-8859-1\').encode(\'utf-8\')\\n            page_data = loads(page_data)\\n\\n        async def enqueue_links(\\n            *, path_template: str, items: list[str], user_data: dict[str, Any] | None = None, label: str | None = None\\n        ) -> None:\\n\\n            requests = list[Request]()\\n            user_data = user_data if user_data else {}\\n\\n            for item in items:\\n                link_user_data = user_data.copy()\\n\\n                if label is not None:\\n                    link_user_data.setdefault(\'label\', label)\\n\\n                if link_user_data.get(\'label\') == \'SEARCH\':\\n                    link_user_data[\'location\'] = item\\n\\n                url = self._base_url + path_template.format(item=item, **user_data)\\n                requests.append(Request.from_url(url, user_data=link_user_data))\\n\\n            await context.add_requests(requests)\\n\\n        yield CustomContext(\\n            request=context.request,\\n            session=context.session,\\n            proxy_info=context.proxy_info,\\n            enqueue_links=enqueue_links,\\n            add_requests=context.add_requests,\\n            send_request=context.send_request,\\n            push_data=context.push_data,\\n            log=context.log,\\n            http_response=context.http_response,\\n            page_data=page_data,\\n        )\\n```\\n\\nAs you can see, if the server response comes in HTML, we get the `build_id` using a simple regular expression. This condition should be executed once for the first link and is necessary to interact further with the Next.js API. In all other cases, we simply convert JSON to a Python `dict` and save it in the context.\\n\\nIn `enqueue_links`, I create logic for generating links based on string templates and input parameters.\\n\\nThat\'s it: our custom Crawler Class for Crawlee for Python is ready, it\'s based on the `CurlImpersonateHttpClient` client, works with JSON responses instead of HTML, and implements the link generation logic we need.\\n\\n Let\'s finalize it by defining public classes for import.\\n\\n```python\\n# init.py\\n\\nfrom .custom_crawler import CustomCrawler\\nfrom .types import CustomContext\\n\\n__all__ = [\'CustomCrawler\', \'CustomContext\']\\n\\n```\\n\\nNow that we have the crawler functionality, let\'s implement routing and data extraction from the site. We\'ll use the [`official documentation`](https://www.crawlee.dev/python/docs/introduction/refactoring) as a template.\\n\\n```python\\n# router.py\\n\\nfrom crawlee.router import Router\\n\\nfrom .constants import LISTING_PATH, SEARCH_PATH, TARGET_LOCATIONS\\nfrom .custom_crawler import CustomContext\\n\\nrouter = Router[CustomContext]()\\n\\n\\n@router.default_handler\\nasync def default_handler(context: CustomContext) -> None:\\n    \\"\\"\\"Handle the start URL to get the Build ID and create search links.\\"\\"\\"\\n    context.log.info(f\'default_handler is processing {context.request.url}\')\\n\\n    await context.enqueue_links(\\n        path_template=SEARCH_PATH, items=TARGET_LOCATIONS, label=\'SEARCH\', user_data={\'page\': 1}\\n    )\\n\\n\\n@router.handler(\'SEARCH\')\\nasync def search_handler(context: CustomContext) -> None:\\n    \\"\\"\\"Handle the SEARCH URL generates links to listings and to the next search page.\\"\\"\\"\\n    context.log.info(f\'search_handler is processing {context.request.url}\')\\n\\n    max_pages = context.page_data[\'pageProps\'][\'initialPageCount\']\\n    current_page = context.request.user_data[\'page\']\\n    if current_page < max_pages:\\n\\n        await context.enqueue_links(\\n            path_template=SEARCH_PATH,\\n            items=[context.request.user_data[\'location\']],\\n            label=\'SEARCH\',\\n            user_data={\'page\': current_page + 1},\\n        )\\n    else:\\n        context.log.info(f\'Last page for {context.request.user_data[\\"location\\"]} location\')\\n\\n    listing_ids = [\\n        listing[\'property\'][\'id\']\\n        for group in context.page_data[\'pageProps\'][\'initialListings\'][\'groups\']\\n        for listing in group[\'results\']\\n        if listing.get(\'property\')\\n    ]\\n\\n    await context.enqueue_links(path_template=LISTING_PATH, items=listing_ids, label=\'LISTING\')\\n\\n\\n@router.handler(\'LISTING\')\\nasync def listing_handler(context: CustomContext) -> None:\\n    \\"\\"\\"Handle the LISTING URL extracts data from the listings and saving it to a dataset.\\"\\"\\"\\n    context.log.info(f\'listing_handler is processing {context.request.url}\')\\n\\n    listing_data = context.page_data[\'pageProps\'][\'viewModel\'][\'propertyDetails\']\\n    if not listing_data[\'exists\']:\\n        context.log.info(f\'listing_handler, data is not available for url {context.request.url}\')\\n        return\\n    property_data = {\\n        \'property_id\': listing_data[\'id\'],\\n        \'property_type\': listing_data[\'propertyType\'],\\n        \'location_latitude\': listing_data[\'coordinates\'][\'lat\'],\\n        \'location_longitude\': listing_data[\'coordinates\'][\'lng\'],\\n        \'address1\': listing_data[\'address\'][\'address1\'],\\n        \'address2\': listing_data[\'address\'][\'address2\'],\\n        \'city\': listing_data[\'address\'][\'city\'],\\n        \'postcode\': listing_data[\'address\'][\'postcode\'],\\n        \'bills_included\': listing_data.get(\'terms\', {}).get(\'billsIncluded\'),\\n        \'description\': listing_data.get(\'description\'),\\n        \'bathrooms\': listing_data.get(\'numberOfBathrooms\'),\\n        \'number_rooms\': len(listing_data[\'rooms\']) if listing_data.get(\'rooms\') else None,\\n        \'rent_ppw\': listing_data.get(\'terms\', {}).get(\'rentPpw\', {}).get(\'value\', None),\\n    }\\n\\n    await context.push_data(property_data)\\n```\\n\\nLet\'s define our `main` function, which will launch the crawler.\\n\\n```python\\n# main.py\\n\\nfrom .custom_crawler import CustomCrawler\\nfrom .router import router\\n\\n\\nasync def main() -> None:\\n    \\"\\"\\"The main function that starts crawling.\\"\\"\\"\\n    crawler = CustomCrawler(max_requests_per_crawl=50, request_handler=router)\\n\\n    # Run the crawler with the initial list of URLs.\\n    await crawler.run([\'https://www.accommodationforstudents.com/\'])\\n\\n    await crawler.export_data(\'results.json\')\\n```\\n\\nLet\'s look at the results.\\n\\n![Final results file](./img/final-results.webp)\\n\\nAs I prefer to manage my projects as packages and use `pyproject.toml` according to [PEP 518](https://peps.python.org/pep-0518/), the final structure of our project will look like this.\\n\\n![PEP 518 file structure](./img/PEP.webp)\\n\\n## Conclusion\\n\\nIn this project, we went through the entire cycle of crawler development, from analyzing a rather interesting dynamic site to full implementation of a crawler using `Crawlee for Python`. You can view the full project code on [GitHub](https://github.com/Mantisus/crawlee_python_example)\\n\\nI would also like to hear your comments and thoughts on the web scraping topic you\'d like to see in the next article. Feel free to comment here in the article or contact me in the [Crawlee developer community](https://apify.com/discord) on Discord.\\n\\nIf you are looking out to how to start scraping using Crawlee for Python, check out our [latest tutorial here](https://blog.apify.com/crawlee-for-python-tutorial/).\\n\\nYou can find me on the following platforms: [Github](https://github.com/Mantisus), [Linkedin](https://www.linkedin.com/in/max-bohomolov/), [Apify](https://apify.com/mantisus), [Upwork](https://www.upwork.com/freelancers/mantisus), [Contra](https://contra.com/mantisus).\\n\\nThank you for your attention. I hope you found this information useful."},{"id":"infinite-scroll-using-python","metadata":{"permalink":"/blog/infinite-scroll-using-python","source":"@site/blog/2024/08-27-how-to-scrape-infinite-scrolling-pages/index.md","title":"How to scrape infinite scrolling webpages with Python","description":"Learn how to scrape infinite scrolling pages with Python and scrape Nike shoes using Crawlee for Python.","date":"2024-08-27T00:00:00.000Z","tags":[],"readingTime":6.61,"hasTruncateMarker":true,"authors":[{"name":"Saurav Jain","title":"Developer Community Manager","url":"https://github.com/souravjain540","socials":{"x":"https://x.com/sauain","github":"https://github.com/souravjain540"},"imageURL":"https://avatars.githubusercontent.com/u/53312820?v=4","key":"SauravJ","page":null}],"frontMatter":{"slug":"infinite-scroll-using-python","title":"How to scrape infinite scrolling webpages with Python","description":"Learn how to scrape infinite scrolling pages with Python and scrape Nike shoes using Crawlee for Python.","image":"./img/infinite-scroll.webp","authors":["SauravJ"]},"unlisted":false,"prevItem":{"title":"Web scraping of a dynamic website using Python with HTTP Client","permalink":"/blog/scraping-dynamic-websites-using-python"},"nextItem":{"title":"Current problems and mistakes of web scraping in Python and tricks to solve them!","permalink":"/blog/common-problems-in-web-scraping"}},"content":"Hello, Crawlee Devs, and welcome back to another tutorial on the Crawlee Blog. This tutorial will teach you how to scrape infinite-scrolling websites using Crawlee for Python.\\n\\nFor context, infinite-scrolling pages are a modern alternative to classic pagination. When users scroll to the bottom of the webpage instead of choosing the next page, the page automatically loads more data, and users can scroll more.\\n\\nAs a big sneakerhead, I\'ll take the Nike shoes infinite-scrolling [website](https://www.nike.com/) as an example, and we\'ll scrape thousands of sneakers from it.\\n\\n![How to scrape infinite scrolling pages with Python](./img/infinite-scroll.webp)\\n\\nCrawlee for Python has some amazing initial features, such as a unified interface for HTTP and headless browser crawling, automatic retries, and much more.\\n\\n\x3c!-- truncate --\x3e\\n\\n## Prerequisites and bootstrapping the project\\n\\nLet\'s start the tutorial by installing Crawlee for Python with this command:\\n\\n```bash\\npipx run crawlee create nike-crawler\\n```\\n\\n:::note\\n\\nBefore going ahead if you like reading this blog, we would be really happy if you gave [Crawlee for Python a star on GitHub!](https://github.com/apify/crawlee-python/)\\n\\n:::\\n\\nWe will scrape using headless browsers. Select `PlaywrightCrawler` in the terminal when Crawlee for Python asks for it.\\n\\nAfter installation, Crawlee for Python will create boilerplate code for you. Redirect into the project folder and then run this command for all the dependencies installation:\\n\\n```bash\\npoetry install\\n```\\n\\n## How to scrape infinite scrolling webpages\\n\\n1.  Handling accept cookie dialog\\n\\n2.  Adding request of all shoes links\\n\\n3.  Extract data from product details\\n\\n4.  Accept Cookies context manager\\n\\n5.  Handling infinite scroll on the listing page\\n\\n6.  Exporting data to CSV format\\n\\n\\n### Handling accept cookie dialog\\n\\nAfter all the necessary installations, we\'ll start looking into the files and configuring them accordingly.\\n\\nWhen you look into the folder, you\'ll see many files, but for now, let\'s focus on `main.py` and `routes.py`.\\n\\nIn `main.py`, let\'s change the target location to the Nike website. Then, just to see how scraping will happen, we\'ll add `headless = False` to the `PlaywrightCrawler` parameters. Let\'s also increase the maximum requests per crawl option to 100 to see the power of parallel scraping in Crawlee for Python.\\n\\nThe final code will look like this:\\n\\n```python\\nfrom crawlee.playwright_crawler import PlaywrightCrawler\\n\\nfrom .routes import router\\n\\n\\nasync def main() -> None:\\n    \\"\\"\\"The crawler entry point.\\"\\"\\"\\n    crawler = PlaywrightCrawler(\\n        headless=False,\\n        request_handler=router,\\n        max_requests_per_crawl=100,\\n    )\\n\\n    await crawler.run(\\n        [\\n            \'https://nike.com/,\\n        ]\\n    )\\n```\\n\\nNow coming to `routes.py`, let\'s remove:\\n\\n```python\\nawait context.enqueue_links()\\n```\\nAs we don\'t want to scrape the whole website.\\n\\nNow, if you run the crawler using the command:\\n\\n```bash\\npoetry run python -m nike-crawler\\n```\\nAs the cookie dialog is blocking us from crawling more than one page\'s worth of shoes, let\'s get it out of our way.\\n\\nWe can handle the cookie dialog by going to Chrome dev tools and looking at the `test_id` of the \\"accept cookies\\" button, which is `dialog-accept-button`.\\n\\nNow, let\'s remove the `context.push_data` call that was left there from the project template and add the code to accept the dialog in routes.py. The updated code will look like this:\\n\\n```python\\nfrom crawlee.router import Router\\nfrom crawlee.playwright_crawler import PlaywrightCrawlingContext\\n\\nrouter = Router[PlaywrightCrawlingContext]()\\n\\n@router.default_handler\\nasync def default_handler(context: PlaywrightCrawlingContext) -> None:\\n    \\"\\"\\"Default request handler.\\"\\"\\"\\n\\n    # Wait for the popup to be visible to ensure it has loaded on the page.\\n    await context.page.get_by_test_id(\'dialog-accept-button\').click()\\n```\\n\\n### Adding request of all shoes links\\n\\nNow, if you hover over the top bar and see all the sections, i.e., man, woman, and kids, you\'ll notice the \u201CAll shoes\u201D section. As we want to scrape all the sneakers, this section interests us. Let\'s use `get_by_test_id` with the filter of `has_text=\'All shoes\'` and add all the links with the text \u201CAll shoes\u201D to the request handler. Let\'s add this code to the existing `routes.py` file:\\n\\n```python\\n    shoe_listing_links = (\\n        await context.page.get_by_test_id(\'link\').filter(has_text=\'All shoes\').all()\\n    )\\n    await context.add_requests(\\n        [\\n            Request.from_url(url, label=\'listing\')\\n            for link in shoe_listing_links\\n            if (url := await link.get_attribute(\'href\'))\\n        ]\\n    )\\n\\n@router.handler(\'listing\')\\nasync def listing_handler(context: PlaywrightCrawlingContext) -> None:\\n    \\"\\"\\"Handler for shoe listings.\\"\\"\\"\\n```\\n\\n### Extract data from product details\\n\\nNow that we have all the links to the pages with the title \u201CAll Shoes,\u201D the next step is to scrape all the products on each page and the information provided on them.\\n\\nWe\'ll extract each shoe\'s URL, title, price, and description. Again, let\'s go to dev tools and extract each parameter\'s relevant `test_id`. After scraping each of the parameters, we\'ll use the `context.push_data` function to add it to the local storage. Now let\'s add the following code to the `listing_handler` and update it in the `routes.py` file:\\n\\n```python\\n\\n@router.handler(\'listing\')\\nasync def listing_handler(context: PlaywrightCrawlingContext) -> None:\\n    \\"\\"\\"Handler for shoe listings.\\"\\"\\"\\n\\n    await context.enqueue_links(selector=\'a.product-card__link-overlay\', label=\'detail\')\\n\\n\\n@router.handler(\'detail\')\\nasync def detail_handler(context: PlaywrightCrawlingContext) -> None:\\n    \\"\\"\\"Handler for shoe details.\\"\\"\\"\\n\\n    title = await context.page.get_by_test_id(\\n        \'product_title\',\\n    ).text_content()\\n\\n    price = await context.page.get_by_test_id(\\n        \'currentPrice-container\',\\n    ).first.text_content()\\n\\n    description = await context.page.get_by_test_id(\\n        \'product-description\',\\n    ).text_content()\\n\\n    await context.push_data(\\n        {\\n            \'url\': context.request.loaded_url,\\n            \'title\': title,\\n            \'price\': price,\\n            \'description\': description,\\n        }\\n    )\\n```\\n\\n### Accept Cookies context manager\\n\\nSince we\'re dealing with multiple browser pages with multiple links and we want to do infinite scrolling, we may encounter an accept cookie dialog on each page. This will prevent loading more shoes via infinite scroll.\\n\\nWe\'ll need to check for cookies on every page, as each one may be opened with a fresh session (no stored cookies) and we\'ll get the accept cookie dialog even though we already accepted it in another browser window. However, if we don\'t get the dialog, we want the request handler to work as usual.\\n\\nTo solve this problem, we\'ll try to deal with the dialog in a parallel task that will run in the background. A context manager is a nice abstraction that will allow us to reuse this logic in all the router handlers. So, let\'s build a context manager:\\n\\n```python\\nfrom playwright.async_api import TimeoutError as PlaywrightTimeoutError\\n\\n@asynccontextmanager\\nasync def accept_cookies(page: Page):\\n    task = asyncio.create_task(page.get_by_test_id(\'dialog-accept-button\').click())\\n    try:\\n        yield\\n    finally:\\n        if not task.done():\\n            task.cancel()\\n\\n        with suppress(asyncio.CancelledError, PlaywrightTimeoutError):\\n            await task\\n```\\n\\nThis context manager will make sure we\'re accepting the cookie dialog if it exists before scrolling and scraping the page. Let\'s implement it in the `routes.py` file, and the updated code is [here](https://github.com/janbuchar/crawlee-python-demo/blob/6ca6f7f1d1bbbf789a3b86f14bec492cf756251e/crawlee-python-webinar/routes.py)\\n\\n### Handling infinite scroll on the listing page\\n\\nNow for the last and most interesting part of the tutorial! How to handle the infinite scroll of each shoe listing page and make sure our crawler is scrolling and scraping the data constantly.\\n\\nThis tutorial is taken from the webinar held on August 5th where Jan Buchar, Senior Python Engineer at Apify, gave a live demo about this use case. Watch the tutorial here:\\n\\n<iframe width=\\"560\\" height=\\"315\\" src=\\"https://www.youtube.com/embed/ip8Ii0eLfRY?si=7ZllUhMhuC7VC23B&amp;start=667\\" title=\\"YouTube video player\\" frameborder=\\"0\\" allow=\\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\\" referrerpolicy=\\"strict-origin-when-cross-origin\\" allowfullscreen></iframe>\\n\\n\\nTo handle infinite scrolling in Crawlee for Python, we just need to make sure the page is loaded, which is done by waiting for the `network_idle` load state, and then use the `infinite_scroll` helper function which will keep scrolling to the bottom of the page as long as that makes additional items appear.\\n\\nLet\'s add two lines of code to the `listing` handler:\\n\\n```python\\n@router.handler(\'listing\')\\nasync def listing_handler(context: PlaywrightCrawlingContext) -> None:\\n    \\"\\"\\"Handler for shoe listings.\\"\\"\\"\\n\\n    async with accept_cookies(context.page):\\n        await context.page.wait_for_load_state(\'networkidle\')\\n        await context.infinite_scroll()\\n        await context.enqueue_links(\\n            selector=\'a.product-card__link-overlay\', label=\'detail\'\\n        )\\n```\\n\\n## Exporting data to CSV format\\n\\nAs we want to store all the shoe data into a CSV file, we can just add a call to the `export_data` helper into the `main.py` file just after the crawler run:\\n\\n```python\\n    await crawler.export_data(\'shoes.csv\')\\n```\\n\\n## Working crawler and its code\\n\\nNow, we have a crawler ready that can scrape all the shoes from the Nike website while handling infinite scrolling and many other problems, like the cookies dialog.\\n\\nYou can find the complete working crawler code here on the [GitHub repository](https://github.com/janbuchar/crawlee-python-demo).\\n\\nLearn more about Crawlee for Python from our latest step by step [tutorial](https://blog.apify.com/crawlee-for-python-tutorial/).\\n\\nIf you have any doubts regarding this tutorial or using Crawlee for Python, feel free to [join our discord community](https://apify.com/discord/) and ask fellow developers or the Crawlee team."},{"id":"common-problems-in-web-scraping","metadata":{"permalink":"/blog/common-problems-in-web-scraping","source":"@site/blog/2024/08-20-problems-in-web-scraping/index.md","title":"Current problems and mistakes of web scraping in Python and tricks to solve them!","description":"Current problems and mistakes that developers encounters while scraping and crawling the internet with the advises and solution from an web scraping expert.","date":"2024-08-20T00:00:00.000Z","tags":[{"inline":true,"label":"community","permalink":"/blog/tags/community"}],"readingTime":14.525,"hasTruncateMarker":true,"authors":[{"name":"Max","title":"Community Member of Crawlee and web scraping expert","url":"https://github.com/Mantisus","socials":{"github":"https://github.com/Mantisus"},"imageURL":"https://avatars.githubusercontent.com/u/34358312?v=4","key":"MaxB","page":null}],"frontMatter":{"slug":"common-problems-in-web-scraping","title":"Current problems and mistakes of web scraping in Python and tricks to solve them!","tags":["community"],"description":"Current problems and mistakes that developers encounters while scraping and crawling the internet with the advises and solution from an web scraping expert.","image":"./img/problems-in-scraping.webp","authors":["MaxB"]},"unlisted":false,"prevItem":{"title":"How to scrape infinite scrolling webpages with Python","permalink":"/blog/infinite-scroll-using-python"},"nextItem":{"title":"Announcing Crawlee for Python: Now you can use Python to build reliable web crawlers","permalink":"/blog/launching-crawlee-python"}},"content":"## Introduction\\n\\nGreetings! I\'m [Max](https://apify.com/mantisus), a Python developer from Ukraine, a developer with expertise in web scraping, data analysis, and processing.\\n\\nMy journey in web scraping started in 2016 when I was solving lead generation challenges for a small company. Initially, I used off-the-shelf solutions such as [Import.io](https://www.import.io/) and Kimono Labs. However, I quickly encountered limitations such as blocking, inaccurate data extraction, and performance issues. This led me to learn Python. Those were the glory days when [`requests`](https://requests.readthedocs.io/en/latest/) and [`lxml`](https://lxml.de/)/[`beautifulsoup`](https://beautiful-soup-4.readthedocs.io/en/latest/) were enough to extract data from most websites. And if you knew how to work with threads, you were already a respected expert :)\\n\\n:::note\\nOne of our community members wrote this blog as a contribution to Crawlee Blog. If you want to contribute blogs like these to Crawlee Blog, please reach out to us on our [discord channel](https://apify.com/discord).\\n:::\\n\\nAs a freelancer, I\'ve built small solutions and large, complex data mining systems for products over the years.\\n\\nToday, I want to discuss the realities of [web scraping with Python in 2024](https://blog.apify.com/web-scraping-python/). We\'ll look at the mistakes I sometimes see and the problems you\'ll encounter and offer solutions to some of them.\\n\\nLet\'s get started.\\n\\nJust take `requests` and `beautifulsoup` and start making a lot of money...\\n\\nNo, this is not that kind of article.\\n\\n\x3c!-- truncate --\x3e\\n\\n## 1. \\"I got a 200 response from the server, but it\'s an unreadable character set.\\"\\n\\nYes, it can be surprising. But I\'ve seen this message from customers and developers six years ago, four years ago, and in 2024. I read a post on Reddit just a few months ago about this issue.\\n\\nLet\'s look at a simple code example. This will work for `requests`, [`httpx`](https://www.python-httpx.org/), and [`aiohttp`](https://docs.aiohttp.org/en/stable/client.html#aiohttp-client) with a clean installation and no extensions.\\n\\n```python\\nimport httpx\\n\\nurl = \'https://www.wayfair.com/\'\\n\\nheaders = {\\n    \\"User-Agent\\": \\"Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:128.0) Gecko/20100101 Firefox/128.0\\",\\n    \\"Accept\\": \\"text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/png,image/svg+xml,*/*;q=0.8\\",\\n    \\"Accept-Language\\": \\"en-US,en;q=0.5\\",\\n    \\"Accept-Encoding\\": \\"gzip, deflate, br, zstd\\",\\n    \\"Connection\\": \\"keep-alive\\",\\n}\\n\\nresponse = httpx.get(url, headers=headers)\\n\\nprint(response.content[:10])\\n```\\n\\nThe print result will be similar to:\\n\\n```bash\\nb\'\\\\x83\\\\x0c\\\\x00\\\\x00\\\\xc4\\\\r\\\\x8e4\\\\x82\\\\x8a\'\\n```\\n\\nIt\'s not an error - it\'s a perfectly valid server response. It\'s encoded somehow.\\n\\nThe answer lies in the `Accept-Encoding` header. In the example above, I just copied it from my browser, so it lists all the compression methods my browser supports: \\"gzip, deflate, br, zstd\\". The Wayfair backend supports compression with \\"br\\", which is [Brotli](https://github.com/google/brotli), and uses it as the most efficient method.\\n\\nThis can happen if none of the libraries listed above have a `Brotli` dependency among their standard dependencies. However, they all support decompression from this format if you already have `Brotli` installed.\\n\\nTherefore, it\'s sufficient to install the appropriate library:\\n\\n```bash\\npip install Brotli\\n```\\n\\nThis will allow you to get the result of the print:\\n\\n```bash\\nb\'<!DOCTYPE \'\\n```\\n\\nYou can obtain the same result for `aiohttp` and `httpx` by doing the installation with extensions:\\n\\n```bash\\npip install aiohttp[speedups]\\npip install httpx[brotli]\\n```\\n\\nBy the way, adding the `brotli` dependency was my first contribution to [`crawlee-python`](https://github.com/apify/crawlee-python). They use `httpx` as the base HTTP client.\\n\\nYou may have also noticed that a new supported data compression format [`zstd`](https://github.com/facebook/zstd) appeared some time ago. I haven\'t seen any backends that use it yet, but `httpx` will support decompression in versions above 0.28.0. I already use it to compress server response dumps in my projects; it shows incredible efficiency in asynchronous solutions with [`aiofiles`](https://github.com/Tinche/aiofiles).\\n\\nThe most common solution to this situation that I\'ve seen is for developers to simply stop using the `Accept-Encoding` header, thus getting an uncompressed response from the server. Why is that bad? The [main page of Wayfair](https://www.wayfair.com/) takes about 1 megabyte uncompressed and about 0.165 megabytes compressed.\\n\\nTherefore, in the absence of this header:\\n\\n- You increase the load on your internet bandwidth.\\n- If you use a proxy with traffic, you increase the cost of each of your requests.\\n- You increase the load on the server\'s internet bandwidth.\\n- You\'re revealing yourself as a scraper, since any browser uses compression.\\n\\nBut I think the problem is a bit deeper than that. Many web scraping developers simply don\'t understand what the headers they use do. So if this applies to you, when you\'re working on your next project, read up on these things; they may surprise you.\\n\\n## 2. \\"I use headers as in an incognito browser, but I get a 403 response\\". Here\'s Johnn-... I mean, Cloudflare\\n\\nYes, that\'s right. 2023 brought us not only Large Language Models like ChatGPT but also improved [Cloudflare](https://www.cloudflare.com/) protection.\\n\\nThose who have been scraping the web for a long time might say, \\"Well, we\'ve already dealt with DataDome, PerimeterX, InCapsula, and the like.\\"\\n\\nBut Cloudflare has changed the rules of the game. It is one of the largest CDN providers in the world, serving a huge number of sites. Therefore, its services are available to many sites with a fairly low entry barrier. This makes it radically different from the technologies mentioned earlier, which were implemented purposefully when they wanted to protect the site from scraping.\\n\\nCloudflare is the reason why, when you start reading another course on \\"How to do web scraping using `requests` and `beautifulsoup`\\", you can close it immediately. Because there\'s a big chance that what you learn will simply not work on any \\"decent\\" website.\\n\\nLet\'s look at another simple code example:\\n\\n```python\\nfrom httpx import Client\\n\\nclient = Client(http2=True)\\n\\nurl = \'https://www.g2.com/\'\\n\\nheaders = {\\n    \\"User-Agent\\": \\"Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:128.0) Gecko/20100101 Firefox/128.0\\",\\n    \\"Accept\\": \\"text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/png,image/svg+xml,*/*;q=0.8\\",\\n    \\"Accept-Language\\": \\"en-US,en;q=0.5\\",\\n    \\"Accept-Encoding\\": \\"gzip, deflate, br, zstd\\",\\n    \\"Connection\\": \\"keep-alive\\",\\n}\\n\\nresponse = client.get(url, headers=headers)\\n\\nprint(response)\\n```\\n\\nOf course, the response would be [403](https://blog.apify.com/web-scraping-how-to-solve-403-errors/).\\n\\nWhat if we use [`curl`](https://curl.se/docs/manpage.html)?\\n\\n```bash\\ncurl -XGET -H \'User-Agent: Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:128.0) Gecko/20100101 Firefox/128.0\\"\' -H \'Accept: text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/png,image/svg+xml,*/*;q=0.8\' -H \'Accept-Language: en-US,en;q=0.5\' -H \'Connection: keep-alive\' \'https://www.g2.com/\' -s -o /dev/null -w \\"%{http_code}\\\\n\\"\\n```\\n\\nAlso 403.\\n\\nWhy is this happening?\\n\\nBecause Cloudflare uses TLS fingerprints of many HTTP clients popular among developers, site administrators can also customize how aggressively Cloudflare blocks clients based on these fingerprints.\\n\\nFor `curl`, we can solve it like this:\\n\\n```bash\\ncurl -XGET -H \'User-Agent: Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:128.0) Gecko/20100101 Firefox/128.0\\"\' -H \'Accept: text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/png,image/svg+xml,*/*;q=0.8\' -H \'Accept-Language: en-US,en;q=0.5\' -H \'Connection: keep-alive\' \'https://www.g2.com/\' --tlsv1.3 -s -o /dev/null -w \\"%{http_code}\\\\n\\"\\n```\\n\\nYou might expect me to write here an equally elegant solution for `httpx`, but no. About six months ago, you could do the \\"dirty trick\\" and change the basic [`httpcore`](https://www.encode.io/httpcore/) parameters that it passes to [`h2`](https://github.com/python-hyper/h2), which are responsible for the HTTP2 handshake. But now, as I\'m writing this article, that doesn\'t work anymore.\\n\\nThere are different approaches to getting around this. But let\'s solve it by manipulating TLS.\\n\\nThe bad news is that all the Python clients I know of use the [`ssl`](https://docs.python.org/3/library/ssl.html) library to handle TLS. And it doesn\'t give you the ability to manipulate TLS subtly.\\n\\nThe good news is that the Python community is great and implements solutions that exist in other programming languages.\\n\\n### The first way to solve this problem is to use [tls-client](https://github.com/FlorianREGAZ/Python-Tls-Client)\\n\\nThis Python wrapper around the [Golang library](https://github.com/bogdanfinn/tls-client) provides an API similar to `requests`.\\n\\n```bash\\npip install tls-client\\n```\\n\\n```python\\nfrom tls_client import Session\\n\\nclient = Session(client_identifier=\\"firefox_120\\")\\n\\nurl = \'https://www.g2.com/\'\\n\\nheaders = {\\n    \\"User-Agent\\": \\"Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:128.0) Gecko/20100101 Firefox/128.0\\",\\n    \\"Accept\\": \\"text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/png,image/svg+xml,*/*;q=0.8\\",\\n    \\"Accept-Language\\": \\"en-US,en;q=0.5\\",\\n    \\"Accept-Encoding\\": \\"gzip, deflate, br, zstd\\",\\n    \\"Connection\\": \\"keep-alive\\",\\n}\\n\\nresponse = client.get(url, headers=headers)\\n\\nprint(response)\\n```\\n\\nThe `tls_client` supports TLS presets for popular browsers, the relevance of which is maintained by developers. To use this, you must pass the necessary `client_identifier`. However, the library also allows for subtle manual manipulation of TLS.\\n\\n### The second way to solve this problem is to use [curl_cffi](https://github.com/yifeikong/curl_cffi)\\n\\nThis wrapper around the C library patches curl and provides an API similar to `requests`.\\n\\n```bash\\npip install curl_cffi\\n```\\n\\n```python\\nfrom curl_cffi import requests\\n\\nurl = \'https://www.g2.com/\'\\n\\nheaders = {\\n    \\"User-Agent\\": \\"Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:128.0) Gecko/20100101 Firefox/128.0\\",\\n    \\"Accept\\": \\"text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/png,image/svg+xml,*/*;q=0.8\\",\\n    \\"Accept-Language\\": \\"en-US,en;q=0.5\\",\\n    \\"Accept-Encoding\\": \\"gzip, deflate, br, zstd\\",\\n    \\"Connection\\": \\"keep-alive\\",\\n}\\n\\nresponse = requests.get(url, headers=headers, impersonate=\\"chrome124\\")\\n\\nprint(response)\\n```\\n\\ncurl_cffi also provides [TLS presets](https://curl-cffi.readthedocs.io/en/latest/impersonate.html#supported-browser-versions) for some browsers, which are specified via the `impersonate` parameter. It also provides options for [subtle manual manipulation of TLS](https://curl-cffi.readthedocs.io/en/latest/impersonate.html#how-to-use-my-own-fingerprints-other-than-the-builtin-ones-e-g-okhttp).\\n\\nI think someone just said, \\"They\'re literally doing the same thing.\\" That\'s right, and they\'re both still very raw.\\n\\nLet\'s do some simple comparisons:\\n\\n| Feature | tls_client | curl_cffi |\\n|:-------:|:----------:|:---------:|\\n|TLS preset| + | + |\\n|TLS manual| + | + |\\n|async support| - | + |\\n|big company support| - | + |\\n|number of contributors| - | + |\\n\\nObviously, `curl_cffi` wins in this comparison. But as an active user, I have to say that sometimes there are some pretty strange errors that I\'m just unsure how to deal with. And let\'s be honest, so far, they are both pretty raw.\\n\\nI think we will soon see other libraries that solve this problem.\\n\\nOne might ask, what about [`Scrapy`](https://scrapy.org/)? I\'ll be honest: I don\'t really keep up with their updates. But I haven\'t heard about [Zyte](https://www.zyte.com/) doing anything to bypass TLS fingerprinting. So out of the box `Scrapy` will also be blocked, but nothing is stopping you from using `curl_cffi` in your Scrapy Spider.\\n\\n## 3. What about headless browsers and Cloudflare Turnstile?\\n\\nYes, sometimes we need to use headless browsers. Although I\'ll be honest, from my point of view, they are used too often even when clearly not necessary.\\n\\nEven in a headless situation, the folks at Cloudflare have managed to make life difficult for the average web scraper by creating a monster called Cloudflare Turnstile.\\n\\nTo test different tools, you can use this demo [page](https://2captcha.com/demo/cloudflare-turnstile).\\n\\nTo quickly test whether a library works with the browser, you should start by checking the usual non-headless mode. You don\'t even need to use automation; just open the site using the desired library and act manually.\\n\\nWhat libraries are worth checking out for this?\\n\\n### Candidate #1 [Playwright](https://playwright.dev/python/docs/intro) + [playwright-stealth](https://github.com/AtuboDad/playwright_stealth)\\n\\nIt\'ll be blocked and won\'t let you solve the captcha.\\n\\nPlaywright is a great library for browser automation. However the developers explicitly state that they don\'t plan to develop it as a web scraping tool.\\n\\nAnd I haven\'t heard of any Python projects that effectively solve this problem.\\n\\n### Candidate #2 [undetected_chromedriver](https://github.com/ultrafunkamsterdam/undetected-chromedriver)\\n\\nIt\'ll be blocked and won\'t let you solve the captcha.\\n\\nThis is a fairly common library for working with headless browsers in Python, and in some cases, it allows bypassing Cloudflare Turnstile. But on the target website, it is blocked. Also, in my projects, I\'ve encountered at least two other cases where Cloudflare blocked undetected_chromedriver.\\n\\nIn general, undetected_chromedriver is a good library for your projects, especially since it uses good old Selenium under the hood.\\n\\n### Candidate #3 [botasaurus-driver](https://github.com/omkarcloud/botasaurus-driver)\\n\\nIt allows you to go past the captcha after clicking.\\n\\nI don\'t know how its developers pulled this off, but it works. Its main feature is that it was developed specifically for web scraping. It also has a higher-level library to work with - [botasaurus](https://github.com/omkarcloud/botasaurus).\\n\\nOn the downside, so far, it\'s pretty raw, and botasaurus-driver has no documentation and has a rather challenging API to work with.\\n\\nTo summarize, most likely, your main library for headless browsing will be `undetected_chromedriver`. But in some particularly challenging cases, you might need to use `botasaurus`.\\n\\n## 4. What about frameworks?\\n\\nHigh-level frameworks are designed to speed up and ease development by allowing us to focus on business logic, although we often pay the price in flexibility and control.\\n\\nSo, what are the frameworks for web scraping in 2024?\\n\\n### [Scrapy](https://docs.scrapy.org/en/latest/)\\n\\nIt\'s impossible to talk about Python web scraping frameworks without mentioning Scrapy. Scrapinghub (now Zyte) first released it in 2008. For 16 years, it has been developed as an open-source library upon which development companies built their business solutions.\\n\\nTalking about the advantages of `Scrapy`, you could write a separate article. But I will emphasize the two of them:\\n\\n- The huge amount of tutorials that have been released over the years\\n- Middleware libraries are written by the community and are extending their functionality. For example, [`scrapy-playwright`](https://github.com/scrapy-plugins/scrapy-playwright).\\n\\nBut what are the downsides?\\n\\nIn recent years, Zyte has been focusing more on developing its own platform. `Scrapy` mostly gets fixes only.\\n- Lack of development towards bypassing anti-scraping systems. You have to implement them yourself, but then, why do you need a framework?\\n- `Scrapy` was originally developed with the asynchronous framework `Twisted`. Partial support for `asyncio` was added only in [`version 2.0`](https://docs.scrapy.org/en/latest/topics/asyncio.html). Looking through the source code, you may notice some workarounds that were added for this purpose.\\n\\nThus, `Scrapy` is a good and proven solution for sites that are not protected against web scraping. You will need to develop and add the necessary solutions to the framework in order to bypass anti-scraping measures.\\n\\n### [Botasaurus](https://www.omkar.cloud/botasaurus/)\\n\\nA new framework for web scraping using browser automation, built on [`botasaurus-driver`](https://github.com/omkarcloud/botasaurus-driver). The initial commit was made on May 9, 2023.\\n\\nLet\'s start with its advantages:\\n\\n- Allows you to bypass any Claudflare protection as well as many others using `botasaurus-driver`.\\n- Good documentation for a quick start\\n\\nDownsides include:\\n\\n- Browser automation only, not intended for HTTP clients.\\n- Tight coupling with `botasaurus-driver`; you can\'t easily replace it with something better if it comes out in the future.\\n- No asynchrony, only multithreading.\\n- At the moment, it\'s quite raw and still requires fixes for stable operation.\\n- There are very few training materials available at the moment.\\n\\nThis is a good framework for quickly building a web scraper based on browser automation. It lacks flexibility and support for HTTP clients, which is crutias for users like me.\\n\\n### [Crawlee for Python](https://www.crawlee.dev/python/)\\n\\nA new framework for web scraping in the Python ecosystem. The initial commit was made on Jan 10, 2024, with a release in the media space on July 5, 2024.\\n\\n:::tip\\nIf you like the blog so far, please consider [giving Crawlee a star on GitHub](https://github.com/apify/crawlee), it helps us to reach and help more developers.\\n:::\\n\\nDeveloped by [Apify](https://apify.com/), it is a Python adaptation of their famous JS framework [`crawlee`](https://github.com/apify/crawlee), first released on Jul 9, 2019.\\n\\nAs this is a completely new solution on the market, it is now in an active design and development stage. The community is also actively involved in its development. So,we can see that the use of [curl_cffi](https://github.com/apify/crawlee-python/issues/292) is already being discussed. The possibility of creating their own Rust-based client was [previously discussed](https://github.com/apify/crawlee-python/issues/80). I hope the company doesn\'t abandon the idea.\\n\\n:::note Crawlee team\\n\\"Yeah, for sure we will keep improving Crawlee for Python for years to come.\\"\\n:::\\n\\nAs I personally would like to see an HTTP client for Python developed and maintained by a major company. And Rust shows itself very well as a library language for Python. Let\'s remember at least [`Ruff`](https://docs.astral.sh/ruff/) and [`Pydantic`](https://docs.pydantic.dev/latest/) v2.\\n\\nAdvantages:\\n\\nThe framework was developed by an established company in the web scraping market, which has well-developed expertise in this sphere.\\n- Support for both browser automation and HTTP clients.\\n- Fully asynchronous, based on `asyncio`.\\n- Active development phase and media activity. As developers listen to the community, it is quite important in this phase.\\n\\nOn a separate note, it has a pretty good modular architecture. If developers introduce the ability to switch between several HTTP clients, we will get a rather flexible framework that allows us to easily change the technologies used, with a simple implementation from the development team.\\n\\nDeficiencies:\\n\\n- The framework is new. There are very few training materials available at the moment.\\n- At the moment, it\'s quite raw and still requires fixes for stable operation, as well as convenient interfaces for configuration.\\n-There is no implementation of any means of bypassing anti-scraping systems for now other than changing sessions and proxies. But they are being discussed.\\n\\nI believe that how successful `crawlee-python` turns out to depends primarily on the community. Due to the small number of tutorials, it is not suitable for beginners. However, experienced developers may decide to try it instead of `Scrapy`.\\n\\nIn the long run, it may turn out to be a better solution than Scrapy and Botasaurus. It already provides flexible tools for working with HTTP clients, automating browsers out of the box, and quickly switching between them. However, it lacks tools to bypass scraping protections, and their implementation in the future may be the deciding factor in choosing a framework for you.\\n\\n## Conclusion\\n\\nIf you have read all the way to here, I assume you found it interesting and maybe even helpful :)\\n\\nThe industry is changing and offering new challenges, and if you are professionally involved in web scraping, you will have to keep a close eye on the situation. In some other field, you would remain a developer who makes products using outdated technologies. But in modern web scraping, you become a developer who makes web scrapers that simply don\'t work.\\n\\nAlso, don\'t forget that you are part of the larger Python community, and your knowledge can be useful in developing tools that make things happen for all of us. As you can see, many of the tools you need are being built literally right now.\\n\\nI\'ll be glad to read your comments. Also, if you need a web scraping expert or do you just want to discuss the article, you can find me on the following platforms: [Github](https://github.com/Mantisus), [Linkedin](https://www.linkedin.com/in/max-bohomolov/), [Apify](https://apify.com/mantisus), [Upwork](https://www.upwork.com/freelancers/mantisus), [Contra](https://contra.com/mantisus).\\n\\nThank you for your attention :)"},{"id":"launching-crawlee-python","metadata":{"permalink":"/blog/launching-crawlee-python","source":"@site/blog/2024/07-05-launching-crawlee-python/index.md","title":"Announcing Crawlee for Python: Now you can use Python to build reliable web crawlers","description":"Launching Crawlee for Python, a web scraping and automation library to build reliable scrapers in Python fastly.","date":"2024-07-05T00:00:00.000Z","tags":[],"readingTime":4.545,"hasTruncateMarker":true,"authors":[{"name":"Saurav Jain","title":"Developer Community Manager","url":"https://github.com/souravjain540","socials":{"x":"https://x.com/sauain","github":"https://github.com/souravjain540"},"imageURL":"https://avatars.githubusercontent.com/u/53312820?v=4","key":"SauravJ","page":null}],"frontMatter":{"slug":"launching-crawlee-python","title":"Announcing Crawlee for Python: Now you can use Python to build reliable web crawlers","description":"Launching Crawlee for Python, a web scraping and automation library to build reliable scrapers in Python fastly.","image":"./img/crawlee-python.webp","authors":["SauravJ"]},"unlisted":false,"prevItem":{"title":"Current problems and mistakes of web scraping in Python and tricks to solve them!","permalink":"/blog/common-problems-in-web-scraping"},"nextItem":{"title":"How Crawlee uses tiered proxies to avoid getting blocked","permalink":"/blog/proxy-management-in-crawlee"}},"content":"> Testimonial from early adopters\\n>\\n> \u201CCrawlee for Python development team did a great job in building the product, it makes things faster for a Python developer.\u201D\\n>\\n> ~ [Maksym Bohomolov](https://apify.com/mantisus)\\n\\nWe launched Crawlee in [August 2022](https://blog.apify.com/announcing-crawlee-the-web-scraping-and-browser-automation-library/) and got an amazing response from the JavaScript community. With many early adopters in its initial days, we got valuable feedback, which gave Crawlee a strong base for its success.\\n\\nToday, [Crawlee built-in TypeScript](https://github.com/apify/crawlee) has nearly **13,000 stars on GitHub**, with 90 open-source contributors worldwide building the best web scraping and automation library.\\n\\nSince the launch, the feedback we\u2019ve received most often [[1]](https://discord.com/channels/801163717915574323/999250964554981446/1138826582581059585)[[2]](https://discord.com/channels/801163717915574323/801163719198638092/1137702376267059290)[[3]](https://discord.com/channels/801163717915574323/1090592836044476426/1103977818221719584) has been to build Crawlee in Python so that the Python community can use all the features the JavaScript community does.\\n\\nWith all these requests in mind and to simplify the life of Python web scraping developers, **we\u2019re launching [Crawlee for Python](https://github.com/apify/crawlee-python) today.**\\n\\nThe new library is still in **beta**, and we are looking for **early adopters**.\\n\\n![Crawlee for Python is looking for early adopters](./img/early-adopters.webp)\\n\\nCrawlee for Python has some amazing initial features, such as a unified interface for HTTP and headless browser crawling, automatic retries, and much more.\\n\\n\x3c!-- truncate --\x3e\\n\\n## Why use Crawlee instead of a random HTTP library with an HTML parser?\\n\\n- Unified interface for HTTP & headless browser crawling.\\n    - HTTP - HTTPX with BeautifulSoup,\\n    - Headless browser - Playwright.\\n- Automatic parallel crawling based on available system resources.\\n- Written in Python with type hints - enhances DX (IDE autocompletion) and reduces bugs (static type checking).\\n- Automatic retries on errors or when you\u2019re getting blocked.\\n- Integrated proxy rotation and session management.\\n- Configurable request routing - direct URLs to the appropriate handlers.\\n- Persistent queue for URLs to crawl.\\n- Pluggable storage of both tabular data and files.\\n\\n## Understanding the why behind the features of Crawlee\\n\\n### Out-of-the-box support for headless browser crawling (Playwright).\\n\\nWhile libraries like Scrapy require additional installation of middleware, i.e, [`scrapy-playwright`](https://github.com/scrapy-plugins/scrapy-playwright) and still doesn\u2019t work with Windows, Crawlee for Python supports a unified interface for HTTP & headless browsers.\\n\\nUsing a headless browser to download web pages and extract data, `PlaywrightCrawler` is ideal for crawling websites that require JavaScript execution.\\n\\nFor websites that don\u2019t require JavaScript, consider using the `BeautifulSoupCrawler,` which utilizes raw HTTP requests and will be much faster.\\n\\n```python\\nimport asyncio\\n\\nfrom crawlee.playwright_crawler import PlaywrightCrawler, PlaywrightCrawlingContext\\n\\n\\nasync def main() -> None:\\n    # Create a crawler instance\\n    crawler = PlaywrightCrawler(\\n        # headless=False,\\n        # browser_type=\'firefox\',\\n    )\\n\\n    @crawler.router.default_handler\\n    async def request_handler(context: PlaywrightCrawlingContext) -> None:\\n        data = {\\n            \'request_url\': context.request.url,\\n            \'page_url\': context.page.url,\\n            \'page_title\': await context.page.title(),\\n            \'page_content\': (await context.page.content())[:10000],\\n        }\\n        await context.push_data(data)\\n\\n    await crawler.run([\'https://crawlee.dev\'])\\n\\n\\nif __name__ == \'__main__\':\\n    asyncio.run(main())\\n```\\n\\nThe above example uses Crawlee\u2019s built-in `PlaywrightCrawler` to crawl the [https://crawlee.dev/](https://crawlee.dev/) website title and its content.\\n\\n### Small learning curve\\n\\nIn other libraries like Scrapy, when you run a command to create a new project, you get many files. Then you need to learn about the architecture, including various components (spiders, middlewares, pipelines, etc.). [The learning curve is very steep](https://crawlee.dev/blog/scrapy-vs-crawlee#language-and-development-environments).\\n\\nWhile building Crawlee, we made sure that the learning curve and the setup would be as fast as possible.\\n\\nWith [ready-made templates](https://github.com/apify/crawlee-python/tree/master/templates), and having only a single file to add the code, it\'s very easy to start building a scraper, you might need to learn a little about request handlers and storage, but that\u2019s all.\\n\\n### Complete type hint coverage\\n\\nWe know how much developers like their code to be high-quality, readable, and maintainable.\\n\\nThat\'s why the whole code base of Crawlee is fully type-hinted.\\n\\nThanks to that, you should have better autocompletion in your IDE, enhancing developer experience while developing your scrapers using Crawlee.\\n\\nType hinting should also reduce the number of bugs thanks to static type checking.\\n\\n![Crawlee_Python_Type_Hint](./img/crawlee-python-type-hint.webp)\\n\\n### Based on Asyncio\\n\\nCrawlee is fully asynchronous and based on [Asyncio](https://docs.python.org/3/library/asyncio.html). For scraping frameworks, where many IO-bounds operations occur, this should be crucial to achieving high performance.\\n\\nAlso, thanks to Asyncio, integration with other applications or the rest of your system should be easy.\\n\\nHow is this different from the Scrapy framework, which is also asynchronous?\\n\\nScrapy relies on the \\"legacy\\" Twisted framework. Integrating Scrapy with modern Asyncio-based applications can be challenging, often requiring more effort and debugging [[1]](https://stackoverflow.com/questions/49201915/debugging-scrapy-project-in-visual-studio-code).\\n\\n## Power of open source community and early adopters giveaway\\n\\nCrawlee for Python is fully open-sourced and the codebase is available on the [GitHub repository of Crawlee for Python](https://github.com/apify/crawlee-python).\\n\\nWe have already started receiving initial and very [valuable contributions from the Python community](https://github.com/apify/crawlee-python/pull/226).\\n\\n> Early adopters also said:\\n>\\n> \u201CCrawlee for Python development team did a great job in building the product, it makes things faster for a Python developer.\u201D\\n>\\n> ~ [Maksym Bohomolov](https://apify.com/mantisus)\\n\\nThere\u2019s still room for improvement. Feel free to open issues, make pull requests, and [star the repository](https://github.com/apify/crawlee-python/) to spread the work to other developers.\\n\\n**We will award the first 10 pieces of feedback** that add value and are accepted by our team with an exclusive Crawlee for Python swag (The first Crawlee for Python swag ever). Check out the [GitHub issue here](https://github.com/apify/crawlee-python/issues/269/).\\n\\nWith such contributions, we\u2019re excited and looking forward to building an amazing library for the Python community.\\n\\nCheck out a step by step guide on how to use Crawlee for Python through one of our [latest tutorial](https://blog.apify.com/crawlee-for-python-tutorial/).\\n\\n[Join our Discord community](https://apify.com/discord) with nearly 8,000 web scraping developers, where our team would be happy to help you with any problems or discuss any use case for Crawlee for Python."},{"id":"proxy-management-in-crawlee","metadata":{"permalink":"/blog/proxy-management-in-crawlee","source":"@site/blog/2024/06-24-proxy-management-in-crawlee/index.md","title":"How Crawlee uses tiered proxies to avoid getting blocked","description":"Find out how Crawlee\u2019s tiered proxy system rotates between different types of proxies to control web scraping costs and avoid getting blocked.","date":"2024-06-24T00:00:00.000Z","tags":[{"inline":true,"label":"proxy","permalink":"/blog/tags/proxy"}],"readingTime":3.68,"hasTruncateMarker":true,"authors":[{"name":"Saurav Jain","title":"Developer Community Manager","url":"https://github.com/souravjain540","socials":{"x":"https://x.com/sauain","github":"https://github.com/souravjain540"},"imageURL":"https://avatars.githubusercontent.com/u/53312820?v=4","key":"SauravJ","page":null}],"frontMatter":{"slug":"proxy-management-in-crawlee","title":"How Crawlee uses tiered proxies to avoid getting blocked","tags":["proxy"],"description":"Find out how Crawlee\u2019s tiered proxy system rotates between different types of proxies to control web scraping costs and avoid getting blocked.","image":"./img/tiered-proxies.webp","authors":["SauravJ"]},"unlisted":false,"prevItem":{"title":"Announcing Crawlee for Python: Now you can use Python to build reliable web crawlers","permalink":"/blog/launching-crawlee-python"},"nextItem":{"title":"Building a Netflix show recommender using Crawlee and React","permalink":"/blog/netflix-show-recommender"}},"content":"Hello Crawlee community,\\n\\nWe are back with another blog, this time explaining how Crawlee rotates proxies and prevents crawlers from getting blocked.\\n\\nProxies vary in quality, speed, reliability, and cost. There are a [few types of proxies](https://blog.apify.com/types-of-proxies/), such as datacenter and residential proxies. Datacenter proxies are cheaper but, on the other hand, more prone to getting blocked, and vice versa with residential proxies.\\n\\nIt is hard for developers to decide which proxy to use while scraping data. We might get blocked if we use [datacenter proxies](https://blog.apify.com/datacenter-proxies-when-to-use-them-and-how-to-make-the-most-of-them/) for low-cost scraping, but residential proxies are sometimes too expensive for bigger projects. Developers need a system that can manage both costs and avoid getting blocked. To manage this, we recently introduced tiered proxies in Crawlee. Let\u2019s take a look at it.\\n\\n\x3c!-- truncate --\x3e\\n\\n:::note\\n\\nIf you like reading this blog, we would be really happy if you gave [Crawlee a star on GitHub!](https://github.com/apify/crawlee/)\\n\\n:::\\n\\n## What are tiered proxies?\\n\\nTiered proxies are a method of organizing and using different types of proxies based on their quality, speed, reliability, and cost. Tiered proxies allow you to rotate between a mix of proxy types to optimize your scraping activities.\\n\\nYou categorize your proxies into different tiers based on their quality. For example:\\n\\n-   **High-tier proxies**: Fast, reliable, and expensive. Best for critical tasks where you need high performance.\\n-   **Mid-tier proxies**: Moderate speed and reliability. A good balance between cost and performance.\\n-   **Low-tier proxies**: Slow and less reliable but cheap. Useful for less critical tasks or high-volume scraping.\\n\\n## Features:\\n\\n-   **Tracking errors**: The system monitors errors (e.g. failed requests, retries) for each domain.\\n-   **Adjusting tiers**: Higher-tier proxies are used if a domain shows more errors. Conversely, if a domain performs well with a high-tier proxy, the system will occasionally test lower-tier proxies. If successful, it continues using the lower tier, optimizing costs.\\n-   **Forgetting old errors**: Old errors are given less weight over time, allowing the system to adjust tiers dynamically as proxies\' performance changes.\\n\\n## Working\\n\\nThe `tieredProxyUrls` option in Crawlee\'s `ProxyConfigurationOptions` allows you to define a list of proxy URLs organized into tiers. Each tier represents a different level of quality, speed, and reliability.\\n\\n## Usage\\n\\n**Fallback Mechanism**: Crawlee starts with the first tier of proxies. If proxies in the current tier fail, it will switch to the next tier.\\n\\n\\n```js\\nimport { CheerioCrawler, ProxyConfiguration } from \'crawlee\';\\n\\nconst proxyConfiguration = new ProxyConfiguration({\\n    tieredProxyUrls: [\\n        [\'http://tier1-proxy1.example.com\', \'http://tier1-proxy2.example.com\'],\\n        [\'http://tier2-proxy1.example.com\', \'http://tier2-proxy2.example.com\'],\\n        [\'http://tier2-proxy1.example.com\', \'http://tier3-proxy2.example.com\'],\\n    ],\\n});\\n\\nconst crawler = new CheerioCrawler({\\n    proxyConfiguration,\\n    requestHandler: async ({ request, response }) => {\\n        // Handle the request\\n    },\\n});\\n\\nawait crawler.addRequests([\\n    { url: \'https://example.com/critical\' },\\n    { url: \'https://example.com/important\' },\\n    { url: \'https://example.com/regular\' },\\n]);\\n\\nawait crawler.run();\\n```\\n\\n## How tiered proxies use Session Pool under the hood\\n\\nA session pool is a way to manage multiple [sessions](https://crawlee.dev/api/core/class/Session) on a website so you can distribute your requests across them, reducing the chances of being detected and blocked. You can imagine each session like a different human user with its own IP address.\\n\\nWhen you use tiered proxies, each proxy tier works with the [session pool](https://crawlee.dev/api/core/class/SessionPool) to enhance request distribution and manage errors effectively.\\n\\n![Diagram explaining how tiered proxies use Session Pool under the hood](./img/session-pool-working.webp)\\n\\nFor each request, the crawler instance asks the `ProxyConfiguration` which proxy it should use. \' ProxyConfiguration` also keeps track of the requests domains, and if it sees more requests being retried or, say, more errors, it returns higher proxy tiers.\\n\\nIn each request, we must pass `sessionId` and the request URL to the proxy configuration to get the needed proxy URL from one of the tiers.\\n\\nChoosing which session to pass is where SessionPool comes in. Session pool automatically creates a pool of sessions, rotates them, and uses one of them without getting blocked and mimicking human-like behavior.\\n\\n## Conclusion: using proxies efficiently\\n\\nThis inbuilt feature is similar to what Scrapy\'s `scrapy-rotating-proxies` plugin offers to its users. The tiered proxy configuration dynamically adjusts proxy usage based on real-time performance data, optimizing cost and performance. The session pool ensures requests are distributed across multiple sessions, mimicking human behavior and reducing detection risk.\\n\\nWe hope this gives you a better understanding of how Crawlee manages proxies and sessions to make your scraping tasks more effective.\\n\\nAs always, we welcome your feedback. [Join our developer community on Discord](https://apify.com/discord) to ask any questions about Crawlee or tell us how you use it."},{"id":"netflix-show-recommender","metadata":{"permalink":"/blog/netflix-show-recommender","source":"@site/blog/2024/06-10-creating-a-netflix-show-recommender-using-crawlee-and-react/index.md","title":"Building a Netflix show recommender using Crawlee and React","description":"Create a Netflix show recommendation system using Crawlee to scrape the data, JavaScript to code, and React to build the front end.","date":"2024-06-10T00:00:00.000Z","tags":[{"inline":true,"label":"community","permalink":"/blog/tags/community"}],"readingTime":6.53,"hasTruncateMarker":true,"authors":[{"name":"Ayush Thakur","title":"Community Member of Crawlee","url":"https://github.com/ayush2390","socials":{"x":"https://x.com/JSAyushThakur","github":"https://github.com/ayush2390"},"imageURL":"https://avatars.githubusercontent.com/u/43995654?v=4","key":"AyushT","page":null}],"frontMatter":{"slug":"netflix-show-recommender","title":"Building a Netflix show recommender using Crawlee and React","tags":["community"],"description":"Create a Netflix show recommendation system using Crawlee to scrape the data, JavaScript to code, and React to build the front end.","image":"./img/create-netflix-show-recommender.webp","authors":["AyushT"]},"unlisted":false,"prevItem":{"title":"How Crawlee uses tiered proxies to avoid getting blocked","permalink":"/blog/proxy-management-in-crawlee"},"nextItem":{"title":"Scrapy vs. Crawlee","permalink":"/blog/scrapy-vs-crawlee"}},"content":"In this blog, we\'ll guide you through the process of using Vite and Crawlee to build a website that recommends Netflix shows based on their categories and genres. To do that, we will first scrape the shows and categories from Netflix using Crawlee, and then visualize the scraped data in a React app built with Vite. By the end of this guide, you\'ll have a functional web show recommender that can provide Netflix show suggestions.\\n\\n:::note\\nOne of our community members wrote this blog as a contribution to Crawlee Blog. If you want to contribute blogs like these to Crawlee Blog, please reach out to us on our [discord channel](https://apify.com/discord).\\n:::\\n\\n![How to scrape Netflix using Crawlee and React to build a show recommender](./img/create-netflix-show-recommender.webp)\\n\\n\x3c!-- truncate --\x3e\\n\\nLet\u2019s get started!\\n\\n## Prerequisites\\n\\nTo use Crawlee, you need to have Node.js 16 or newer.\\n\\n:::tip\\nIf you like the posts on the Crawlee blog so far, please consider [giving Crawlee a star on GitHub](https://github.com/apify/crawlee), it helps us to reach and help more developers.\\n:::\\n\\nYou can install the latest version of Node.js from the [official website](https://nodejs.org/en/). This great [Node.js installation guide](https://blog.apify.com/how-to-install-nodejs/) gives you tips to avoid issues later on.\\n\\n## Creating a React app\\n\\nFirst, we will create a React app (for the front end) using Vite. Run this command in the terminal to create it:\\n\\n```\\nnpx create-vite@latest\\n```\\n\\nYou can check out the [Vite Docs](https://vitejs.dev/guide/) for more details on how to create a React app.\\n\\nOnce the React app is created, open it in VS Code.\\n\\n![react](./img/react.webp)\\n\\nThis will be the structure of your React app.\\n\\nRun `npm run dev` command in the terminal to run the app.\\n\\n![viteandreact](./img/viteandreact.webp)\\n\\nThis will be the output displayed.\\n\\n## Adding Scraper code\\n\\nAs per our project requirements, we will scrape the genres and the titles of the shows available on Netflix.\\n\\nLet\u2019s start building the scraper code.\\n\\n### Installation\\n\\nRun this command to install `crawlee`:\\n\\n```\\nnpm install crawlee\\n```\\n\\nCrawlee utilizes Cheerio for HTML parsing and scraping of static websites. While faster and [less resource-intensive](https://crawlee.dev/docs/guides/scaling-crawlers), it can only scrape websites that do not require JavaScript rendering, making it unsuitable for SPAs (single page applications).\\n\\nIn this tutorial we can extract data from the HTML structure, so we will go with Cheerio but for extracting data from SPAs or JavaScript-rendered websites, Crawlee also supports headless browser libraries like [Playwright](https://playwright.dev/) and [Puppeteer](https://pptr.dev/)\\n\\nAfter installing the libraries, it\u2019s time to create the scraper code.\\n\\nCreate a file in `src` directory and name it `scraper.js`. The entire scraper code will be created in this file.\\n\\n### Scraping genres and shows\\n\\nTo scrape the genres and shows, we will utilize the [browser DevTools](https://developer.mozilla.org/en-US/docs/Learn/Common`questions/Tools`and`setup/What`are`browser`developer`tools) to identify the tags and CSS selectors targeting the genre elements on the Netflix website.\\n\\nWe can capture the HTML structure and call `$(element)` to query the element\'s subtree.\\n\\n![genre](./img/genre.webp)\\n\\nHere, we can observe that the name of the genre is captured by a `span` tag with `nm-collections-row-name` class. So we can use the `span.nm-collections-row-name` selector to capture this and similar elements.\\n\\n![title](./img/title.webp)\\n\\nSimilarly, we can observe that the title of the show is captured by the `span` tag having `nm-collections-title-name` class. So we can use the `span.nm-collections-title-name` selector to capture this and similar elements.\\n\\n```js\\n// Use parseWithCheerio for efficient HTML parsing\\nconst $ = await parseWithCheerio();\\n\\n// Extract genre and shows directly from the HTML structure\\nconst data = $(\'[data-uia=\\"collections-row\\"]\')\\n  .map((_, el) => {\\n    const genre = $(el)\\n      .find(\'[data-uia=\\"collections-row-title\\"]\')\\n      .text()\\n      .trim();\\n    const items = $(el)\\n      .find(\'[data-uia=\\"collections-title\\"]\')\\n      .map((_, itemEl) => $(itemEl).text().trim())\\n      .get();\\n    return { genre, items };\\n  })\\n  .get();\\n\\nconst genres = data.map((d) => d.genre);\\nconst shows = data.map((d) => d.items);\\n```\\n\\nIn the code snippet given above, we are using `parseWithCheerio` to parse the HTML content of the current page and extract `genres` and `shows` information from the HTML structure using Cheerio.\\n\\nThis will give the `genres` and `shows` array having list of genres and shows stored in it respectively.\\n### Storing data\\n\\nNow we have all the data that we want for our project and it\u2019s time to store or save the scraped data. To store the data, Crawlee comes with a `pushData()` method.\\n\\nThe [pushData()](https://crawlee.dev/docs/introduction/saving-data) method creates a storage folder in the project directory and stores the scraped data in JSON format.\\n\\n```js\\nawait pushData({\\n      genres: genres,\\n      shows: shows,\\n    });\\n```\\n\\nThis will save the `genres` and `shows` arrays as values in the `genres` and `shows` keys.\\n\\nHere\u2019s the full code that we will use in our project:\\n\\n```js\\nimport { CheerioCrawler, log, Dataset } from \\"crawlee\\";\\n\\nconst crawler = new CheerioCrawler({\\n  requestHandler: async ({ request, parseWithCheerio, pushData }) => {\\n    log.info(`Processing: ${request.url}`);\\n\\n    // Use parseWithCheerio for efficient HTML parsing\\n    const $ = await parseWithCheerio();\\n\\n    // Extract genre and shows directly from the HTML structure\\n    const data = $(\'[data-uia=\\"collections-row\\"]\')\\n      .map((_, el) => {\\n        const genre = $(el)\\n          .find(\'[data-uia=\\"collections-row-title\\"]\')\\n          .text()\\n          .trim();\\n        const items = $(el)\\n          .find(\'[data-uia=\\"collections-title\\"]\')\\n          .map((_, itemEl) => $(itemEl).text().trim())\\n          .get();\\n        return { genre, items };\\n      })\\n      .get();\\n\\n    // Prepare data for pushing\\n    const genres = data.map((d) => d.genre);\\n    const shows = data.map((d) => d.items);\\n\\n    await pushData({\\n      genres: genres,\\n      shows: shows,\\n    });\\n  },\\n\\n  // Limit crawls for efficiency\\n  maxRequestsPerCrawl: 20,\\n});\\n\\nawait crawler.run([\\"https://www.netflix.com/in/browse/genre/1191605\\"]);\\nawait Dataset.exportToJSON(\\"results\\");\\n\\n```\\n\\nNow, we will run Crawlee to scrape the website. Before running Crawlee, we need to tweak the `package.json` file. We will add the `start` script targeting the `scraper.js` file to run Crawlee.\\n\\nAdd the following code in `\'scripts\'` object:\\n\\n```\\n\\"start\\": \\"node src/scraper.js\\"\\n```\\n\\nand save it. Now run this command to run Crawlee to scrape the data:\\n\\n```sh\\nnpm start\\n```\\n\\nAfter running this command, you will see a `storage` folder with the `key_value_stores/default/results.json` file. The scraped data will be stored in JSON format in this file.\\n\\nNow we can use this JSON data and display it in the `App.jsx` component to create the project.\\n\\nIn the `App.jsx` component, we will import `jsonData` from the `results.json` file:\\n\\n```js\\nimport { useState } from \\"react\\";\\nimport \\"./App.css\\";\\nimport jsonData from \\"../storage/key_value_stores/default/results.json\\";\\n\\nfunction HeaderAndSelector({ handleChange }) {\\n  return (\\n    <>\\n      <h1 className=\\"header\\">Netflix Web Show Recommender</h1>\\n      <div className=\\"genre-selector\\">\\n        <select onChange={handleChange} className=\\"select-genre\\">\\n          <option value=\\"\\">Select your genre</option>\\n          {jsonData[0].genres.map((genres, key) => {\\n            return (\\n              <option key={key} value={key}>\\n                {genres}\\n              </option>\\n            );\\n          })}\\n        </select>\\n      </div>\\n    </>\\n  );\\n}\\n\\nfunction App() {\\n  const [count, setCount] = useState(null);\\n\\n  const handleChange = (event) => {\\n    const value = event.target.value;\\n    if (value) setCount(parseInt(value));\\n  };\\n\\n  // Validate count to ensure it is within the bounds of the jsonData.shows array\\n  const isValidCount = count !== null && count <= jsonData[0].shows.length;\\n\\n  return (\\n    <div className=\\"app-container\\">\\n      <HeaderAndSelector handleChange={handleChange} />\\n      <div className=\\"shows-container\\">\\n        {isValidCount && (\\n          <>\\n            <div className=\\"shows-list\\">\\n              <ul>\\n                {jsonData[0].shows[count].slice(0, 20).map((show, index) => (\\n                  <li key={index} className=\\"show-item\\">\\n                    {show}\\n                  </li>\\n                ))}\\n              </ul>\\n            </div>\\n            <div className=\\"shows-list\\">\\n              <ul>\\n                {jsonData[0].shows[count].slice(20).map((show, index) => (\\n                  <li key={index} className=\\"show-item\\">\\n                    {show}\\n                  </li>\\n                ))}\\n              </ul>\\n            </div>\\n          </>\\n        )}\\n      </div>\\n    </div>\\n  );\\n}\\n\\nexport default App;\\n```\\n\\nIn this code snippet, the `genre` array is used to display the list of genres. User can select their desired genre and based upon that a list of web shows available on Netflix will be displayed using the `shows` array.\\n\\nMake sure to update CSS on the `App.css` file from here: [https://github.com/ayush2390/web-show-recommender/blob/main/src/App.css](https://github.com/ayush2390/web-show-recommender/blob/main/src/App.css)\\n\\nand download and save this image file in main project folder: [Download Image](https://raw.githubusercontent.com/ayush2390/web-show-recommender/main/Netflix.png)\\n\\nOur project is ready!\\n\\n## Result\\n\\nNow, to run your project on localhost, run this command:\\n\\n```\\nnpm run dev\\n```\\n\\nThis command will run your project on localhost. Here is a demo of the project:\\n\\n![result](./img/result.gif)\\n\\nProject link - [https://github.com/ayush2390/web-show-recommender](https://github.com/ayush2390/web-show-recommender)\\n\\nIn this project, we used Crawlee to scrape Netflix; similarly, Crawlee can be used to scrape single application pages (SPAs) and JavaScript-rendered websites. The best part is all of this can be done while coding in JavaScript/TypeScript and using a single library.\\n\\nIf you want to learn more about Crawlee, go through the [documentation](https://crawlee.dev/docs/quick-start) and this step-by-step [Crawlee web scraping tutorial](https://blog.apify.com/crawlee-web-scraping-tutorial/) from Apify."},{"id":"scrapy-vs-crawlee","metadata":{"permalink":"/blog/scrapy-vs-crawlee","source":"@site/blog/2024/04-23-scrapy-vs-crawlee/index.md","title":"Scrapy vs. Crawlee","description":"Which web scraping library should you use in 2024? Learn how each handles headless mode, autoscaling, proxy rotation, errors, and anti-scraping techniques.","date":"2024-04-23T00:00:00.000Z","tags":[],"readingTime":10.28,"hasTruncateMarker":true,"authors":[{"name":"Saurav Jain","title":"Developer Community Manager","url":"https://github.com/souravjain540","socials":{"x":"https://x.com/sauain","github":"https://github.com/souravjain540"},"imageURL":"https://avatars.githubusercontent.com/u/53312820?v=4","key":"SauravJ","page":null}],"frontMatter":{"slug":"scrapy-vs-crawlee","title":"Scrapy vs. Crawlee","description":"Which web scraping library should you use in 2024? Learn how each handles headless mode, autoscaling, proxy rotation, errors, and anti-scraping techniques.","image":"./img/scrapy-vs-crawlee.webp","authors":["SauravJ"]},"unlisted":false,"prevItem":{"title":"Building a Netflix show recommender using Crawlee and React","permalink":"/blog/netflix-show-recommender"},"nextItem":{"title":"How to scrape Amazon products","permalink":"/blog/how-to-scrape-amazon"}},"content":"import Tabs from \'@theme/Tabs\';\\nimport TabItem from \'@theme/TabItem\';\\n\\nHey, crawling masters!\\n\\nWelcome to another post on the Crawlee blog; this time, we are going to compare Scrapy, one of the oldest and most popular web scraping libraries in the world, with Crawlee, a relative newcomer. This article will answer your questions about when to use Scrapy and help you decide when it would be better to use Crawlee instead. This article will be the first in a series comparing the various technical aspects of Crawlee with Scrapy.\\n\\n## Introduction:\\n\\n[Scrapy](https://scrapy.org/) is an open-source Python-based web scraping framework that extracts data from websites. With Scrapy, you create spiders, which are autonomous scripts to download and process web content. The limitation of Scrapy is that it does not work very well with JavaScript rendered websites, as it was designed for static HTML pages. We will do a comparison later in the article about this.\\n\\nCrawlee is also an open-source library that originated as [Apify SDK](https://docs.apify.com/sdk/js/). Crawlee has the advantage of being the latest library in the market, so it already has many features that Scrapy lacks, like autoscaling, headless browsing, working with JavaScript rendered websites without any plugins, and many more, which we are going to explain later on.\\n\\n\x3c!-- truncate --\x3e\\n\\n## Feature comparison\\n\\n\\nWe\'ll start comparing Scrapy and Crawlee by looking at language and development environments, and then features to make the scraping process easier for developers, like autoscaling, headless browsing, queue management, and more.\\n\\n\\n### Language and development environments\\n\\nScrapy is written in Python, making it easier for the data science community to integrate it with various tools. While Scrapy offers very detailed documentation, it can take a lot of work to get started with Scrapy. One of the reasons why it is considered not so beginner-friendly[[1]](https://towardsdatascience.com/web-scraping-with-scrapy-theoretical-understanding-f8639a25d9cd)[[2]](https://www.accordbox.com/blog/scrapy-tutorial-1-scrapy-vs-beautiful-soup/#:~:text=Since%20Scrapy%20does%20no%20only,to%20become%20a%20Scrapy%20expert.)[[3]](https://www.udemy.com/tutorial/scrapy-tutorial-web-scraping-with-python/scrapy-vs-beautiful-soup-vs-selenium//1000) is its [complex architecture](https://docs.scrapy.org/en/latest/topics/architecture.html), which consists of various components like spiders, middleware, item pipelines, and settings. These can be challenging for beginners.\\n\\nCrawlee is one of the few web scraping and automation libraries that supports JavaScript and TypeScript. Crawlee supports CLI just like Scrapy, but it also provides [pre-built templates](https://github.com/apify/crawlee/tree/master/packages/templates/templates) in TypeScript and JavaScript with support for Playwright and Puppeteer. These templates help beginners to quickly understand the file structure and how it works.\\n\\n### Headless browsing and JS rendering\\n\\nScrapy does not support headless browsers natively, but it supports them with its plugin system, similarly it does not support scraping JavaScript rendered websites, but the plugin system makes this possible. One of the best examples is its [Playwright plugin](https://github.com/scrapy-plugins/scrapy-playwright/tree/main).\\n\\nApify Store is a JavaScript rendered website, so we will scrape it in this example using the `scrapy-playwright` integration.\\n\\nFor installation and to make changes to [`settings.py`], please follow the instructions on the `scrapy-playwright` [repository on GitHub](https://github.com/scrapy-plugins/scrapy-playwright/tree/main?tab=readme-ov-file#installation).\\n\\nThen, create a spider with this code to scrape the data:\\n\\n\\n```py title=\\"spider.py\\"\\nimport scrapy\\n\\nclass ActorSpider(scrapy.Spider):\\n    name = \'actor_spider\'\\n    start_urls = [\'https://apify.com/store\']\\n\\n    def start_requests(self):\\n        for url in self.start_urls:\\n            yield scrapy.Request(\\n                url,\\n                meta={\\"playwright\\": True, \\"playwright_include_page\\": True},\\n                callback=self.parse_playwright\\n            )\\n\\n    async def parse_playwright(self, response):\\n        page = response.meta[\'playwright_page\']\\n        await page.wait_for_selector(\'.ActorStoreItem-title-wrapper\')\\n        actor_card = await page.query_selector(\'.ActorStoreItem-title-wrapper\')\\n\\n        if actor_card:\\n            actor_text = await actor_card.text_content()\\n            yield {\\n                \'actor\': actor_text.strip() if actor_text else \'N/A\'\\n            }\\n\\n        await page.close()\\n```\\n\\nOne of the drawbacks of this plugin is its [lack of native support for windows](https://github.com/scrapy-plugins/scrapy-playwright/tree/main?tab=readme-ov-file#lack-of-native-support-for-windows).\\n\\nIn Crawlee, you can scrape JavaScript rendered websites using the built-in headless [Puppeteer](https://github.com/puppeteer/puppeteer/) and [Playwright](https://github.com/microsoft/playwright) browsers. It is important to note that, by default, Crawlee scrapes in headless mode. If you don\'t want headless, then just set `headless: false`.\\n\\n<Tabs>\\n<TabItem value=\\"javascript\\" label=\\"Playwright\\">\\n\\n```js title=\\"crawler.js\\"\\nimport { PlaywrightCrawler } from \'crawlee\';\\n\\nconst crawler = new PlaywrightCrawler({\\n    async requestHandler({ page }) {\\n        const actorCard = page.locator(\'.ActorStoreItem-title-wrapper\').first();\\n        const actorText = await actorCard.textContent();\\n        await crawler.pushData({\\n            \'actor\': actorText\\n        });\\n    },\\n});\\n\\nawait crawler.run([\'https://apify.com/store\']);\\n```\\n\\n</TabItem>\\n<TabItem value=\\"js\\" label=\\"Puppeteer\\">\\n\\n```js title=\\"crawler.js\\"\\nimport { PuppeteerCrawler } from \'crawlee\';\\n\\nconst crawler = new PuppeteerCrawler({\\n    async requestHandler({ page }) {\\n        await page.waitForSelector(\'.ActorStoreItem-title-wrapper\');\\n        const actorText = await page.$eval(\'.ActorStoreItem-title-wrapper\', (el) => {\\n            return el.textContent;\\n        });\\n        await crawler.pushData({\\n            \'actor\': actorText\\n        });\\n    },\\n});\\n\\nawait crawler.run([\'https://apify.com/store\']);\\n```\\n\\n</TabItem>\\n</Tabs>\\n\\n### Autoscaling support\\n\\nAutoscaling refers to the capability of a library to automatically adjusting the number of concurrent tasks (such as browser instances, HTTP requests, etc.) based on the current load and system resources. This feature is particularly useful when handling web scraping and crawling tasks that may require dynamically scaled resources to optimize performance, manage system load, and handle rate limitations efficiently.\\n\\nScrapy does not have built-in autoscaling capabilities, but it can be done using external services like [Scrapyd](https://scrapyd.readthedocs.io/en/latest/) or deployed in a distributed manner with Scrapy Cluster.\\n\\nCrawlee has [built-in autoscaling](https://crawlee.dev/api/core/class/AutoscaledPool) with `AutoscaledPool`. It increases the number of requests that are processed concurrently within one crawler.\\n\\n### Queue management\\n\\nScrapy supports both breadth-first and depth-first crawling strategies using a disk-based queuing system. By default, it uses the LIFO queue for the pending requests, which means it is using depth-first order, but if you want to use breadth-first order, you can do it by changing these settings:\\n\\n```py title=\\"settings.py\\"\\nDEPTH_PRIORITY = 1\\nSCHEDULER_DISK_QUEUE = \\"scrapy.squeues.PickleFifoDiskQueue\\"\\nSCHEDULER_MEMORY_QUEUE = \\"scrapy.squeues.FifoMemoryQueue\\"\\n```\\n\\nCrawlee uses breadth-first by default and you can override it on a per-request basis by using the `forefront: true` argument in `addRequest` and its derivatives. If you use `forefront: true` for all requests, it becomes a depth-first process.\\n\\n### CLI support\\n\\nScrapy has a [powerful command-line interface](https://docs.scrapy.org/en/latest/topics/commands.html#command-line-tool) that offers functionalities like starting a project, generating spiders, and controlling the crawling process.\\n\\nScrapy CLI comes with Scrapy. Just run this command, and you are good to go:\\n\\n```bash\\npip install scrapy\\n```\\n\\nCrawlee also [includes a CLI tool](https://crawlee.dev/docs/quick-start#installation-with-crawlee-cli) (`crawlee-cli`) that facilitates project setup, crawler creation and execution, streamlining the development process for users familiar with Node.js environments. The command for installation is:\\n\\n```bash\\nnpx crawlee create my-crawler\\n```\\n\\n### Proxy rotation and storage management\\n\\nScrapy handles it via custom middleware. You have to install their [`scrapy-rotating-proxies`](https://pypi.org/project/scrapy-rotating-proxies/) package using pip.\\n\\n```bash\\npip install scrapy-rotating-proxies\\n```\\n\\nThen in the `settings.py` file, add `ROTATING_PROXY_LIST` and the middleware to the `DOWNLOADER_MIDDLEWARES` and specify the list of proxy servers. For example:\\n\\n```py title=\\"settings.py\\"\\nDOWNLOADER_MIDDLEWARES = {\\n    # Lower value means higher priority\\n    \'scrapy.downloadermiddlewares.retry.RetryMiddleware\': 90,\\n    \'scrapy_rotating_proxies.middlewares.RotatingProxyMiddleware\': 610,\\n    \'scrapy_rotating_proxies.middlewares.BanDetectionMiddleware\': 620,\\n}\\n\\nROTATING_PROXY_LIST = [\\n    \'proxy1.com:8000\',\\n    \'proxy2.com:8031\',\\n    # Add more proxies as needed\\n]\\n```\\n\\nNow create a spider with the code you want to scrape any site and the `ROTATING_PROXY_LIST` in `settings.py` will manage which proxy to use for each request. Here middleware will treat each proxy initially as valid and then when a request is made, the middleware selects a proxy from the list of available proxies. The selection isn\'t purely sequential but is influenced by the recent history of proxy performance. The middleware has mechanisms to detect when a proxy might be banned or rendered ineffective. When such conditions are detected, the proxy is temporarily deactivated and put into a cooldown period. After the cooldown period expires, the proxy is reconsidered for use.\\n\\nIn Crawlee, you can [use your own proxy servers](https://crawlee.dev/docs/guides/proxy-management) or proxy servers acquired from third-party providers. If you already have your proxy URLs, you can start using them like this:\\n\\n```js title=\\"crawler.js\\"\\nimport { ProxyConfiguration } from \'crawlee\';\\n\\nconst proxyConfiguration = new ProxyConfiguration({\\n    proxyUrls: [\\n        \'http://proxy1.example.com\',\\n        \'http://proxy2.example.com\',\\n    ]\\n});\\nconst crawler = new CheerioCrawler({\\n    proxyConfiguration,\\n    // ...\\n});\\n```\\n\\nCrawlee also has [`SessionPool`](https://crawlee.dev/api/core/class/SessionPool), a built-in allocation system for proxies. It handles the rotation, creation, and persistence of user-like sessions. It creates a pool of session instances that are randomly rotated.\\n\\n### Data storage\\n\\nOne of the most frequently required features when implementing scrapers is being able to store the scraped data as an \\"export file\\".\\n\\nScrapy provides this functionality out of the box with the [`Feed Exports`](https://docs.scrapy.org/en/latest/topics/feed-exports.html), which allows it to generate feeds with the scraped items, using multiple serialization formats and storage backends. It supports `CSV, JSON, JSON Lines, and XML.`\\n\\nTo do this, you need to modify your `settings.py` file and enter:\\n\\n```py title=\\"settings.py\\"\\n# To store in CSV format\\nFEEDS = {\\n    \'data/crawl_data.csv\': {\'format\': \'csv\', \'overwrite\': True}\\n}\\n\\n# OR to store in JSON format\\n\\nFEEDS = {\\n    \'data/crawl_data.json\': {\'format\': \'json\', \'overwrite\': True}\\n}\\n```\\n\\nCrawlee\'s storage can be divided into two categories: Request Storage (Request Queue and Request List) and Results Storage (Datasets and Key Value Stores). Both are stored locally by default in the `./storage` directory.\\n\\nAlso, remember that Crawlee, by default, clears its storages before starting a crawler run. This action is taken to prevent old data from interfering with new crawling sessions.\\n\\nLet\'s see how Crawlee stores the result:\\n\\n- You can use local storage with dataset\\n\\n    ```js title=\\"crawler.js\\"\\n    import { PlaywrightCrawler } from \'crawlee\';\\n\\n    const crawler = new PlaywrightCrawler({\\n        requestHandler: async ({ page }) => {\\n\\n            const title = await page.title();\\n            const price = await page.textContent(\'.price\');\\n\\n            await crawler.pushData({\\n                url: request.url,\\n                title,\\n                price\\n            });\\n        }\\n    })\\n\\n    await crawler.run([\'http://example.com\']);\\n    ```\\n\\n- Using Key-Value Store\\n\\n    ```js title=\\"crawler.js\\"\\n    import { KeyValueStore } from \'crawlee\';\\n    //... Code to crawl the data\\n    await KeyValueStore.setValue(\'key\', { foo: \'bar\' });\\n    ```\\n\\n### Anti-blocking and fingerprints\\n\\nIn Scrapy, handling anti-blocking strategies like [IP rotation](https://pypi.org/project/scrapy-rotated-proxy/), [user-agent rotation](https://python.plainenglish.io/rotating-user-agent-with-scrapy-78ca141969fe), custom solutions via middleware, and plugins are needed.\\n\\nCrawlee provides HTTP crawling and [browser fingerprints](https://crawlee.dev/docs/guides/avoid-blocking) with zero configuration necessary; fingerprints are enabled by default and available in `PlaywrightCrawler` and `PuppeteerCrawler` but also work with `CheerioCrawler` and the other HTTP Crawlers.\\n\\n### Error handling\\n\\nBoth libraries support error-handling practices like automatic retries, logging, and custom error handling.\\n\\nIn Scrapy, you can handle errors using middleware and [signals](https://docs.scrapy.org/en/latest/topics/signals.html). There are also [exceptions](https://docs.scrapy.org/en/latest/topics/exceptions.html) like `IgnoreRequest`, which can be raised by Scheduler or any downloader middleware to indicate that the request should be ignored. Similarly, a spider callback can raise\' CloseSpider\' to close the spider.\\n\\nScrapy has built-in support for retrying failed requests. You can configure the retry policy (e.g., the number of retries, retrying on particular HTTP codes) via settings such as `RETRY_TIMES`, as shown in the example:\\n\\n```py title=\\"settings.py\\"\\nRETRY_ENABLED = True\\nRETRY_TIMES = 2  # Number of retry attempts\\nRETRY_HTTP_CODES = [500, 502, 503, 504, 522, 524]  # HTTP error codes to retry\\n```\\n\\nIn Crawlee, you can also set up a custom error handler. For retries, `maxRequestRetries` controls how often Crawlee will retry a request before marking it as failed. To set it up, you just need to add the following line of code in your crawler.\\n\\n```js title=\\"crawler.js\\"\\nconst crawler = new CheerioCrawler({\\n    maxRequestRetries: 3 // Crawler will retry three times.\\n    // ...\\n})\\n```\\n\\nThere is also `noRetry`. If set to `true` then the request will not be automatically tried.\\n\\nCrawlee also provides a built-in [logging mechanism](https://crawlee.dev/api/core/class/Log) via `log`, allowing you to log warnings, errors, and other information effectively.\\n\\n### Deployment using Docker\\n\\nScrapy can be containerized using Docker, though it typically requires manual setup to create Dockerfiles and configure environments. While Crawlee includes [ready-to-use Docker configurations](https://crawlee.dev/docs/guides/docker-images), making deployment straightforward across various environments without additional configuration.\\n\\n## Community\\n\\nBoth projects are open source. Scrapy benefits from a large and well-established community. It has been around since 2008 and has attracted a lot of attention among developers, particularly those in the Python ecosystem.\\n\\nCrawlee started its journey as Apify SDK in 2018. It now has more than [12K stars on GitHub](https://github.com/apify/crawlee), a community of more than 7,000 developers in its [Discord Community](https://apify.com/discord), and is used by the TypeScript and JavaScript community.\\n\\n## So which is better - Scrapy or Crawlee?\\n\\nBoth frameworks can handle a wide range of scraping tasks, and the best choice will depend on specific technical needs like language preference, project requirements, ease of use, etc.\\n\\nIf you are comfortable with Python and want to work only with it, go with Scrapy. It has very detailed documentation, and it is one of the oldest and most stable libraries in the space.\\n\\nBut if you want to explore or are comfortable working with TypeScript or JavaScript, our recommendation is Crawlee. With all the valuable features like a single interface for HTTP requests and headless browsing, making it work well with JavaScript rendered websites, autoscaling and fingerprint support, it is the best choice for scraping websites that can be complex, resource intensive, using JavaScript, or even have blocking methods.\\n\\nAs promised, this is just the first of the many articles comparing Scrapy and Crawlee. With the upcoming articles, you will learn more about every technical detail.\\n\\nMeanwhile, if you want to learn more about Crawlee, read our [introduction to Crawlee](https://crawlee.dev/docs/introduction) or Apify\'s [Crawlee web scraping tutorial](https://blog.apify.com/crawlee-web-scraping-tutorial/)."},{"id":"how-to-scrape-amazon","metadata":{"permalink":"/blog/how-to-scrape-amazon","source":"@site/blog/2024/03-27-how-to-scrape-amazon-using-typescript-cheerio-and-crawlee/index.md","title":"How to scrape Amazon products","description":"A detailed step-by-step guide to scraping products on Amazon using TypeScript, Cheerio, and Crawlee.","date":"2024-03-27T00:00:00.000Z","tags":[],"readingTime":11.49,"hasTruncateMarker":true,"authors":[{"name":"Luk\xe1\u0161 Pr\u016F\u0161a","title":"Junior Web Automation Engineer","url":"https://github.com/Patai5","socials":{"github":"https://github.com/Patai5"},"imageURL":"./img/lukasp.webp","key":"LukasP","page":null}],"frontMatter":{"slug":"how-to-scrape-amazon","title":"How to scrape Amazon products","description":"A detailed step-by-step guide to scraping products on Amazon using TypeScript, Cheerio, and Crawlee.","image":"./img/how-to-scrape-amazon.webp","authors":["LukasP"]},"unlisted":false,"prevItem":{"title":"Scrapy vs. Crawlee","permalink":"/blog/scrapy-vs-crawlee"},"nextItem":{"title":"Launching Crawlee Blog","permalink":"/blog/crawlee-blog-launch"}},"content":"## Introduction\\n\\nAmazon is one of the largest and most complex websites, which means scraping it is pretty challenging. Thankfully, the Crawlee library makes things a little easier, with utilities like JSON file outputs, automatic scaling, and request queue management.\\n\\nIn this guide, we\'ll be extracting information from Amazon product pages using the power of [TypeScript](https://www.typescriptlang.org) in combination with the [Cheerio](https://cheerio.js.org) and [Crawlee](https://crawlee.dev) libraries. We\'ll explore how to retrieve and extract detailed product data such as titles, prices, image URLs, and more from Amazon\'s vast marketplace. We\'ll also discuss handling potential blocking issues that may arise during the scraping process.\\n\\n![How to scrape Amazon using Typescript, Cheerio, and Crawlee](./img/how-to-scrape-amazon.webp)\\n\\n\x3c!-- truncate --\x3e\\n\\n## Prerequisites\\n\\nYou\'ll find the journey smoother if you have a decent grasp of the TypeScript language and a fundamental understanding of [HTML](https://developer.mozilla.org/en-US/docs/Web/HTML) structure. A familiarity with Cheerio and Crawlee is advised but optional. This guide is built to introduce these tools and their use cases in an approachable manner.\\n\\nCrawlee is open-source with nearly 12,000 stars on GitHub. You can check out the [source code here](https://github.com/apify/crawlee). Feel free to play with Crawlee with the inbuilt templates that they offer.\\n\\n## Writing the scraper\\n\\nTo begin with, let\'s identify the product fields that we\'re interested in scraping:\\n\\n-   Product Title\\n-   Price\\n-   List Price\\n-   Review Rating\\n-   Review Count\\n-   Image URLs\\n-   Product Overview Attributes\\n\\n![Image highlighting the product fields to be scraped on Amazon](./img/fields-to-scrape.webp)\\n\\nFor now, our focus will be solely on the scraping part. In a later section, we\'ll shift our attention to Crawlee, our crawling tool. Let\'s begin!\\n\\n### Scraping the individual data points\\n\\nOur first step will be to utilize [browser DevTools](https://developer.mozilla.org/en-US/docs/Learn/Common_questions/Tools_and_setup/What_are_browser_developer_tools) to inspect the layout and discover the [CSS selectors](https://developer.mozilla.org/en-US/docs/Learn/CSS/Building_blocks/Selectors) for the data points we aim to scrape. (by default on [Chrome](https://developer.chrome.com/docs/devtools), press `Ctrl + Shift + C`)\\n\\nFor example, let\'s take a look at how we find the selector for the product title:\\n![Amazon product title selector in DevTools](./img/dev-tools-example.webp)\\n\\nThe product title selector we\'ve deduced is `span#productTitle`. This selector targets all `span` elements with the id of `productTitle`. Luckily, there\'s only one such element on the page - exactly what we\'re after.\\n\\nWe can find the selectors for the remaining data points using the same principle combined with a sprinkle of trial and error. Next, let\'s write a function that uses a [Cheerio object](https://cheerio.js.org/docs/api/interfaces/CheerioAPI) of the product page as input and outputs our extracted data in a structured format.\\n\\nInitially, we\'ll focus on scraping simple data points. We\'ll leave the more complex ones, like image URLs and product attributes overview, for later.\\n\\n```typescript\\nimport { CheerioAPI } from \'cheerio\';\\n\\ntype ProductDetails = {\\n    title: string;\\n    price: string;\\n    listPrice: string;\\n    reviewRating: string;\\n    reviewCount: string;\\n};\\n\\n/**\\n * CSS selectors for the product details. Feel free to figure out different variations of these selectors.\\n */\\nconst SELECTORS = {\\n    TITLE: \'span#productTitle\',\\n    PRICE: \'span.priceToPay\',\\n    LIST_PRICE: \'span.basisPrice .a-offscreen\',\\n    REVIEW_RATING: \'#acrPopover a > span\',\\n    REVIEW_COUNT: \'#acrCustomerReviewText\',\\n} as const;\\n\\n/**\\n * Scrapes the product details from the given Cheerio object.\\n */\\nexport const extractProductDetails = ($: CheerioAPI): ProductDetails => {\\n    const title = $(SELECTORS.TITLE).text().trim();\\n\\n    const price = $(SELECTORS.PRICE).first().text();\\n    const listPrice = $(SELECTORS.LIST_PRICE).first().text();\\n    const reviewRating = $(SELECTORS.REVIEW_RATING).first().text();\\n    const reviewCount = $(SELECTORS.REVIEW_COUNT).first().text();\\n\\n    return { title, price, listPrice, reviewRating, reviewCount };\\n};\\n```\\n\\n## Improving the scraper\\n\\nAt this point, our scraper extracts all fields as strings, which isn\'t ideal for numerical fields like prices and review counts - we\'d rather have those as numbers.\\n\\nSimple casting from string to numbers will only work for some fields. In some cases, such as processing the price fields, we must clean the string and remove unnecessary characters before conversion. To address this, write a utility function parsing a number from a string. We\'ll also have another function to find the first element matching our selector and return it parsed as a number.\\n\\n```typescript\\n/**\\n * Parses a number from a string by removing all non-numeric characters.\\n * - Keeps the decimal point.\\n */\\nconst parseNumberValue = (rawString: string): number => {\\n    return Number(rawString.replace(/[^\\\\d.]+/g, \'\'));\\n};\\n\\n/**\\n * Parses a number value from the first element matching the given selector.\\n */\\nexport const parseNumberFromSelector = ($: CheerioAPI, selector: string): number => {\\n    const rawValue = $(selector).first().text();\\n    return parseNumberValue(rawValue);\\n};\\n```\\n\\nWith the function above: `parseNumberValue`, we can now update and simplify the main scraping function `extractProductDetails`.\\n\\n```typescript\\nimport { CheerioAPI } from \'cheerio\';\\nimport { parseNumberFromSelector } from \'./utils.js\';\\n\\ntype ProductDetails = {\\n    title: string;\\n    price: number;        //\\n    listPrice: number;    // updated to numbers\\n    reviewRating: number; //\\n    reviewCount: number;  //\\n};\\n\\n...\\n\\n/**\\n * Scrapes the product details from the given Cheerio object.\\n */\\nexport const extractProductDetails = ($: CheerioAPI): ProductDetails => {\\n    const title = $(SELECTORS.TITLE).text().trim();\\n\\n    const price = parseNumberFromSelector($, SELECTORS.PRICE);\\n    const listPrice = parseNumberFromSelector($, SELECTORS.LIST_PRICE);\\n    const reviewRating = parseNumberFromSelector($, SELECTORS.REVIEW_RATING);\\n    const reviewCount = parseNumberFromSelector($, SELECTORS.REVIEW_COUNT);\\n\\n    return { title, price, listPrice, reviewRating, reviewCount };\\n};\\n```\\n\\n### Scraping the advanced data points\\n\\nAs we progress in our scraping journey, it\'s time to focus on the more complex data fields, like image URLs and product attributes overview. To extract data from these fields, we must utilize the `map` function to iterate over all matching elements and fetch data from each. Let\'s start with image URLs.\\n\\n```typescript\\nconst SELECTORS = {\\n    ...\\n    IMAGES: \'#altImages .item img\',\\n} as const;\\n\\n/**\\n * Extracts the product image URLs from the given Cheerio object.\\n * - We have to iterate over the image elements and extract the `src` attribute.\\n */\\nconst extractImageUrls = ($: CheerioAPI): string[] => {\\n    const imageUrls = $(SELECTORS.IMAGES)\\n        .map((_, imageEl) => $(imageEl).attr(\'src\'))\\n        .get(); // `get()` - Retrieve all elements matched by the Cheerio object, as an array. Removes `undefined` values.\\n\\n    return imageUrls;\\n};\\n```\\n\\nExtracting images is relatively simple yet still deserves a separate function for clarity. We\'ll now parse the product attributes overview.\\n\\n```typescript\\ntype ProductAttribute = {\\n    label: string;\\n    value: string;\\n};\\n\\nconst SELECTORS = {\\n    ...\\n    PRODUCT_ATTRIBUTE_ROWS: \'#productOverview_feature_div tr\',\\n    ATTRIBUTES_LABEL: \'td:nth-of-type(1) span\',\\n    ATTRIBUTES_VALUE: \'td:nth-of-type(2) span\',\\n} as const;\\n\\n/**\\n * Extracts the product attributes from the given Cheerio object.\\n * - We have to iterate over the attribute rows and extract both label and value for each row.\\n */\\nconst extractProductAttributes = ($: CheerioAPI): ProductAttribute[] => {\\n    const attributeRowEls = $(SELECTORS.PRODUCT_ATTRIBUTE_ROWS).get();\\n\\n    const attributeRows = attributeRowEls.map((rowEl) => {\\n        const label = $(rowEl).find(SELECTORS.ATTRIBUTES_LABEL).text();\\n        const value = $(rowEl).find(SELECTORS.ATTRIBUTES_VALUE).text();\\n\\n        return { label, value };\\n    });\\n\\n    return attributeRows;\\n};\\n```\\n\\nWe\'ve now effectively crafted our scraping functions. Here\'s the complete `scraper.ts` file:\\n\\n```typescript\\nimport { CheerioAPI } from \'cheerio\';\\nimport { parseNumberFromSelector } from \'./utils.js\';\\n\\ntype ProductAttribute = {\\n    label: string;\\n    value: string;\\n};\\n\\ntype ProductDetails = {\\n    title: string;\\n    price: number;\\n    listPrice: number;\\n    reviewRating: number;\\n    reviewCount: number;\\n    imageUrls: string[];\\n    attributes: ProductAttribute[];\\n};\\n\\n/**\\n * CSS selectors for the product details. Feel free to figure out different variations of these selectors.\\n */\\nconst SELECTORS = {\\n    TITLE: \'span#productTitle\',\\n    PRICE: \'span.priceToPay\',\\n    LIST_PRICE: \'span.basisPrice .a-offscreen\',\\n    REVIEW_RATING: \'#acrPopover a > span\',\\n    REVIEW_COUNT: \'#acrCustomerReviewText\',\\n    IMAGES: \'#altImages .item img\',\\n\\n    PRODUCT_ATTRIBUTE_ROWS: \'#productOverview_feature_div tr\',\\n    ATTRIBUTES_LABEL: \'td:nth-of-type(1) span\',\\n    ATTRIBUTES_VALUE: \'td:nth-of-type(2) span\',\\n} as const;\\n\\n/**\\n * Extracts the product image URLs from the given Cheerio object.\\n * - We have to iterate over the image elements and extract the `src` attribute.\\n */\\nconst extractImageUrls = ($: CheerioAPI): string[] => {\\n    const imageUrls = $(SELECTORS.IMAGES)\\n        .map((_, imageEl) => $(imageEl).attr(\'src\'))\\n        .get(); // `get()` - Retrieve all elements matched by the Cheerio object, as an array. Removes `undefined` values.\\n\\n    return imageUrls;\\n};\\n\\n/**\\n * Extracts the product attributes from the given Cheerio object.\\n * - We have to iterate over the attribute rows and extract both label and value for each row.\\n */\\nconst extractProductAttributes = ($: CheerioAPI): ProductAttribute[] => {\\n    const attributeRowEls = $(SELECTORS.PRODUCT_ATTRIBUTE_ROWS).get();\\n\\n    const attributeRows = attributeRowEls.map((rowEl) => {\\n        const label = $(rowEl).find(SELECTORS.ATTRIBUTES_LABEL).text();\\n        const value = $(rowEl).find(SELECTORS.ATTRIBUTES_VALUE).text();\\n\\n        return { label, value };\\n    });\\n\\n    return attributeRows;\\n};\\n\\n/**\\n * Scrapes the product details from the given Cheerio object.\\n */\\nexport const extractProductDetails = ($: CheerioAPI): ProductDetails => {\\n    const title = $(SELECTORS.TITLE).text().trim();\\n\\n    const price = parseNumberFromSelector($, SELECTORS.PRICE);\\n    const listPrice = parseNumberFromSelector($, SELECTORS.LIST_PRICE);\\n    const reviewRating = parseNumberFromSelector($, SELECTORS.REVIEW_RATING);\\n    const reviewCount = parseNumberFromSelector($, SELECTORS.REVIEW_COUNT);\\n\\n    const imageUrls = extractImageUrls($);\\n    const attributes = extractProductAttributes($);\\n\\n    return { title, price, listPrice, reviewRating, reviewCount, imageUrls, attributes };\\n};\\n```\\n\\nNext up is the task of making the scraping part functional. Let\'s implement the crawling part using Crawlee.\\n\\n## Crawling the product pages\\n\\nWe\'ll utilize the features that Crawlee offers to crawl the product pages. As we mentioned at the beginning, it considerably simplifies web scraping with JSON file outputs, automatic scaling, and request queue management.\\n\\nOur next stepping stone is to wrap our scraping logic within Crawlee, thereby implementing the crawling part of our process.\\n\\n```typescript\\nimport { CheerioCrawler, CheerioCrawlingContext, log } from \'crawlee\';\\nimport { extractProductDetails } from \'./scraper.js\';\\n\\n/**\\n * Performs the logic of the crawler. It is called for each URL to crawl.\\n * - Passed to the crawler using the `requestHandler` option.\\n */\\nconst requestHandler = async (context: CheerioCrawlingContext) => {\\n    const { $, request } = context;\\n    const { url } = request;\\n\\n    log.info(`Scraping product page`, { url });\\n    const extractedProduct = extractProductDetails($);\\n\\n    log.info(`Scraped product details for \\"${extractedProduct.title}\\", saving...`, { url });\\n    crawler.pushData(extractedProduct);\\n};\\n\\n/**\\n * The crawler instance. Crawlee provides a few different crawlers, but we\'ll use CheerioCrawler, as it\'s very fast and simple to use.\\n * - Alternatively, we could use a full browser crawler like `PlaywrightCrawler` to imitate a real browser.\\n */\\nconst crawler = new CheerioCrawler({ requestHandler });\\n\\nawait crawler.run([\'https://www.amazon.com/dp/B0BV7XQ9V9\']);\\n```\\n\\nThe code now successfully extracts the product details from the given URLs. We\'ve integrated our scraping function into Crawlee, and it\'s ready to scrape. Here\'s an example of the extracted data:\\n\\n```json\\n{\\n    \\"title\\": \\"ASUS ROG Strix G16 (2023) Gaming Laptop, 16\u201D 16:10 FHD 165Hz, GeForce RTX 4070, Intel Core i9-13980HX, 16GB DDR5, 1TB PCIe SSD, Wi-Fi 6E, Windows 11, G614JI-AS94, Eclipse Gray\\",\\n    \\"price\\": 1799.99,\\n    \\"listPrice\\": 1999.99,\\n    \\"reviewRating\\": 4.3,\\n    \\"reviewCount\\": 372,\\n    \\"imageUrls\\": [\\n        \\"https://m.media-amazon.com/images/I/41EWnXeuMzL._AC_US40_.jpg\\",\\n        \\"https://m.media-amazon.com/images/I/51gAOHZbtUL._AC_US40_.jpg\\",\\n        \\"https://m.media-amazon.com/images/I/51WLw+9ItgL._AC_US40_.jpg\\",\\n        \\"https://m.media-amazon.com/images/I/41D-FN8qjLL._AC_US40_.jpg\\",\\n        \\"https://m.media-amazon.com/images/I/41X+oNPvdkL._AC_US40_.jpg\\",\\n        \\"https://m.media-amazon.com/images/I/41X6TCWz69L._AC_US40_.jpg\\",\\n        \\"https://m.media-amazon.com/images/I/31rphsiD0lL.SS40_BG85,85,85_BR-120_PKdp-play-icon-overlay__.jpg\\"\\n    ],\\n    \\"attributes\\": [\\n        {\\n            \\"label\\": \\"Brand\\",\\n            \\"value\\": \\"ASUS\\"\\n        },\\n        {\\n            \\"label\\": \\"Model Name\\",\\n            \\"value\\": \\"ROG Strix G16\\"\\n        },\\n        {\\n            \\"label\\": \\"Screen Size\\",\\n            \\"value\\": \\"16 Inches\\"\\n        },\\n        {\\n            \\"label\\": \\"Color\\",\\n            \\"value\\": \\"Eclipse Gray\\"\\n        },\\n        {\\n            \\"label\\": \\"Hard Disk Size\\",\\n            \\"value\\": \\"1 TB\\"\\n        },\\n        {\\n            \\"label\\": \\"CPU Model\\",\\n            \\"value\\": \\"Intel Core i9\\"\\n        },\\n        {\\n            \\"label\\": \\"Ram Memory Installed Size\\",\\n            \\"value\\": \\"16 GB\\"\\n        },\\n        {\\n            \\"label\\": \\"Operating System\\",\\n            \\"value\\": \\"Windows 11 Home\\"\\n        },\\n        {\\n            \\"label\\": \\"Special Feature\\",\\n            \\"value\\": \\"Anti Glare Coating\\"\\n        },\\n        {\\n            \\"label\\": \\"Graphics Card Description\\",\\n            \\"value\\": \\"Dedicated\\"\\n        }\\n    ]\\n}\\n```\\n\\n## How to avoid getting blocked when scraping Amazon\\n\\nWith a giant website like Amazon, one is bound to face some issues with blocking. Let\'s discuss how to handle them.\\n\\nAmazon frequently presents annoying CAPTCHAs or warning screens that may detect or block your scraper. We can counter this inconvenience by implementing a mechanism to detect and handle these blocks. As soon as we stumble upon one, we retry the request.\\n\\n```typescript\\nimport { CheerioAPI } from \'cheerio\';\\n\\nconst CAPTCHA_SELECTOR = \'[action=\\"/errors/validateCaptcha\\"]\';\\n\\n/**\\n * Handles the captcha blocking. Throws an error if a captcha is displayed.\\n * - Crawlee automatically retries any requests that throw an error.\\n * - Status code blocking (e.g. Amazon\'s `503`) is handled automatically by Crawlee.\\n */\\nexport const handleCaptchaBlocking = ($: CheerioAPI) => {\\n    const isCaptchaDisplayed = $(CAPTCHA_SELECTOR).length > 0;\\n    if (isCaptchaDisplayed) throw new Error(\'Captcha is displayed! Retrying...\');\\n};\\n```\\n\\nMake a small tweak in the request handler to use `handleCaptchaBlocking`:\\n\\n```typescript\\nimport { handleCaptchaBlocking } from \'./blocking-detection.js\';\\n\\nconst requestHandler = async (context: CheerioCrawlingContext) => {\\n    const { request, $ } = context;\\n    const { url } = request;\\n\\n    handleCaptchaBlocking($); // Alternatively, we can put this into the crawler\'s `postNavigationHooks`\\n\\n    log.info(`Scraping product page`, { url });\\n    ...\\n};\\n```\\n\\nWhile Crawlee\'s browser-like user-agent headers prevent blocking to a certain extent, this is only partially effective for a site as vast as Amazon.\\n\\n### Using proxies\\n\\nThe use of proxies marks another significant tactic in evading blocking. You\'ll be pleased to know that Crawlee excels in this domain, supporting both [custom proxies](https://crawlee.dev/docs/guides/proxy-management) and [Apify proxies](https://apify.com/proxy).\\n\\nHere\'s an example of how to use Apify\'s [residential proxies](https://docs.apify.com/platform/proxy/residential-proxy), which are highly effective in preventing blocking:\\n\\n```typescript\\nimport { ProxyConfiguration } from \'apify\';\\n\\nconst proxyConfiguration = new ProxyConfiguration({\\n    groups: [\'RESIDENTIAL\'],\\n    countryCode: \'US\', // Optionally, you can specify the proxy country code.\\n    // This is useful for sites like Amazon, which display different content based on the user\'s location.\\n});\\n\\nconst crawler = new CheerioCrawler({ requestHandler, proxyConfiguration });\\n\\n...\\n```\\n\\n### Using headless browsers to scrape Amazon\\n\\nFor more advanced scraping, you can use a headless browser like [Playwright](https://crawlee.dev/docs/examples/playwright-crawler) to scrape Amazon. This method is more effective in preventing blocking and can handle websites with complex JavaScript interactions.\\n\\nTo use Playwright with Crawlee, we can replace the `CheerioCrawler` with `PlaywrightCrawler`:\\n\\n```typescript\\nimport { PlaywrightCrawler } from \'crawlee\';\\n\\nconst crawler = new PlaywrightCrawler({ requestHandler, proxyConfiguration });\\n\\n...\\n```\\n\\nAnd update our Cheerio-dependent code to work within Playwright:\\n\\n```typescript\\nimport { PlaywrightCrawlingContext } from \'crawlee\';\\n\\nconst requestHandler = async (context: PlaywrightCrawlingContext) => {\\n    const { request, parseWithCheerio } = context;\\n    const { url } = request;\\n\\n    const $ = await parseWithCheerio(); // Get the Cheerio object for the page.\\n\\n    ...\\n};\\n```\\n\\n## Conclusion and next steps\\n\\nYou\'ve now journeyed through the basic and advanced terrains of web scraping Amazon product pages using the capabilities of TypeScript, Cheerio, and Crawlee. It can seem like a lot to digest but don\'t worry! With more practice, each step will become more familiar and intuitive - until you become a web scraping ninja. So go ahead and start experimenting. If you want to learn more, check out our detailed tutorial on building a [HackerNews scraper using Crawlee](https://blog.apify.com/crawlee-web-scraping-tutorial/). For more extensive web scraping abilities, check out pre-built scrapers from Apify, like [Amazon Web Scraper](https://apify.com/junglee/amazon-crawler)!"},{"id":"crawlee-blog-launch","metadata":{"permalink":"/blog/crawlee-blog-launch","source":"@site/blog/2024/02-22-launching-crawlee-blog/index.md","title":"Launching Crawlee Blog","description":"Your Node.js resource hub for web scraping and automation.","date":"2024-02-22T00:00:00.000Z","tags":[],"readingTime":2.42,"hasTruncateMarker":true,"authors":[{"name":"Saurav Jain","title":"Developer Community Manager","url":"https://github.com/souravjain540","socials":{"x":"https://x.com/sauain","github":"https://github.com/souravjain540"},"imageURL":"https://avatars.githubusercontent.com/u/53312820?v=4","key":"SauravJ","page":null}],"frontMatter":{"slug":"crawlee-blog-launch","title":"Launching Crawlee Blog","description":"Your Node.js resource hub for web scraping and automation.","image":"https://raw.githubusercontent.com/souravjain540/crawlee-first-blog/main/og-image.webp","authors":["SauravJ"]},"unlisted":false,"prevItem":{"title":"How to scrape Amazon products","permalink":"/blog/how-to-scrape-amazon"}},"content":"Hey, crawling masters!\\n\\nI\u2019m Saurav, Developer Community Manager at Apify, and I\u2019m thrilled to announce that we\u2019re launching the Crawlee blog today \uD83C\uDF89\\n\\nWe launched Crawlee, the successor to our Apify SDK, in [August 2022](https://blog.apify.com/announcing-crawlee-the-web-scraping-and-browser-automation-library/) to make the best web scraping and automation library for Node.js developers who like to write code in JavaScript or TypeScript.\\n\\nSince then, our dev community has grown exponentially. I\u2019m proud to tell you that we have **over 11,500 Stars on GitHub**, over **6,000 community members on our Discord**, and over **125,000 downloads monthly on npm**. We\u2019re now the most popular web scraping and automation library for Node.js developers \uD83D\uDC4F\\n\\n\x3c!-- truncate --\x3e\\n\\n## Changes in Crawlee since the launch\\n\\nCrawlee has progressively evolved with the introduction of key features to enhance web scraping and automation:\\n\\n- [v3.1](https://github.com/apify/crawlee/releases/tag/v3.1.0) added an [error tracker](https://crawlee.dev/api/core/class/ErrorTracker) for analyzing and summarizing failed requests.\\n- The [v3.3](https://github.com/apify/crawlee/releases/tag/v3.3.0) update brought an `exclude` option to the `enqueueLinks` helper and integrated status messages. This improved usability on the Apify platform with automatic summary updates in the console UI.\\n- [v3.4](https://github.com/apify/crawlee/releases/tag/v3.4.0) introduced the [`linkedom` crawler](https://crawlee.dev/api/linkedom-crawler), offering a new parsing option.\\n- The [v3.5](https://github.com/apify/crawlee/releases/tag/v3.5.0) update optimized link enqueuing for efficiency.\\n- [v3.6](https://github.com/apify/crawlee/releases/tag/v3.6.0) launched experimental support for a [new request queue API](https://crawlee.dev/docs/experiments/experiments-request-locking), enabling parallel execution and improved scalability for multiple scrapers working concurrently.\\n\\nAll of this marked significant strides in making web scraping more efficient and robust.\\n\\n## Future of Crawlee!\\n\\nThe Crawlee team is actively developing an adaptive crawling feature to revolutionize how Crawlee interacts with and navigates through websites.\\n\\nWe just launched [v3.8](https://github.com/apify/crawlee/releases/tag/v3.8.0) with experimental support for the new [adaptive crawler type](https://crawlee.dev/api/playwright-crawler/class/AdaptivePlaywrightCrawler).\\n\\n## Support us on GitHub.\\n\\nBefore I tell you about our upcoming plans for Crawlee Blog, I recommend you check out Crawlee if you haven\u2019t already.\\n\\nWe are open-source. You can see our [source code here](https://github.com/apify/crawlee/). If you like Crawlee, then please don\u2019t forget to give us a :star: on GitHub.\\n\\n![Crawlee_presentation_final](https://github.com/souravjain540/crawlee-first-blog/assets/53312820/051ec8a3-86a7-4109-8fb3-135e399cbe93)\\n\\n## Crawlee Blog and upcoming plans!\\n\\nThe first step to achieving this goal is to reach out to the broader developer community through our content.\\n\\nThe Crawlee blog aims to be the best informational hub for Node.js developers interested in web scraping and automation.\\n\\n**What to expect:**\\n\\n- How-to-tutorials on making web crawlers, scrapers, and automation applications using Crawlee.\\n- Thought leadership content on web crawling.\\n- Crawlee feature updates and changes.\\n- Community content collaboration.\\n\\nWe\u2019ll be posting content monthly for our dev community, so stay tuned!\\n\\nIf you have ideas on specific content topics and want to give us input, please [join our Discord community](https://apify.com/discord) and tag me with your ideas.\\n\\nAlso, we encourage collaboration with the community, so if you have some interesting pieces of content related to Crawlee, let us know in Discord, and we\u2019ll feature them on our blog. \uD83D\uDE00\\n\\nIn the meantime, you might want to check out this article on [Crawlee data storage types](https://blog.apify.com/crawlee-data-storage-types/) on the Apify Blog."}]}}')}}]);