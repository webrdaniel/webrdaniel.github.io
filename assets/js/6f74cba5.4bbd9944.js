"use strict";(self.webpackChunk=self.webpackChunk||[]).push([["67495"],{38514:function(e,t,a){a.r(t),a.d(t,{default:()=>w,frontMatter:()=>c,metadata:()=>r,assets:()=>h,toc:()=>p,contentTitle:()=>d});var r=JSON.parse('{"id":"examples/basic-crawler","title":"Basic crawler","description":"This is the most bare-bones example of using Crawlee, which demonstrates some of its building blocks such as the BasicCrawler. You probably don\'t need to go this deep though, and it would be better to start with one of the full-featured crawlers","source":"@site/versioned_docs/version-3.11/examples/basic_crawler.mdx","sourceDirName":"examples","slug":"/examples/basic-crawler","permalink":"/docs/3.11/examples/basic-crawler","draft":false,"unlisted":false,"editUrl":"https://github.com/apify/crawlee/edit/master/website/versioned_docs/version-3.11/examples/basic_crawler.mdx","tags":[],"version":"3.11","lastUpdatedBy":"Jind\u0159ich B\xe4r","lastUpdatedAt":1720711256000,"frontMatter":{"id":"basic-crawler","title":"Basic crawler"},"sidebar":"docs","previous":{"title":"Add data to dataset","permalink":"/docs/3.11/examples/add-data-to-dataset"},"next":{"title":"Cheerio crawler","permalink":"/docs/3.11/examples/cheerio-crawler"}}'),s=a("85893"),n=a("50065"),i=a("60643"),l=a("47927");let o={code:"import { BasicCrawler } from 'crawlee';\n\n// Create a BasicCrawler - the simplest crawler that enables\n// users to implement the crawling logic themselves.\nconst crawler = new BasicCrawler({\n    // This function will be called for each URL to crawl.\n    async requestHandler({ pushData, request, sendRequest, log }) {\n        const { url } = request;\n        log.info(`Processing ${url}...`);\n\n        // Fetch the page HTML via the crawlee sendRequest utility method\n        // By default, the method will use the current request that is being handled, so you don't have to\n        // provide it yourself. You can also provide a custom request if you want.\n        const { body } = await sendRequest();\n\n        // Store the HTML and URL to the default dataset.\n        await pushData({\n            url,\n            html: body,\n        });\n    },\n});\n\n// The initial list of URLs to crawl. Here we use just a few hard-coded URLs.\nawait crawler.addRequests([\n    'https://www.google.com',\n    'https://www.example.com',\n    'https://www.bing.com',\n    'https://www.wikipedia.com',\n]);\n\n// Run the crawler and wait for it to finish.\nawait crawler.run();\n\nconsole.log('Crawler finished.');\n",hash:"invalid-token"},c={id:"basic-crawler",title:"Basic crawler"},d=void 0,h={},p=[];function u(e){let t={a:"a",code:"code",p:"p",...(0,n.a)(),...e.components};return(0,s.jsxs)(s.Fragment,{children:[(0,s.jsxs)(t.p,{children:["This is the most bare-bones example of using Crawlee, which demonstrates some of its building blocks such as the ",(0,s.jsx)(l.Z,{to:"basic-crawler/class/BasicCrawler",children:(0,s.jsx)(t.code,{children:"BasicCrawler"})}),". You probably don't need to go this deep though, and it would be better to start with one of the full-featured crawlers\nlike ",(0,s.jsx)(l.Z,{to:"cheerio-crawler/class/CheerioCrawler",children:(0,s.jsx)(t.code,{children:"CheerioCrawler"})})," or ",(0,s.jsx)(l.Z,{to:"playwright-crawler/class/PlaywrightCrawler",children:(0,s.jsx)(t.code,{children:"PlaywrightCrawler"})}),"."]}),"\n",(0,s.jsxs)(t.p,{children:["The script simply downloads several web pages with plain HTTP requests using the ",(0,s.jsx)(l.Z,{to:"basic-crawler/interface/BasicCrawlingContext#sendRequest",children:(0,s.jsx)(t.code,{children:"sendRequest"})})," utility function (which uses the ",(0,s.jsx)(t.a,{href:"https://github.com/apify/got-scraping",target:"_blank",rel:"noopener",children:(0,s.jsx)(t.code,{children:"got-scraping"})}),"\nnpm module internally) and stores their raw HTML and URL in the default dataset. In local configuration, the data will be stored as JSON files in\n",(0,s.jsx)(t.code,{children:"./storage/datasets/default"}),"."]}),"\n",(0,s.jsx)(i.Z,{className:"language-js",type:"cheerio",children:o})]})}function w(e={}){let{wrapper:t}={...(0,n.a)(),...e.components};return t?(0,s.jsx)(t,{...e,children:(0,s.jsx)(u,{...e})}):u(e)}},47927:function(e,t,a){a.d(t,{Z:function(){return d}});var r=a(85893);a(67294);var s=a(53367),n=a(89873),i=a(87262);let[l,o]=a(99074).version.split("."),c=[l,o].join("."),d=e=>{let{to:t,children:a}=e,l=(0,n.E)(),{siteConfig:o}=(0,i.default)();return o.presets[0][1].docs.disableVersioning||l.version===c?(0,r.jsx)(s.default,{to:`/api/${t}`,children:a}):(0,r.jsx)(s.default,{to:`/api/${"current"===l.version?"next":l.version}/${t}`,children:a})}},60643:function(e,t,a){a.d(t,{Z:()=>c});var r=a("85893");a("67294");var s=a("67026"),n=a("96199"),i=a("53367");let l={button:"button_YBBj",container:"container_TGAW"},o={playwright:"6i5QsHBMtm3hKph70",puppeteer:"7tWSD8hrYzuc9Lte7",cheerio:"kk67IcZkKSSBTslXI"},c=e=>{let{children:t,actor:a,hash:c,type:d,...h}=e;if(c=c??t.hash,!t.code)throw Error(`RunnableCodeBlock requires "code" and "hash" props
Make sure you are importing the code block contents with the roa-loader.`);if(!c)return(0,r.jsx)(n.default,{...h,children:t.code});let p=`https://console.apify.com/actors/${a??o[d??"playwright"]}?runConfig=${c}&asrc=run_on_apify`;return(0,r.jsxs)("div",{className:(0,s.Z)(l.container,"runnable-code-block"),children:[(0,r.jsxs)(i.default,{href:p,className:l.button,rel:"follow",children:["Run on",(0,r.jsxs)("svg",{width:"91",height:"25",viewBox:"0 0 91 25",fill:"none",xmlns:"http://www.w3.org/2000/svg",className:"apify-logo-light alignMiddle_src-theme-Footer-index-module",children:[(0,r.jsx)("path",{d:"M3.135 2.85A3.409 3.409 0 0 0 .227 6.699l2.016 14.398 8.483-19.304-7.59 1.059Z",fill:"#97D700"}),(0,r.jsx)("path",{d:"M23.604 14.847 22.811 3.78a3.414 3.414 0 0 0-3.64-3.154c-.077 0-.153.014-.228.025l-3.274.452 7.192 16.124c.54-.67.805-1.52.743-2.379Z",fill:"#71C5E8"}),(0,r.jsx)("path",{d:"M5.336 24.595c.58.066 1.169-.02 1.706-.248l12.35-5.211L13.514 5.97 5.336 24.595Z",fill:"#FF9013"}),(0,r.jsx)("path",{d:"M33.83 5.304h3.903l5.448 14.623h-3.494l-1.022-2.994h-5.877l-1.025 2.994h-3.384L33.83 5.304Zm-.177 9.032h4.14l-2-5.994h-.086l-2.054 5.994ZM58.842 5.304h3.302v14.623h-3.302V5.304ZM64.634 5.304h10.71v2.7h-7.4v4.101h5.962v2.632h-5.963v5.186h-3.309V5.303ZM82.116 14.38l-5.498-9.076h3.748l3.428 6.016h.085l3.599-6.016H91l-5.56 9.054v5.569h-3.324v-5.548ZM51.75 5.304h-7.292v14.623h3.3v-4.634h3.993a4.995 4.995 0 1 0 0-9.99Zm-.364 7.417h-3.628V7.875h3.627a2.423 2.423 0 0 1 0 4.846Z",className:"apify-logo",fill:"#000"})]})]}),(0,r.jsx)(n.default,{...h,className:(0,s.Z)(l.codeBlock,"code-block",null!=h.title?"has-title":"no-title"),children:t.code})]})}},50065:function(e,t,a){a.d(t,{Z:function(){return l},a:function(){return i}});var r=a(67294);let s={},n=r.createContext(s);function i(e){let t=r.useContext(n);return r.useMemo(function(){return"function"==typeof e?e(t):{...t,...e}},[t,e])}function l(e){let t;return t=e.disableParentContext?"function"==typeof e.components?e.components(s):e.components||s:i(e.components),r.createElement(n.Provider,{value:t},e.children)}},99074:function(e){e.exports=JSON.parse('{"name":"crawlee","version":"3.13.0","description":"The scalable web crawling and scraping library for JavaScript/Node.js. Enables development of data extraction and web automation jobs (not only) with headless Chrome and Puppeteer.","engines":{"node":">=16.0.0"},"bin":"./src/cli.ts","main":"./dist/index.js","module":"./dist/index.mjs","types":"./dist/index.d.ts","exports":{".":{"import":"./dist/index.mjs","require":"./dist/index.js","types":"./dist/index.d.ts"},"./package.json":"./package.json"},"keywords":["apify","headless","chrome","puppeteer","crawler","scraper"],"author":{"name":"Apify","email":"support@apify.com","url":"https://apify.com"},"contributors":["Jan Curn <jan@apify.com>","Marek Trunkat <marek@apify.com>","Ondra Urban <ondra@apify.com>"],"license":"Apache-2.0","repository":{"type":"git","url":"git+https://github.com/apify/crawlee"},"bugs":{"url":"https://github.com/apify/crawlee/issues"},"homepage":"https://crawlee.dev","scripts":{"build":"yarn clean && yarn compile && yarn copy","clean":"rimraf ./dist","compile":"tsc -p tsconfig.build.json && gen-esm-wrapper ./dist/index.js ./dist/index.mjs","copy":"tsx ../../scripts/copy.ts"},"publishConfig":{"access":"public"},"dependencies":{"@crawlee/basic":"3.13.0","@crawlee/browser":"3.13.0","@crawlee/browser-pool":"3.13.0","@crawlee/cheerio":"3.13.0","@crawlee/cli":"3.13.0","@crawlee/core":"3.13.0","@crawlee/http":"3.13.0","@crawlee/jsdom":"3.13.0","@crawlee/linkedom":"3.13.0","@crawlee/playwright":"3.13.0","@crawlee/puppeteer":"3.13.0","@crawlee/utils":"3.13.0","import-local":"^3.1.0","tslib":"^2.4.0"},"peerDependencies":{"playwright":"*","puppeteer":"*"},"peerDependenciesMeta":{"playwright":{"optional":true},"puppeteer":{"optional":true}}}')}}]);