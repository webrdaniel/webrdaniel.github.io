"use strict";(self.webpackChunk=self.webpackChunk||[]).push([["97980"],{11190:function(e,r,n){n.r(r),n.d(r,{default:()=>h,frontMatter:()=>l,metadata:()=>t,assets:()=>c,toc:()=>p,contentTitle:()=>u});var t=JSON.parse('{"id":"guides/running-in-web-server/running-in-web-server-guide","title":"Running in web server Guide","description":"Run Crawlee in web server using a request/response approach","source":"@site/../docs/guides/running-in-web-server/running-in-web-server.mdx","sourceDirName":"guides/running-in-web-server","slug":"/guides/running-in-web-server/","permalink":"/docs/next/guides/running-in-web-server/","draft":false,"unlisted":false,"editUrl":"https://github.com/apify/crawlee/edit/master/website/../docs/guides/running-in-web-server/running-in-web-server.mdx","tags":[],"version":"current","lastUpdatedBy":"Luk\xe1\u0161 K\u0159ivka","lastUpdatedAt":1719227925000,"frontMatter":{"id":"running-in-web-server-guide","title":"Running in web server Guide","sidebar_label":"Running in web server","description":"Run Crawlee in web server using a request/response approach"}}'),s=n("85893"),i=n("50065"),a=n("96199"),o=n("47927");let l={id:"running-in-web-server-guide",title:"Running in web server Guide",sidebar_label:"Running in web server",description:"Run Crawlee in web server using a request/response approach"},u=void 0,c={},p=[{value:"Set up a web server",id:"set-up-a-web-server",level:2},{value:"Create the Crawler",id:"create-the-crawler",level:2},{value:"Glue it together",id:"glue-it-together",level:2}];function d(e){let r={a:"a",code:"code",h2:"h2",p:"p",pre:"pre",...(0,i.a)(),...e.components};return(0,s.jsxs)(s.Fragment,{children:[(0,s.jsxs)(r.p,{children:["Most of the time, Crawlee jobs are run as batch jobs. You have a list of URLs you want to scrape every week or you might want to scrape a whole website once per day. After the scrape, you send the data to your warehouse for analytics. Batch jobs are efficient because they can use ",(0,s.jsx)(r.a,{href:"https://crawlee.dev/docs/guides/scaling-crawlers",children:"Crawlee's built-in autoscaling"})," to fully utilize the resources you have available. But sometimes you have a use-case where you need to return scrape data as soon as possible. There might be a user waiting on the other end so every millisecond counts. This is where running Crawlee in a web server comes in."]}),"\n",(0,s.jsxs)(r.p,{children:["We will build a simple HTTP server that receives a page URL and returns the page title in the response. We will base this guide on the approach used in ",(0,s.jsx)(r.a,{href:"https://github.com/apify/super-scraper",target:"_blank",rel:"noopener",children:"Apify's Super Scraper API repository"})," which maps incoming HTTP requests to Crawlee ",(0,s.jsx)(o.Z,{to:"core/class/Request",children:"Request"}),"."]}),"\n",(0,s.jsx)(r.h2,{id:"set-up-a-web-server",children:"Set up a web server"}),"\n",(0,s.jsxs)(r.p,{children:["There are many popular web server frameworks for Node.js, such as Express, Koa, Fastify, and Hapi but in this guide, we will use the built-in ",(0,s.jsx)(r.code,{children:"http"})," Node.js module to keep things simple."]}),"\n",(0,s.jsx)(r.p,{children:"This will be our core server setup:"}),"\n",(0,s.jsx)(r.pre,{children:(0,s.jsx)(r.code,{className:"language-javascript",children:"import { createServer } from 'http';\nimport { log } from 'crawlee';\n\nconst server = createServer(async (req, res) => {\n    log.info(`Request received: ${req.method} ${req.url}`);\n\n    res.writeHead(200, { 'Content-Type': 'text/plain' });\n    // We will return the page title here later instead\n    res.end('Hello World\\n');\n});\n\nserver.listen(3000, () => {\n    log.info('Server is listening for user requests');\n});\n"})}),"\n",(0,s.jsx)(r.h2,{id:"create-the-crawler",children:"Create the Crawler"}),"\n",(0,s.jsxs)(r.p,{children:["We will create a standard ",(0,s.jsx)(o.Z,{to:"cheerio-crawler/class/CheerioCrawler",children:"CheerioCrawler"})," and use the ",(0,s.jsx)(o.Z,{to:"cheerio-crawler/interface/CheerioCrawlerOptions#keepAlive",children:(0,s.jsx)(r.code,{children:"keepAlive: true"})})," option to keep the crawler running even if there are no requests currently in the ",(0,s.jsx)(o.Z,{to:"core/class/RequestQueue",children:"Request Queue"}),". This way it will always be waiting for new requests to come in."]}),"\n",(0,s.jsx)(r.pre,{children:(0,s.jsx)(r.code,{className:"language-javascript",children:"import { CheerioCrawler, log } from 'crawlee';\n\nconst crawler = new CheerioCrawler({\n    keepAlive: true,\n    requestHandler: async ({ request, $ }) => {\n        const title = $('title').text();\n        // We will send the response here later\n        log.info(`Page title: ${title} on ${request.url}`);\n    },\n});\n"})}),"\n",(0,s.jsx)(r.h2,{id:"glue-it-together",children:"Glue it together"}),"\n",(0,s.jsx)(r.p,{children:"Now we need to glue the server and the crawler together using the mapping of Crawlee Requests to HTTP responses discussed above. The whole program is actually quite simple. For production-grade service, you will need to improve error handling, logging, and monitoring but this is a good starting point."}),"\n",(0,s.jsx)(a.default,{language:"js",title:"src/web-server.mjs",children:"import { randomUUID } from 'node:crypto';\nimport { CheerioCrawler, log } from 'crawlee';\nimport { createServer } from 'http';\n\n// We will bind an HTTP response that we want to send to the Request.uniqueKey\nconst requestsToResponses = new Map();\n\nconst crawler = new CheerioCrawler({\n    keepAlive: true,\n    requestHandler: async ({ request, $ }) => {\n        const title = $('title').text();\n        log.info(`Page title: ${title} on ${request.url}, sending response`);\n\n        // We will pick the response from the map and send it to the user\n        // We know the response is there with this uniqueKey\n        const httpResponse = requestsToResponses.get(request.uniqueKey);\n        httpResponse.writeHead(200, { 'Content-Type': 'application/json' });\n        httpResponse.end(JSON.stringify({ title }));\n        // We can delete the response from the map now to free up memory\n        requestsToResponses.delete(request.uniqueKey);\n    },\n});\n\nconst server = createServer(async (req, res) => {\n    // We parse the requested URL from the query parameters, e.g. localhost:3000/?url=https://example.com\n    const urlObj = new URL(req.url, 'http://localhost:3000');\n    const requestedUrl = urlObj.searchParams.get('url');\n\n    log.info(`HTTP request received for ${requestedUrl}, adding to the queue`);\n    if (!requestedUrl) {\n        log.error('No URL provided as query parameter, returning 400');\n        res.writeHead(400, { 'Content-Type': 'application/json' });\n        res.end(JSON.stringify({ error: 'No URL provided as query parameter' }));\n        return;\n    }\n\n    // We will add it first to the map and then enqueue it to the crawler that immediately processes it\n    // uniqueKey must be random so we process the same URL again\n    const crawleeRequest = { url: requestedUrl, uniqueKey: randomUUID() };\n    requestsToResponses.set(crawleeRequest.uniqueKey, res);\n    await crawler.addRequests([crawleeRequest]);\n});\n\n// Now we start the server, the crawler and wait for incoming connections\nserver.listen(3000, () => {\n    log.info('Server is listening for user requests');\n});\n\nawait crawler.run();\n"})]})}function h(e={}){let{wrapper:r}={...(0,i.a)(),...e.components};return r?(0,s.jsx)(r,{...e,children:(0,s.jsx)(d,{...e})}):d(e)}},47927:function(e,r,n){n.d(r,{Z:function(){return c}});var t=n(85893);n(67294);var s=n(53367),i=n(89873),a=n(87262);let[o,l]=n(99074).version.split("."),u=[o,l].join("."),c=e=>{let{to:r,children:n}=e,o=(0,i.E)(),{siteConfig:l}=(0,a.default)();return l.presets[0][1].docs.disableVersioning||o.version===u?(0,t.jsx)(s.default,{to:`/api/${r}`,children:n}):(0,t.jsx)(s.default,{to:`/api/${"current"===o.version?"next":o.version}/${r}`,children:n})}},50065:function(e,r,n){n.d(r,{Z:function(){return o},a:function(){return a}});var t=n(67294);let s={},i=t.createContext(s);function a(e){let r=t.useContext(i);return t.useMemo(function(){return"function"==typeof e?e(r):{...r,...e}},[r,e])}function o(e){let r;return r=e.disableParentContext?"function"==typeof e.components?e.components(s):e.components||s:a(e.components),t.createElement(i.Provider,{value:r},e.children)}},99074:function(e){e.exports=JSON.parse('{"name":"crawlee","version":"3.13.0","description":"The scalable web crawling and scraping library for JavaScript/Node.js. Enables development of data extraction and web automation jobs (not only) with headless Chrome and Puppeteer.","engines":{"node":">=16.0.0"},"bin":"./src/cli.ts","main":"./dist/index.js","module":"./dist/index.mjs","types":"./dist/index.d.ts","exports":{".":{"import":"./dist/index.mjs","require":"./dist/index.js","types":"./dist/index.d.ts"},"./package.json":"./package.json"},"keywords":["apify","headless","chrome","puppeteer","crawler","scraper"],"author":{"name":"Apify","email":"support@apify.com","url":"https://apify.com"},"contributors":["Jan Curn <jan@apify.com>","Marek Trunkat <marek@apify.com>","Ondra Urban <ondra@apify.com>"],"license":"Apache-2.0","repository":{"type":"git","url":"git+https://github.com/apify/crawlee"},"bugs":{"url":"https://github.com/apify/crawlee/issues"},"homepage":"https://crawlee.dev","scripts":{"build":"yarn clean && yarn compile && yarn copy","clean":"rimraf ./dist","compile":"tsc -p tsconfig.build.json && gen-esm-wrapper ./dist/index.js ./dist/index.mjs","copy":"tsx ../../scripts/copy.ts"},"publishConfig":{"access":"public"},"dependencies":{"@crawlee/basic":"3.13.0","@crawlee/browser":"3.13.0","@crawlee/browser-pool":"3.13.0","@crawlee/cheerio":"3.13.0","@crawlee/cli":"3.13.0","@crawlee/core":"3.13.0","@crawlee/http":"3.13.0","@crawlee/jsdom":"3.13.0","@crawlee/linkedom":"3.13.0","@crawlee/playwright":"3.13.0","@crawlee/puppeteer":"3.13.0","@crawlee/utils":"3.13.0","import-local":"^3.1.0","tslib":"^2.4.0"},"peerDependencies":{"playwright":"*","puppeteer":"*"},"peerDependenciesMeta":{"playwright":{"optional":true},"puppeteer":{"optional":true}}}')}}]);