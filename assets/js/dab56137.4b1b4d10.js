"use strict";(self.webpackChunk=self.webpackChunk||[]).push([["59490"],{76199:function(e,n,t){t.r(n),t.d(n,{assets:function(){return l},contentTitle:function(){return i},default:function(){return u},frontMatter:function(){return o},metadata:function(){return r},toc:function(){return c}});var r=t(98412),a=t(85893),s=t(50065);let o={slug:"crawlee-for-python-v05",title:"Crawlee for Python v0.5",description:"Announcing the Crawlee for Python v0.5 release.",authors:["VladaD"]},i=void 0,l={authorsImageUrls:[void 0]},c=[{value:"Getting started",id:"getting-started",level:2},{value:"New package structure",id:"new-package-structure",level:2},{value:"Crawlers",id:"crawlers",level:3},{value:"Storage clients",id:"storage-clients",level:3},{value:"Continued parity with Crawlee JS",id:"continued-parity-with-crawlee-js",level:2},{value:"HTML to text context helper",id:"html-to-text-context-helper",level:3},{value:"Use state",id:"use-state",level:3},{value:"Brand new features",id:"brand-new-features",level:2},{value:"Crawler&#39;s stop method",id:"crawlers-stop-method",level:3},{value:"Request loaders",id:"request-loaders",level:3},{value:"Service locator",id:"service-locator",level:3},{value:"Conclusion",id:"conclusion",level:2}];function h(e){let n={a:"a",code:"code",h2:"h2",h3:"h3",img:"img",p:"p",pre:"pre",...(0,s.a)(),...e.components};return(0,a.jsxs)(a.Fragment,{children:[(0,a.jsxs)(n.p,{children:["Crawlee for Python v0.5 is now available! This is our biggest release to date, bringing new ported functionality from the ",(0,a.jsx)(n.a,{href:"https://github.com/apify/crawlee",children:"Crawlee for JavaScript"}),", brand-new features that are exclusive to the Python library (for now), a new consolidated package structure, and a bunch of bug fixes and further improvements."]}),"\n",(0,a.jsx)(n.h2,{id:"getting-started",children:"Getting started"}),"\n",(0,a.jsxs)(n.p,{children:["You can upgrade to the latest version straight from ",(0,a.jsx)(n.a,{href:"https://pypi.org/project/crawlee/",children:"PyPI"}),":"]}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-shell",children:"pip install --upgrade crawlee\n"})}),"\n",(0,a.jsxs)(n.p,{children:["Check out the full changelog on our ",(0,a.jsx)(n.a,{href:"https://www.crawlee.dev/python/docs/changelog#050-2025-01-02",children:"website"})," to see all the details. If you are updating from an older version, make sure to follow our ",(0,a.jsx)(n.a,{href:"https://www.crawlee.dev/python/docs/upgrading/upgrading-to-v0x#upgrading-to-v05",children:"Upgrading to v0.5"})," guide for a smooth upgrade."]}),"\n",(0,a.jsx)(n.h2,{id:"new-package-structure",children:"New package structure"}),"\n",(0,a.jsx)(n.p,{children:"We have introduced a new consolidated package structure. The goal is to streamline the development experience, help you find the crawlers you are looking for faster, and improve the IDE's code suggestions while importing."}),"\n",(0,a.jsx)(n.h3,{id:"crawlers",children:"Crawlers"}),"\n",(0,a.jsxs)(n.p,{children:["We have grouped all crawler classes (and their corresponding crawling context classes) into a single sub-package called ",(0,a.jsx)(n.code,{children:"crawlers"}),". Here is a quick example of how the imports have changed:"]}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-diff",children:"- from crawlee.beautifulsoup_crawler import BeautifulSoupCrawler, BeautifulSoupCrawlingContext\n+ from crawlee.crawlers import BeautifulSoupCrawler, BeautifulSoupCrawlingContext\n"})}),"\n",(0,a.jsx)(n.p,{children:"Look how you can see all the crawlers that we have, isn't that cool!"}),"\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.img,{alt:"Import from crawlers subpackage.",src:t(69418).Z+"",width:"1892",height:"804"})}),"\n",(0,a.jsx)(n.h3,{id:"storage-clients",children:"Storage clients"}),"\n",(0,a.jsxs)(n.p,{children:["Similarly, we have moved all storage client classes under ",(0,a.jsx)(n.code,{children:"storage_clients"})," sub-package. For instance:"]}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-diff",children:"- from crawlee.memory_storage_client import MemoryStorageClient\n+ from crawlee.storage_clients import MemoryStorageClient\n"})}),"\n",(0,a.jsx)(n.p,{children:"This consolidation makes it clearer where each class belongs and ensures that your IDE can provide better autocompletion when you are looking for the right crawler or storage client."}),"\n",(0,a.jsx)(n.h2,{id:"continued-parity-with-crawlee-js",children:"Continued parity with Crawlee JS"}),"\n",(0,a.jsxs)(n.p,{children:["We are constantly working toward feature parity with our JavaScript library, ",(0,a.jsx)(n.a,{href:"https://github.com/apify/crawlee",children:"Crawlee JS"}),". With v0.5, we have brought over more functionality:"]}),"\n",(0,a.jsx)(n.h3,{id:"html-to-text-context-helper",children:"HTML to text context helper"}),"\n",(0,a.jsxs)(n.p,{children:["The ",(0,a.jsx)(n.code,{children:"html_to_text"})," crawling context helper simplifies extracting text from an HTML page by automatically removing all tags and returning only the raw text content. It's available in the ",(0,a.jsx)(n.a,{href:"https://www.crawlee.dev/python/api/class/ParselCrawlingContext#html_to_text",children:(0,a.jsx)(n.code,{children:"ParselCrawlingContext"})})," and ",(0,a.jsx)(n.a,{href:"https://www.crawlee.dev/python/api/class/BeautifulSoupCrawlingContext#html_to_text",children:(0,a.jsx)(n.code,{children:"BeautifulSoupCrawlingContext"})}),"."]}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:"import asyncio\n\nfrom crawlee.crawlers import ParselCrawler, ParselCrawlingContext\n\n\nasync def main() -> None:\n    crawler = ParselCrawler()\n\n    @crawler.router.default_handler\n    async def handler(context: ParselCrawlingContext) -> None:\n        context.log.info('Crawling: %s', context.request.url)\n        text = context.html_to_text()\n        # Continue with the processing...\n\n    await crawler.run(['https://crawlee.dev'])\n\n\nif __name__ == '__main__':\n    asyncio.run(main())\n"})}),"\n",(0,a.jsxs)(n.p,{children:["In this example, we use a ",(0,a.jsx)(n.a,{href:"https://www.crawlee.dev/python/api/class/ParselCrawler",children:(0,a.jsx)(n.code,{children:"ParselCrawler"})})," to fetch a webpage, then invoke ",(0,a.jsx)(n.code,{children:"context.html_to_text()"})," to extract clean text for further processing."]}),"\n",(0,a.jsx)(n.h3,{id:"use-state",children:"Use state"}),"\n",(0,a.jsxs)(n.p,{children:["The ",(0,a.jsx)(n.a,{href:"https://www.crawlee.dev/python/api/class/UseStateFunction",children:(0,a.jsx)(n.code,{children:"use_state"})})," crawling context helper makes it simple to create and manage persistent state values within your crawler. It ensures that all state values are automatically persisted. It enables you to maintain data across different crawler runs, restarts, and failures. It acts as a convenient abstraction for interaction with ",(0,a.jsx)(n.a,{href:"https://www.crawlee.dev/python/api/class/KeyValueStore",children:(0,a.jsx)(n.code,{children:"KeyValueStore"})}),"."]}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:"import asyncio\n\nfrom crawlee import Request\nfrom crawlee.configuration import Configuration\nfrom crawlee.crawlers import ParselCrawler, ParselCrawlingContext\n\n\nasync def main() -> None:\n    # Create a crawler with purge_on_start disabled to retain state across runs.\n    crawler = ParselCrawler(\n        configuration=Configuration(purge_on_start=False),\n    )\n\n    @crawler.router.default_handler\n    async def handler(context: ParselCrawlingContext) -> None:\n        context.log.info(f'Crawling {context.request.url}')\n\n        # Retrieve or initialize the state with a default value.\n        state = await context.use_state('state', default_value={'runs': 0})\n\n        # Increment the run count.\n        state['runs'] += 1\n\n    # Create a request with always_enqueue enabled to bypass deduplication and ensure it is processed.\n    request = Request.from_url('https://crawlee.dev/', always_enqueue=True)\n\n    # Run the crawler with the start request.\n    await crawler.run([request])\n\n    # Fetch the persisted state from the key-value store.\n    kvs = await crawler.get_key_value_store()\n    state = await kvs.get_auto_saved_value('state')\n    crawler.log.info(f'Final state after run: {state}')\n\n\nif __name__ == '__main__':\n    asyncio.run(main())\n"})}),"\n",(0,a.jsxs)(n.p,{children:["Please note that the ",(0,a.jsx)(n.code,{children:"use_state"})," is an experimental feature. Its behavior and interface may evolve in future versions."]}),"\n",(0,a.jsx)(n.h2,{id:"brand-new-features",children:"Brand new features"}),"\n",(0,a.jsx)(n.p,{children:"In addition to porting features from JS, we are introducing new, Python-first functionalities that will eventually make their way into Crawlee JS in the coming months."}),"\n",(0,a.jsx)(n.h3,{id:"crawlers-stop-method",children:"Crawler's stop method"}),"\n",(0,a.jsxs)(n.p,{children:["The ",(0,a.jsx)(n.a,{href:"https://www.crawlee.dev/python/api/class/BasicCrawler",children:(0,a.jsx)(n.code,{children:"BasicCrawler"})}),", and by extension, all crawlers that inherit from it, now has a ",(0,a.jsx)(n.a,{href:"https://www.crawlee.dev/python/api/class/BasicCrawler#stop",children:(0,a.jsx)(n.code,{children:"stop"})})," method. This makes it easy to halt the crawling when a specific condition is met, for instance, if you have found the data you were looking for."]}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:"import asyncio\n\nfrom crawlee.crawlers import ParselCrawler, ParselCrawlingContext\n\n\nasync def main() -> None:\n    crawler = ParselCrawler()\n\n    @crawler.router.default_handler\n    async def handler(context: ParselCrawlingContext) -> None:\n        context.log.info('Crawling: %s', context.request.url)\n\n        # Extract and enqueue links from the page.\n        await context.enqueue_links()\n\n        title = context.selector.css('title::text').get()\n\n        # Condition when you want to stop the crawler, e.g. you\n        # have found what you were looking for.\n        if 'Crawlee for Python' in title:\n            context.log.info('Condition met, stopping the crawler.')\n            await crawler.stop()\n\n    await crawler.run(['https://crawlee.dev'])\n\n\nif __name__ == '__main__':\n    asyncio.run(main())\n"})}),"\n",(0,a.jsx)(n.h3,{id:"request-loaders",children:"Request loaders"}),"\n",(0,a.jsxs)(n.p,{children:["There are new classes ",(0,a.jsx)(n.a,{href:"https://www.crawlee.dev/python/api/class/RequestLoader",children:(0,a.jsx)(n.code,{children:"RequestLoader"})}),", ",(0,a.jsx)(n.a,{href:"https://www.crawlee.dev/python/api/class/RequestManager",children:(0,a.jsx)(n.code,{children:"RequestManager"})})," and ",(0,a.jsx)(n.a,{href:"https://www.crawlee.dev/python/api/class/RequestManagerTandem",children:(0,a.jsx)(n.code,{children:"RequestManagerTandem"})})," that manage how Crawlee accesses and stores requests. They allow you to use other component (service) as a source for requests and optionally you can combine it with a ",(0,a.jsx)(n.a,{href:"https://www.crawlee.dev/python/api/class/RequestQueue",children:(0,a.jsx)(n.code,{children:"RequestQueue"})}),". They let you plug in any request source, and combine the external data sources with Crawlee's standard ",(0,a.jsx)(n.code,{children:"RequestQueue"}),"."]}),"\n",(0,a.jsxs)(n.p,{children:["You can learn more about these new features in the ",(0,a.jsx)(n.a,{href:"https://www.crawlee.dev/python/docs/guides/request-loaders",children:"Request loaders guide"}),"."]}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:"import asyncio\n\nfrom crawlee.crawlers import ParselCrawler, ParselCrawlingContext\nfrom crawlee.request_loaders import RequestList, RequestManagerTandem\nfrom crawlee.storages import RequestQueue\n\n\nasync def main() -> None:\n    rl = RequestList(\n        [\n            'https://crawlee.dev',\n            'https://apify.com',\n            # Long list of URLs...\n        ],\n    )\n\n    rq = await RequestQueue.open()\n\n    # Combine them into a single request source.\n    tandem = RequestManagerTandem(rl, rq)\n\n    crawler = ParselCrawler(request_manager=tandem)\n\n    @crawler.router.default_handler\n    async def handler(context: ParselCrawlingContext) -> None:\n        context.log.info(f'Crawling {context.request.url}')\n        # ...\n\n    await crawler.run()\n\n\nif __name__ == '__main__':\n    asyncio.run(main())\n"})}),"\n",(0,a.jsxs)(n.p,{children:["In this example we combine a ",(0,a.jsx)(n.a,{href:"https://www.crawlee.dev/python/api/class/RequestList",children:(0,a.jsx)(n.code,{children:"RequestList"})})," with a ",(0,a.jsx)(n.a,{href:"https://www.crawlee.dev/python/api/class/RequestQueue",children:(0,a.jsx)(n.code,{children:"RequestQueue"})}),". However, instead of the ",(0,a.jsx)(n.code,{children:"RequestList"})," you can use any other class that implements the ",(0,a.jsx)(n.a,{href:"https://www.crawlee.dev/python/api/class/RequestLoader",children:(0,a.jsx)(n.code,{children:"RequestLoader"})})," interface to suit your specific requirements."]}),"\n",(0,a.jsx)(n.h3,{id:"service-locator",children:"Service locator"}),"\n",(0,a.jsxs)(n.p,{children:["The ",(0,a.jsx)(n.a,{href:"https://www.crawlee.dev/python/api/class/ServiceLocator",children:(0,a.jsx)(n.code,{children:"ServiceLocator"})})," is primarily an internal mechanism for managing the services that Crawlee depends on. Specifically, the ",(0,a.jsx)(n.a,{href:"https://www.crawlee.dev/python/api/class/ServiceLocator",children:(0,a.jsx)(n.code,{children:"Configuration"})}),", ",(0,a.jsx)(n.a,{href:"https://www.crawlee.dev/python/api/class/ServiceLocator",children:(0,a.jsx)(n.code,{children:"StorageClient"})}),", and ",(0,a.jsx)(n.a,{href:"https://www.crawlee.dev/python/api/class/ServiceLocator",children:(0,a.jsx)(n.code,{children:"EventManager"})}),". By swapping out these components, you can adapt Crawlee to suit different runtime environments."]}),"\n",(0,a.jsx)(n.p,{children:"You can use the service locator explicitly:"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:"import asyncio\n\nfrom crawlee import service_locator\nfrom crawlee.configuration import Configuration\nfrom crawlee.crawlers import ParselCrawler, ParselCrawlingContext\nfrom crawlee.events import LocalEventManager\nfrom crawlee.storage_clients import MemoryStorageClient\n\n\nasync def main() -> None:\n    service_locator.set_configuration(Configuration())\n    service_locator.set_storage_client(MemoryStorageClient())\n    service_locator.set_event_manager(LocalEventManager())\n\n    crawler = ParselCrawler()\n\n    # ...\n\n\nif __name__ == '__main__':\n    asyncio.run(main())\n"})}),"\n",(0,a.jsx)(n.p,{children:"Or pass the services directly to the crawler instance, and they will be set under the hood:"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:"import asyncio\n\nfrom crawlee.configuration import Configuration\nfrom crawlee.crawlers import ParselCrawler, ParselCrawlingContext\nfrom crawlee.events import LocalEventManager\nfrom crawlee.storage_clients import MemoryStorageClient\n\n\nasync def main() -> None:\n    crawler = ParselCrawler(\n        configuration=Configuration(),\n        storage_client=MemoryStorageClient(),\n        event_manager=LocalEventManager(),\n    )\n\n    # ...\n\n\nif __name__ == '__main__':\n    asyncio.run(main())\n"})}),"\n",(0,a.jsx)(n.h2,{id:"conclusion",children:"Conclusion"}),"\n",(0,a.jsxs)(n.p,{children:["We are excited to share that Crawlee v0.5 is here. If you have any questions or feedback, please open a ",(0,a.jsx)(n.a,{href:"https://github.com/apify/crawlee-python/discussions",children:"GitHub discussion"}),". If you encounter any bugs, or have an idea for a new feature, please open a ",(0,a.jsx)(n.a,{href:"https://github.com/apify/crawlee-python/issues",children:"GitHub issue"}),"."]})]})}function u(e={}){let{wrapper:n}={...(0,s.a)(),...e.components};return n?(0,a.jsx)(n,{...e,children:(0,a.jsx)(h,{...e})}):h(e)}},69418:function(e,n,t){t.d(n,{Z:function(){return r}});let r=t.p+"assets/images/import_crawlers-32dc36ba69192c5d936cbc8c05a9b946.webp"},50065:function(e,n,t){t.d(n,{Z:function(){return i},a:function(){return o}});var r=t(67294);let a={},s=r.createContext(a);function o(e){let n=r.useContext(s);return r.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function i(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(a):e.components||a:o(e.components),r.createElement(s.Provider,{value:n},e.children)}},98412:function(e){e.exports=JSON.parse('{"permalink":"/blog/crawlee-for-python-v05","source":"@site/blog/2025/01-10/index.md","title":"Crawlee for Python v0.5","description":"Announcing the Crawlee for Python v0.5 release.","date":"2025-01-10T00:00:00.000Z","tags":[],"readingTime":5.53,"hasTruncateMarker":true,"authors":[{"name":"Vlada Dusek","title":"Developer of Crawlee for Python","url":"https://github.com/vdusek","socials":{"github":"https://github.com/vdusek"},"imageURL":"https://avatars.githubusercontent.com/u/25082181?v=4","key":"VladaD","page":null}],"frontMatter":{"slug":"crawlee-for-python-v05","title":"Crawlee for Python v0.5","description":"Announcing the Crawlee for Python v0.5 release.","authors":["VladaD"]},"unlisted":false,"prevItem":{"title":"Inside implementing SuperScraper with Crawlee","permalink":"/blog/superscraper-with-crawlee"},"nextItem":{"title":"How to scrape Crunchbase using Python in 2024 (Easy Guide)","permalink":"/blog/scrape-crunchbase-python"}}')}}]);