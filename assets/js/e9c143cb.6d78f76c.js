"use strict";(self.webpackChunk=self.webpackChunk||[]).push([["99467"],{21227:function(e,t,a){a.r(t),a.d(t,{default:()=>m,frontMatter:()=>l,metadata:()=>i,assets:()=>p,toc:()=>d,contentTitle:()=>c});var i=JSON.parse('{"id":"examples/skip-navigation","title":"Skipping navigations for certain requests","description":"While crawling a website, you may encounter certain resources you\'d like to save, but don\'t need the full power of a crawler to do so (like images delivered through a CDN).","source":"@site/versioned_docs/version-3.3/examples/skip-navigation.mdx","sourceDirName":"examples","slug":"/examples/skip-navigation","permalink":"/docs/3.3/examples/skip-navigation","draft":false,"unlisted":false,"editUrl":"https://github.com/apify/crawlee/edit/master/website/versioned_docs/version-3.3/examples/skip-navigation.mdx","tags":[],"version":"3.3","lastUpdatedBy":"Martin Ad\xe1mek","lastUpdatedAt":1678354083000,"frontMatter":{"id":"skip-navigation","title":"Skipping navigations for certain requests"},"sidebar":"docs","previous":{"title":"Puppeteer recursive crawl","permalink":"/docs/3.3/examples/puppeteer-recursive-crawl"},"next":{"title":"Upgrading","permalink":"/docs/3.3/upgrading"}}'),n=a("85893"),r=a("50065"),s=a("96199"),o=a("47927");let l={id:"skip-navigation",title:"Skipping navigations for certain requests"},c=void 0,p={},d=[];function u(e){let t={admonition:"admonition",code:"code",p:"p",...(0,r.a)(),...e.components};return(0,n.jsxs)(n.Fragment,{children:[(0,n.jsx)(t.p,{children:"While crawling a website, you may encounter certain resources you'd like to save, but don't need the full power of a crawler to do so (like images delivered through a CDN)."}),"\n",(0,n.jsxs)(t.p,{children:["By combining the ",(0,n.jsx)(o.Z,{to:"core/class/Request#skipNavigation",children:(0,n.jsx)(t.code,{children:"Request#skipNavigation"})})," option with ",(0,n.jsx)(o.Z,{to:"basic-crawler/interface/BasicCrawlingContext#sendRequest",children:(0,n.jsx)(t.code,{children:"sendRequest"})}),", we can fetch the image from the CDN, and save it to our key-value store without needing to use the full crawler."]}),"\n",(0,n.jsx)(t.admonition,{type:"info",children:(0,n.jsxs)(t.p,{children:["For this example, we are using the ",(0,n.jsx)(o.Z,{to:"playwright-crawler/class/PlaywrightCrawler",children:(0,n.jsx)(t.code,{children:"PlaywrightCrawler"})})," to showcase this, but this is available on all the crawlers we provide."]})}),"\n",(0,n.jsx)(s.default,{className:"language-js",children:"import { PlaywrightCrawler, KeyValueStore } from 'crawlee';\n\n// Create a key value store for all images we find\nconst imageStore = await KeyValueStore.open('images');\n\nconst crawler = new PlaywrightCrawler({\n    async requestHandler({ request, page, sendRequest }) {\n        // The request should have the navigation skipped\n        if (request.skipNavigation) {\n            // Request the image and get its buffer back\n            const imageResponse = await sendRequest({ responseType: 'buffer' });\n\n            // Save the image in the key-value store\n            await imageStore.setValue(`${request.userData.key}.png`, imageResponse.body);\n\n            // Prevent executing the rest of the code as we do not need it\n            return;\n        }\n\n        // Get all the image sources in the current page\n        const images = await page.$$eval('img', (imgs) => imgs.map((img) => img.src));\n\n        // Add all the urls as requests for the crawler, giving each image a key\n        await crawler.addRequests(images.map((url, i) => ({ url, skipNavigation: true, userData: { key: i } })));\n    },\n});\n\nawait crawler.addRequests(['https://crawlee.dev']);\n\n// Run the crawler\nawait crawler.run();\n"})]})}function m(e={}){let{wrapper:t}={...(0,r.a)(),...e.components};return t?(0,n.jsx)(t,{...e,children:(0,n.jsx)(u,{...e})}):u(e)}},47927:function(e,t,a){a.d(t,{Z:function(){return p}});var i=a(85893);a(67294);var n=a(53367),r=a(89873),s=a(87262);let[o,l]=a(99074).version.split("."),c=[o,l].join("."),p=e=>{let{to:t,children:a}=e,o=(0,r.E)(),{siteConfig:l}=(0,s.default)();return l.presets[0][1].docs.disableVersioning||o.version===c?(0,i.jsx)(n.default,{to:`/api/${t}`,children:a}):(0,i.jsx)(n.default,{to:`/api/${"current"===o.version?"next":o.version}/${t}`,children:a})}},50065:function(e,t,a){a.d(t,{Z:function(){return o},a:function(){return s}});var i=a(67294);let n={},r=i.createContext(n);function s(e){let t=i.useContext(r);return i.useMemo(function(){return"function"==typeof e?e(t):{...t,...e}},[t,e])}function o(e){let t;return t=e.disableParentContext?"function"==typeof e.components?e.components(n):e.components||n:s(e.components),i.createElement(r.Provider,{value:t},e.children)}},99074:function(e){e.exports=JSON.parse('{"name":"crawlee","version":"3.13.0","description":"The scalable web crawling and scraping library for JavaScript/Node.js. Enables development of data extraction and web automation jobs (not only) with headless Chrome and Puppeteer.","engines":{"node":">=16.0.0"},"bin":"./src/cli.ts","main":"./dist/index.js","module":"./dist/index.mjs","types":"./dist/index.d.ts","exports":{".":{"import":"./dist/index.mjs","require":"./dist/index.js","types":"./dist/index.d.ts"},"./package.json":"./package.json"},"keywords":["apify","headless","chrome","puppeteer","crawler","scraper"],"author":{"name":"Apify","email":"support@apify.com","url":"https://apify.com"},"contributors":["Jan Curn <jan@apify.com>","Marek Trunkat <marek@apify.com>","Ondra Urban <ondra@apify.com>"],"license":"Apache-2.0","repository":{"type":"git","url":"git+https://github.com/apify/crawlee"},"bugs":{"url":"https://github.com/apify/crawlee/issues"},"homepage":"https://crawlee.dev","scripts":{"build":"yarn clean && yarn compile && yarn copy","clean":"rimraf ./dist","compile":"tsc -p tsconfig.build.json && gen-esm-wrapper ./dist/index.js ./dist/index.mjs","copy":"tsx ../../scripts/copy.ts"},"publishConfig":{"access":"public"},"dependencies":{"@crawlee/basic":"3.13.0","@crawlee/browser":"3.13.0","@crawlee/browser-pool":"3.13.0","@crawlee/cheerio":"3.13.0","@crawlee/cli":"3.13.0","@crawlee/core":"3.13.0","@crawlee/http":"3.13.0","@crawlee/jsdom":"3.13.0","@crawlee/linkedom":"3.13.0","@crawlee/playwright":"3.13.0","@crawlee/puppeteer":"3.13.0","@crawlee/utils":"3.13.0","import-local":"^3.1.0","tslib":"^2.4.0"},"peerDependencies":{"playwright":"*","puppeteer":"*"},"peerDependenciesMeta":{"playwright":{"optional":true},"puppeteer":{"optional":true}}}')}}]);