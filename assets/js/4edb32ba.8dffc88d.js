"use strict";(self.webpackChunk=self.webpackChunk||[]).push([["93770"],{94639:function(e,n,t){t.r(n),t.d(n,{assets:function(){return c},contentTitle:function(){return o},default:function(){return d},frontMatter:function(){return a},metadata:function(){return r},toc:function(){return l}});var r=t(5317),s=t(85893),i=t(50065);let a={slug:"scrape-crunchbase-python",title:"How to scrape Crunchbase using Python in 2024 (Easy Guide)",tags:["community"],description:"Learn how to scrape Crunchbase using Crawlee for Python",image:"./img/scrape_crunchbase.webp",authors:["MaxB"]},o=void 0,c={image:t(6893).Z,authorsImageUrls:[void 0]},l=[{value:"Prerequisites",id:"prerequisites",level:2},{value:"Project setup",id:"project-setup",level:3},{value:"Choosing the data source",id:"choosing-the-data-source",level:3},{value:"Scraping Crunchbase using sitemap and Crawlee for Python",id:"scraping-crunchbase-using-sitemap-and-crawlee-for-python",level:2},{value:"1. Configuring the crawler for scraping",id:"1-configuring-the-crawler-for-scraping",level:3},{value:"2. Implementing sitemap navigation",id:"2-implementing-sitemap-navigation",level:3},{value:"3. Extracting and saving data",id:"3-extracting-and-saving-data",level:3},{value:"4. Running the project",id:"4-running-the-project",level:3},{value:"5. Finally, characteristics of using the sitemap crawler",id:"5-finally-characteristics-of-using-the-sitemap-crawler",level:3},{value:"Using search for scraping Crunchbase",id:"using-search-for-scraping-crunchbase",level:2},{value:"Working with the official Crunchbase API",id:"working-with-the-official-crunchbase-api",level:2},{value:"1. Setting up API access",id:"1-setting-up-api-access",level:3},{value:"2. Configuring the crawler for API work",id:"2-configuring-the-crawler-for-api-work",level:3},{value:"3. Processing search results",id:"3-processing-search-results",level:3},{value:"4. Extracting company data",id:"4-extracting-company-data",level:3},{value:"5. Advanced location-based search",id:"5-advanced-location-based-search",level:3},{value:"6. Finally, free API limitations",id:"6-finally-free-api-limitations",level:3},{value:"What\u2019s your best path forward?",id:"whats-your-best-path-forward",level:2}];function h(e){let n={a:"a",admonition:"admonition",code:"code",h2:"h2",h3:"h3",img:"img",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,i.a)(),...e.components};return(0,s.jsxs)(s.Fragment,{children:[(0,s.jsxs)(n.p,{children:["Python developers know the drill: you need reliable company data, and Crunchbase has it. This guide shows you how to build an effective ",(0,s.jsx)(n.a,{href:"https://www.crunchbase.com/",children:"Crunchbase"})," scraper in Python that gets you the data you need."]}),"\n",(0,s.jsx)(n.p,{children:"Crunchbase tracks details that matter: locations, business focus, founders, and investment histories. Manual extraction from such a large dataset isn't practical -automation is essential for transforming this information into an analyzable format."}),"\n",(0,s.jsxs)(n.p,{children:["By the end of this blog, we'll explore three different ways to extract data from Crunchbase using ",(0,s.jsx)(n.a,{href:"https://github.com/apify/crawlee-python",children:(0,s.jsx)(n.code,{children:"Crawlee for Python"})}),". We'll fully implement two of them and discuss the specifics and challenges of the third. This will help us better understand how important it is to properly ",(0,s.jsx)(n.a,{href:"https://www.crawlee.dev/blog/web-scraping-tips#1-choosing-a-data-source-for-the-project",children:"choose the right data source"}),"."]}),"\n",(0,s.jsx)(n.admonition,{type:"note",children:(0,s.jsxs)(n.p,{children:["This guide comes from a developer in our growing community. Have you built interesting projects with Crawlee? Join us on ",(0,s.jsx)(n.a,{href:"https://discord.com/invite/jyEM2PRvMU",children:"Discord"})," to share your experiences and blog ideas - we value these contributions from developers like you."]})}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.img,{alt:"How to Scrape Crunchbase Using Python",src:t(97479).Z+"",width:"1152",height:"649"})}),"\n",(0,s.jsx)(n.p,{children:"Key steps we'll cover:"}),"\n",(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsx)(n.li,{children:"Project setup"}),"\n",(0,s.jsx)(n.li,{children:"Choosing the data source"}),"\n",(0,s.jsx)(n.li,{children:"Implementing sitemap-based crawler"}),"\n",(0,s.jsx)(n.li,{children:"Analysis of search-based approach and its limitations"}),"\n",(0,s.jsx)(n.li,{children:"Implementing the official API crawler"}),"\n",(0,s.jsx)(n.li,{children:"Conclusion and repository access"}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"prerequisites",children:"Prerequisites"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Python 3.9 or higher"}),"\n",(0,s.jsx)(n.li,{children:"Familiarity with web scraping concepts"}),"\n",(0,s.jsxs)(n.li,{children:["Crawlee for Python ",(0,s.jsx)(n.code,{children:"v0.5.0"})]}),"\n",(0,s.jsxs)(n.li,{children:["poetry ",(0,s.jsx)(n.code,{children:"v2.0"})," or higher"]}),"\n"]}),"\n",(0,s.jsx)(n.h3,{id:"project-setup",children:"Project setup"}),"\n",(0,s.jsxs)(n.p,{children:["Before we start scraping, we need to set up our project. In this guide, we won't be using crawler templates (",(0,s.jsx)(n.code,{children:"Playwright"})," and ",(0,s.jsx)(n.code,{children:"Beautifulsoup"}),"), so we'll set up the project manually."]}),"\n",(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:["Install ",(0,s.jsx)(n.a,{href:"https://python-poetry.org/",children:(0,s.jsx)(n.code,{children:"Poetry"})})]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"pipx install poetry\n"})}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsx)(n.p,{children:"Create and navigate to the project folder."}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"mkdir crunchbase-crawlee && cd crunchbase-crawlee\n"})}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsx)(n.p,{children:"Initialize the project using Poetry, leaving all fields empty."}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"poetry init\n"})}),"\n",(0,s.jsx)(n.p,{children:"When prompted:"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:['For "Compatible Python versions", enter: ',(0,s.jsx)(n.code,{children:">={your Python version},<4.0"}),"\n(For example, if you're using Python 3.10, enter: ",(0,s.jsx)(n.code,{children:">=3.10,<4.0"}),")"]}),"\n",(0,s.jsx)(n.li,{children:"Leave all other fields empty by pressing Enter"}),"\n",(0,s.jsx)(n.li,{children:'Confirm the generation by typing "yes"'}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:["Add and install Crawlee with necessary dependencies to your project using ",(0,s.jsx)(n.code,{children:"Poetry."})]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"poetry add crawlee[parsel,curl-impersonate]\n"})}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:["Complete the project setup by creating the standard file structure for ",(0,s.jsx)(n.code,{children:"Crawlee for Python"})," projects."]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"mkdir crunchbase-crawlee && touch crunchbase-crawlee/{__init__.py,__main__.py,main.py,routes.py}\n"})}),"\n"]}),"\n"]}),"\n",(0,s.jsx)(n.p,{children:"After setting up the basic project structure, we can explore different methods of obtaining data from Crunchbase."}),"\n",(0,s.jsx)(n.h3,{id:"choosing-the-data-source",children:"Choosing the data source"}),"\n",(0,s.jsxs)(n.p,{children:["While we can extract target data directly from the ",(0,s.jsx)(n.a,{href:"https://www.crunchbase.com/organization/apify",children:"company page"}),", we need to choose the best way to navigate the site."]}),"\n",(0,s.jsx)(n.p,{children:"A careful examination of Crunchbase's structure shows that we have three main options for obtaining data:"}),"\n",(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.a,{href:"https://www.crunchbase.com/www-sitemaps/sitemap-index.xml",children:(0,s.jsx)(n.code,{children:"Sitemap"})})," - for complete site traversal."]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.a,{href:"https://www.crunchbase.com/discover/organization.companies",children:(0,s.jsx)(n.code,{children:"Search"})})," - for targeted data collection."]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.a,{href:"https://data.crunchbase.com/v4-legacy/docs/crunchbase-basic-getting-started",children:"Official API"})," - recommended method."]}),"\n"]}),"\n",(0,s.jsx)(n.p,{children:"Let's examine each of these approaches in detail."}),"\n",(0,s.jsx)(n.h2,{id:"scraping-crunchbase-using-sitemap-and-crawlee-for-python",children:"Scraping Crunchbase using sitemap and Crawlee for Python"}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.code,{children:"Sitemap"})," is a standard way of site navigation used by crawlers like ",(0,s.jsx)(n.a,{href:"https://google.com/",children:(0,s.jsx)(n.code,{children:"Google"})}),", ",(0,s.jsx)(n.a,{href:"https://ahrefs.com/",children:(0,s.jsx)(n.code,{children:"Ahrefs"})}),", and other search engines. All crawlers must follow the rules described in ",(0,s.jsx)(n.a,{href:"https://www.crunchbase.com/robots.txt",children:(0,s.jsx)(n.code,{children:"robots.txt"})}),"."]}),"\n",(0,s.jsx)(n.p,{children:"Let's look at the structure of Crunchbase's Sitemap:"}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.img,{alt:"Sitemap first lvl",src:t(95997).Z+"",width:"1335",height:"994"})}),"\n",(0,s.jsxs)(n.p,{children:["As you can see, links to organization pages are located inside second-level ",(0,s.jsx)(n.code,{children:"Sitemap"})," files, which are compressed using ",(0,s.jsx)(n.code,{children:"gzip"}),"."]}),"\n",(0,s.jsx)(n.p,{children:"The structure of one of these files looks like this:"}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.img,{alt:"Sitemap second lvl",src:t(91349).Z+"",width:"1374",height:"919"})}),"\n",(0,s.jsxs)(n.p,{children:["The ",(0,s.jsx)(n.code,{children:"lastmod"})," field is particularly important here. It allows tracking which companies have updated their information since the previous data collection. This is especially useful for regular data updates."]}),"\n",(0,s.jsx)(n.h3,{id:"1-configuring-the-crawler-for-scraping",children:"1. Configuring the crawler for scraping"}),"\n",(0,s.jsxs)(n.p,{children:["To work with the site, we'll use ",(0,s.jsx)(n.a,{href:"https://www.crawlee.dev/python/api/class/CurlImpersonateHttpClient",children:(0,s.jsx)(n.code,{children:"CurlImpersonateHttpClient"})}),", which impersonates a ",(0,s.jsx)(n.code,{children:"Safari"})," browser. While this choice might seem unexpected for working with a sitemap, it's necessitated by Crunchbase's protection features."]}),"\n",(0,s.jsxs)(n.p,{children:["The reason is that Crunchbase uses ",(0,s.jsx)(n.a,{href:"https://www.cloudflare.com/",children:"Cloudflare"})," to protect against automated access. This is clearly visible when analyzing traffic on a company page:"]}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.img,{alt:"Cloudflare Link",src:t(47456).Z+"",width:"1919",height:"995"})}),"\n",(0,s.jsxs)(n.p,{children:["An interesting feature is that ",(0,s.jsx)(n.code,{children:"challenges.cloudflare"})," is executed after loading the document with data. This means we receive the data first, and only then JavaScript checks if we're a bot. If our HTTP client's fingerprint is sufficiently similar to a real browser, we'll successfully receive the data."]}),"\n",(0,s.jsxs)(n.p,{children:["Cloudflare ",(0,s.jsx)(n.a,{href:"https://developers.cloudflare.com/waf/custom-rules/use-cases/allow-traffic-from-verified-bots/",children:"also analyzes traffic at the sitemap level"}),". If our crawler doesn't look legitimate, access will be blocked. That's why we impersonate a real browser."]}),"\n",(0,s.jsxs)(n.p,{children:["To prevent blocks due to overly aggressive crawling, we'll configure ",(0,s.jsx)(n.a,{href:"https://www.crawlee.dev/python/api/class/ConcurrencySettings",children:(0,s.jsx)(n.code,{children:"ConcurrencySettings"})}),"."]}),"\n",(0,s.jsxs)(n.p,{children:["When scaling this approach, you'll likely need proxies. Detailed information about proxy setup can be found in the ",(0,s.jsx)(n.a,{href:"https://www.crawlee.dev/python/docs/guides/proxy-management",children:"documentation"}),"."]}),"\n",(0,s.jsxs)(n.p,{children:["We'll save our scraping results in ",(0,s.jsx)(n.code,{children:"JSON"})," format. Here's how the basic crawler configuration looks:"]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:"# main.py\n\nfrom crawlee import ConcurrencySettings, HttpHeaders\nfrom crawlee.crawlers import ParselCrawler\nfrom crawlee.http_clients import CurlImpersonateHttpClient\n\nfrom .routes import router\n\n\nasync def main() -> None:\n    \"\"\"The crawler entry point.\"\"\"\n    concurrency_settings = ConcurrencySettings(max_concurrency=1, max_tasks_per_minute=50)\n\n    http_client = CurlImpersonateHttpClient(\n        impersonate='safari17_0',\n        headers=HttpHeaders(\n            {\n                'accept-language': 'en',\n                'accept-encoding': 'gzip, deflate, br, zstd',\n            }\n        ),\n    )\n    crawler = ParselCrawler(\n        request_handler=router,\n        max_request_retries=1,\n        concurrency_settings=concurrency_settings,\n        http_client=http_client,\n        max_requests_per_crawl=30,\n    )\n\n    await crawler.run(['https://www.crunchbase.com/www-sitemaps/sitemap-index.xml'])\n\n    await crawler.export_data_json('crunchbase_data.json')\n"})}),"\n",(0,s.jsx)(n.h3,{id:"2-implementing-sitemap-navigation",children:"2. Implementing sitemap navigation"}),"\n",(0,s.jsx)(n.p,{children:"Sitemap navigation happens in two stages. In the first stage, we need to get a list of all files containing organization information:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'# routes.py\n\nfrom crawlee.crawlers import ParselCrawlingContext\nfrom crawlee.router import Router\nfrom crawlee import Request\n\nrouter = Router[ParselCrawlingContext]()\n\n\n@router.default_handler\nasync def default_handler(context: ParselCrawlingContext) -> None:\n    """Default request handler."""\n    context.log.info(f\'default_handler processing {context.request} ...\')\n\n    requests = [\n        Request.from_url(url, label=\'sitemap\')\n        for url in context.selector.xpath(\'//loc[contains(., "sitemap-organizations")]/text()\').getall()\n    ]\n\n    # Since this is a tutorial, I don\'t want to upload more than one sitemap link\n    await context.add_requests(requests, limit=1)\n'})}),"\n",(0,s.jsxs)(n.p,{children:["In the second stage, we process second-level sitemap files stored in ",(0,s.jsx)(n.code,{children:"gzip"})," format. This requires a special approach as the data needs to be decompressed first:"]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:"# routes.py\n\nfrom gzip import decompress\nfrom parsel import Selector\n\n\n@router.handler('sitemap')\nasync def sitemap_handler(context: ParselCrawlingContext) -> None:\n    \"\"\"Sitemap gzip request handler.\"\"\"\n    context.log.info(f'sitemap_handler processing {context.request.url} ...')\n\n    data = context.http_response.read()\n    data = decompress(data)\n\n    selector = Selector(data.decode())\n\n    requests = [Request.from_url(url, label='company') for url in selector.xpath('//loc/text()').getall()]\n\n    await context.add_requests(requests)\n"})}),"\n",(0,s.jsx)(n.h3,{id:"3-extracting-and-saving-data",children:"3. Extracting and saving data"}),"\n",(0,s.jsxs)(n.p,{children:["Each company page contains a large amount of information. For demonstration purposes, we'll focus on the main fields: ",(0,s.jsx)(n.code,{children:"Company Name"}),", ",(0,s.jsx)(n.code,{children:"Short Description"}),", ",(0,s.jsx)(n.code,{children:"Website"}),", and ",(0,s.jsx)(n.code,{children:"Location"}),"."]}),"\n",(0,s.jsxs)(n.p,{children:["One of Crunchbase's advantages is that all data is stored in ",(0,s.jsx)(n.code,{children:"JSON"})," format within the page:"]}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.img,{alt:"Company Data",src:t(57885).Z+"",width:"1919",height:"841"})}),"\n",(0,s.jsxs)(n.p,{children:["This significantly simplifies data extraction - we only need to use one ",(0,s.jsx)(n.code,{children:"Xpath"})," selector to get the ",(0,s.jsx)(n.code,{children:"JSON"}),", and then apply ",(0,s.jsx)(n.a,{href:"https://jmespath.org/",children:(0,s.jsx)(n.code,{children:"jmespath"})})," to extract the needed fields:"]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:"# routes.py\n\n@router.handler('company')\nasync def company_handler(context: ParselCrawlingContext) -> None:\n    \"\"\"Company request handler.\"\"\"\n    context.log.info(f'company_handler processing {context.request.url} ...')\n\n    json_selector = context.selector.xpath('//*[@id=\"ng-state\"]/text()')\n\n    await context.push_data(\n        {\n            'Company Name': json_selector.jmespath('HttpState.*.data[].properties.identifier.value').get(),\n            'Short Description': json_selector.jmespath('HttpState.*.data[].properties.short_description').get(),\n            'Website': json_selector.jmespath('HttpState.*.data[].cards.company_about_fields2.website.value').get(),\n            'Location': '; '.join(\n                json_selector.jmespath(\n                    'HttpState.*.data[].cards.company_about_fields2.location_identifiers[].value'\n                ).getall()\n            ),\n        }\n    )\n"})}),"\n",(0,s.jsxs)(n.p,{children:["The collected data is saved in ",(0,s.jsx)(n.code,{children:"Crawlee for Python"}),"'s internal storage using the ",(0,s.jsx)(n.code,{children:"context.push_data"})," method. When the crawler finishes, we export all collected data to a JSON file:"]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:"# main.py\n\nawait crawler.export_data_json('crunchbase_data.json')\n"})}),"\n",(0,s.jsx)(n.h3,{id:"4-running-the-project",children:"4. Running the project"}),"\n",(0,s.jsx)(n.p,{children:"With all components in place, we need to create an entry point for our crawler:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:"# __main__.py\nimport asyncio\n\nfrom .main import main\n\nif __name__ == '__main__':\n    asyncio.run(main())\n"})}),"\n",(0,s.jsx)(n.p,{children:"Execute the crawler using Poetry:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"poetry run python -m crunchbase-crawlee\n"})}),"\n",(0,s.jsx)(n.h3,{id:"5-finally-characteristics-of-using-the-sitemap-crawler",children:"5. Finally, characteristics of using the sitemap crawler"}),"\n",(0,s.jsx)(n.p,{children:"The sitemap approach has its distinct advantages and limitations. It's ideal in the following cases:"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"When you need to collect data about all companies on the platform"}),"\n",(0,s.jsx)(n.li,{children:"When there are no specific company selection criteria"}),"\n",(0,s.jsx)(n.li,{children:"If you have sufficient time and computational resources"}),"\n"]}),"\n",(0,s.jsx)(n.p,{children:"However, there are significant limitations to consider:"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Almost no ability to filter data during collection"}),"\n",(0,s.jsx)(n.li,{children:"Requires constant monitoring of Cloudflare blocks"}),"\n",(0,s.jsx)(n.li,{children:"Scaling the solution requires proxy servers, which increases project costs"}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"using-search-for-scraping-crunchbase",children:"Using search for scraping Crunchbase"}),"\n",(0,s.jsx)(n.p,{children:"The limitations of the sitemap approach might point to search as the next solution. However, Crunchbase applies tighter security measures to its search functionality compared to its public pages."}),"\n",(0,s.jsxs)(n.p,{children:["The key difference lies in how Cloudflare protection works. While we receive data before the ",(0,s.jsx)(n.code,{children:"challenges.cloudflare"})," check when accessing a company page, the search API requires valid ",(0,s.jsx)(n.code,{children:"cookies"})," that have passed this check."]}),"\n",(0,s.jsx)(n.p,{children:"Let's verify this in practice. Open the following link in Incognito mode:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-plaintext",children:"<https://www.crunchbase.com/v4/data/autocompletes?query=Ap&collection_ids=organizations&limit=25&source=topSearch>\n"})}),"\n",(0,s.jsx)(n.p,{children:"When analyzing the traffic, we'll see the following pattern:"}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.img,{alt:"Search Protect",src:t(15754).Z+"",width:"1916",height:"995"})}),"\n",(0,s.jsx)(n.p,{children:"The sequence of events here is:"}),"\n",(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsxs)(n.li,{children:["First, the page is blocked with code ",(0,s.jsx)(n.code,{children:"403"})]}),"\n",(0,s.jsxs)(n.li,{children:["Then the ",(0,s.jsx)(n.code,{children:"challenges.cloudflare"})," check is performed"]}),"\n",(0,s.jsxs)(n.li,{children:["Only after successfully passing the check do we receive data with code ",(0,s.jsx)(n.code,{children:"200"})]}),"\n"]}),"\n",(0,s.jsxs)(n.p,{children:["Automating this process would require a ",(0,s.jsx)(n.code,{children:"headless"})," browser capable of bypassing ",(0,s.jsx)(n.a,{href:"https://www.cloudflare.com/application-services/products/turnstile/",children:(0,s.jsx)(n.code,{children:"Cloudflare Turnstile"})}),". The current version of ",(0,s.jsx)(n.code,{children:"Crawlee for Python"})," (v0.5.0) doesn't provide this functionality, although it's planned for future development."]}),"\n",(0,s.jsxs)(n.p,{children:["You can extend the capabilities of Crawlee for Python by integrating ",(0,s.jsx)(n.a,{href:"https://camoufox.com/",children:(0,s.jsx)(n.code,{children:"Camoufox"})})," following this ",(0,s.jsx)(n.a,{href:"https://www.crawlee.dev/python/docs/examples/playwright-crawler-with-camoufox",children:"example."})]}),"\n",(0,s.jsx)(n.h2,{id:"working-with-the-official-crunchbase-api",children:"Working with the official Crunchbase API"}),"\n",(0,s.jsxs)(n.p,{children:["Crunchbase provides a ",(0,s.jsx)(n.a,{href:"https://data.crunchbase.com/v4-legacy/docs/crunchbase-basic-using-api",children:"free API"})," with basic functionality. Paid subscription users get expanded data access. Complete documentation for available endpoints can be found in the ",(0,s.jsx)(n.a,{href:"https://app.swaggerhub.com/apis-docs/Crunchbase/crunchbase-enterprise_api",children:"official API specification"}),"."]}),"\n",(0,s.jsx)(n.h3,{id:"1-setting-up-api-access",children:"1. Setting up API access"}),"\n",(0,s.jsx)(n.p,{children:"To start working with the API, follow these steps:"}),"\n",(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsx)(n.li,{children:(0,s.jsx)(n.a,{href:"https://www.crunchbase.com/register",children:"Create a Crunchbase account"})}),"\n",(0,s.jsx)(n.li,{children:"Go to the Integrations section"}),"\n",(0,s.jsx)(n.li,{children:"Create a Crunchbase Basic API key"}),"\n"]}),"\n",(0,s.jsx)(n.p,{children:"Although the documentation states that key activation may take up to an hour, it usually starts working immediately after creation."}),"\n",(0,s.jsx)(n.h3,{id:"2-configuring-the-crawler-for-api-work",children:"2. Configuring the crawler for API work"}),"\n",(0,s.jsxs)(n.p,{children:["An important API feature is the limit - no more than 200 requests per minute, but in the free version, this number is significantly lower. Taking this into account, let's configure ",(0,s.jsx)(n.a,{href:"https://www.crawlee.dev/python/api/class/ConcurrencySettings",children:(0,s.jsx)(n.code,{children:"ConcurrencySettings"})}),". Since we're working with the official API, we don't need to mask our HTTP client. We'll use the standard ",(0,s.jsx)(n.a,{href:"https://www.crawlee.dev/python/api/class/HttpxHttpClient",children:"'HttpxHttpClient'"})," with preset headers."]}),"\n",(0,s.jsx)(n.p,{children:"First, let's save the API key in an environment variable:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"export CRUNCHBASE_TOKEN={YOUR KEY}\n"})}),"\n",(0,s.jsx)(n.p,{children:"Here's how the crawler configuration for working with the API looks:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:"# main.py\n\nimport os\n\nfrom crawlee.crawlers import HttpCrawler\nfrom crawlee.http_clients import HttpxHttpClient\nfrom crawlee import ConcurrencySettings, HttpHeaders\n\nfrom .routes import router\n\nCRUNCHBASE_TOKEN = os.getenv('CRUNCHBASE_TOKEN', '')\n\n\nasync def main() -> None:\n    \"\"\"The crawler entry point.\"\"\"\n\n    concurrency_settings = ConcurrencySettings(max_tasks_per_minute=60)\n\n    http_client = HttpxHttpClient(\n        headers=HttpHeaders({'accept-encoding': 'gzip, deflate, br, zstd', 'X-cb-user-key': CRUNCHBASE_TOKEN})\n    )\n    crawler = HttpCrawler(\n        request_handler=router,\n        concurrency_settings=concurrency_settings,\n        http_client=http_client,\n        max_requests_per_crawl=30,\n    )\n\n    await crawler.run(\n        ['https://api.crunchbase.com/api/v4/autocompletes?query=apify&collection_ids=organizations&limit=25']\n    )\n\n    await crawler.export_data_json('crunchbase_data.json')\n"})}),"\n",(0,s.jsx)(n.h3,{id:"3-processing-search-results",children:"3. Processing search results"}),"\n",(0,s.jsx)(n.p,{children:"For working with the API, we'll need two main endpoints:"}),"\n",(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.a,{href:"https://app.swaggerhub.com/apis-docs/Crunchbase/crunchbase-enterprise_api/1.0.3#/Autocomplete/get_autocompletes",children:"get_autocompletes"})," - for searching"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.a,{href:"https://app.swaggerhub.com/apis-docs/Crunchbase/crunchbase-enterprise_api/1.0.3#/Entity/get_entities_organizations__entity_id_",children:"get_entities_organizations__entity_id"})," - for getting data"]}),"\n"]}),"\n",(0,s.jsx)(n.p,{children:"First, let's implement search results processing:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:"import json\n\nfrom crawlee.crawlers import HttpCrawler\nfrom crawlee.router import Router\nfrom crawlee import Request\n\nrouter = Router[HttpCrawlingContext]()\n\n\n@router.default_handler\nasync def default_handler(context: HttpCrawlingContext) -> None:\n    \"\"\"Default request handler.\"\"\"\n    context.log.info(f'default_handler processing {context.request.url} ...')\n\n    data = json.loads(context.http_response.read())\n\n    requests = []\n\n    for entity in data['entities']:\n        permalink = entity['identifier']['permalink']\n        requests.append(\n            Request.from_url(\n                url=f'https://api.crunchbase.com/api/v4/entities/organizations/{permalink}?field_ids=short_description%2Clocation_identifiers%2Cwebsite_url',\n                label='company',\n            )\n        )\n\n    await context.add_requests(requests)\n"})}),"\n",(0,s.jsx)(n.h3,{id:"4-extracting-company-data",children:"4. Extracting company data"}),"\n",(0,s.jsx)(n.p,{children:"After getting the list of companies, we extract detailed information about each one:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:"@router.handler('company')\nasync def company_handler(context: HttpCrawlingContext) -> None:\n    \"\"\"Company request handler.\"\"\"\n    context.log.info(f'company_handler processing {context.request.url} ...')\n\n    data = json.loads(context.http_response.read())\n\n    await context.push_data(\n        {\n            'Company Name': data['properties']['identifier']['value'],\n            'Short Description': data['properties']['short_description'],\n            'Website': data['properties'].get('website_url'),\n            'Location': '; '.join([item['value'] for item in data['properties'].get('location_identifiers', [])]),\n        }\n    )\n"})}),"\n",(0,s.jsx)(n.h3,{id:"5-advanced-location-based-search",children:"5. Advanced location-based search"}),"\n",(0,s.jsxs)(n.p,{children:["If you need more flexible search capabilities, the API provides a special ",(0,s.jsx)(n.a,{href:"https://app.swaggerhub.com/apis-docs/Crunchbase/crunchbase-enterprise_api/1.0.3#/Search/post_searches_organizations",children:(0,s.jsx)(n.code,{children:"search"})})," endpoint. Here's an example of searching for all companies in Prague:"]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:"payload = {\n    'field_ids': ['identifier', 'location_identifiers', 'short_description', 'website_url'],\n    'limit': 200,\n    'order': [{'field_id': 'rank_org', 'sort': 'asc'}],\n    'query': [\n        {\n            'field_id': 'location_identifiers',\n            'operator_id': 'includes',\n            'type': 'predicate',\n            'values': ['e0b951dc-f710-8754-ddde-5ef04dddd9f8'],\n        },\n        {'field_id': 'facet_ids', 'operator_id': 'includes', 'type': 'predicate', 'values': ['company']},\n    ],\n}\n\nserialiazed_payload = json.dumps(payload)\nawait crawler.run(\n    [\n        Request.from_url(\n            url='https://api.crunchbase.com/api/v4/searches/organizations',\n            method='POST',\n            payload=serialiazed_payload,\n            use_extended_unique_key=True,\n            headers=HttpHeaders({'Content-Type': 'application/json'}),\n            label='search',\n        )\n    ]\n)\n"})}),"\n",(0,s.jsx)(n.p,{children:"For processing search results and pagination, we use the following handler:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:"@router.handler('search')\nasync def search_handler(context: HttpCrawlingContext) -> None:\n    \"\"\"Search results handler with pagination support.\"\"\"\n    context.log.info(f'search_handler processing {context.request.url} ...')\n\n    data = json.loads(context.http_response.read())\n\n    last_entity = None\n    results = []\n\n    for entity in data['entities']:\n        last_entity = entity['uuid']\n        results.append(\n            {\n                'Company Name': entity['properties']['identifier']['value'],\n                'Short Description': entity['properties']['short_description'],\n                'Website': entity['properties'].get('website_url'),\n                'Location': '; '.join([item['value'] for item in entity['properties'].get('location_identifiers', [])]),\n            }\n        )\n\n    if results:\n        await context.push_data(results)\n\n    if last_entity:\n        payload = json.loads(context.request.payload)\n        payload['after_id'] = last_entity\n        payload = json.dumps(payload)\n\n        await context.add_requests(\n            [\n                Request.from_url(\n                    url='https://api.crunchbase.com/api/v4/searches/organizations',\n                    method='POST',\n                    payload=payload,\n                    use_extended_unique_key=True,\n                    headers=HttpHeaders({'Content-Type': 'application/json'}),\n                    label='search',\n                )\n            ]\n        )\n"})}),"\n",(0,s.jsx)(n.h3,{id:"6-finally-free-api-limitations",children:"6. Finally, free API limitations"}),"\n",(0,s.jsx)(n.p,{children:"The free version of the API has significant limitations:"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Limited set of available endpoints"}),"\n",(0,s.jsx)(n.li,{children:"Autocompletes function only works for company searches"}),"\n",(0,s.jsx)(n.li,{children:"Not all data fields are accessible"}),"\n",(0,s.jsx)(n.li,{children:"Limited search filtering capabilities"}),"\n"]}),"\n",(0,s.jsx)(n.p,{children:"Consider a paid subscription for production-level work. The API provides the most reliable way to access Crunchbase data, even with its rate constraints."}),"\n",(0,s.jsx)(n.h2,{id:"whats-your-best-path-forward",children:"What\u2019s your best path forward?"}),"\n",(0,s.jsx)(n.p,{children:"We've explored three different approaches to obtaining data from Crunchbase:"}),"\n",(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Sitemap"})," - for large-scale data collection"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Search"})," - difficult to automate due to Cloudflare protection"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Official API"})," - the most reliable solution for commercial projects"]}),"\n"]}),"\n",(0,s.jsx)(n.p,{children:"Each method has its advantages, but for most projects, I recommend using the official API despite its limitations in the free version."}),"\n",(0,s.jsxs)(n.p,{children:["The complete source code is available in my ",(0,s.jsx)(n.a,{href:"https://github.com/Mantisus/crunchbase-crawlee",children:"repository"}),". Have questions or want to discuss implementation details? Join our ",(0,s.jsx)(n.a,{href:"https://discord.com/invite/jyEM2PRvMU",children:"Discord"})," - our community of developers is there to help."]})]})}function d(e={}){let{wrapper:n}={...(0,i.a)(),...e.components};return n?(0,s.jsx)(n,{...e,children:(0,s.jsx)(h,{...e})}):h(e)}},6893:function(e,n,t){t.d(n,{Z:function(){return r}});let r=t.p+"assets/images/scrape_crunchbase-28a71b5380492fe6618bbd9c90989543.webp"},47456:function(e,n,t){t.d(n,{Z:function(){return r}});let r=t.p+"assets/images/cloudflare_link-bf8b6ba2c873ccb31463258e5964e39b.webp"},57885:function(e,n,t){t.d(n,{Z:function(){return r}});let r=t.p+"assets/images/data_json-7c79a7387510a995f29ba5ce157f0845.webp"},97479:function(e,n,t){t.d(n,{Z:function(){return r}});let r=t.p+"assets/images/scrape_crunchbase-28a71b5380492fe6618bbd9c90989543.webp"},15754:function(e,n,t){t.d(n,{Z:function(){return r}});let r=t.p+"assets/images/search_protect-3b4a1a1934d54c12ac210217919b8b88.webp"},95997:function(e,n,t){t.d(n,{Z:function(){return r}});let r=t.p+"assets/images/sitemap_lvl_one-553a6b9df5c5d3c35a8987878456fe7b.webp"},91349:function(e,n,t){t.d(n,{Z:function(){return r}});let r=t.p+"assets/images/sitemap_lvl_two-8f3213f305713ebf8bf91b32febfa234.webp"},50065:function(e,n,t){t.d(n,{Z:function(){return o},a:function(){return a}});var r=t(67294);let s={},i=r.createContext(s);function a(e){let n=r.useContext(i);return r.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function o(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(s):e.components||s:a(e.components),r.createElement(i.Provider,{value:n},e.children)}},5317:function(e){e.exports=JSON.parse('{"permalink":"/blog/scrape-crunchbase-python","source":"@site/blog/2025/01-03-scrape-crunchbase/index.md","title":"How to scrape Crunchbase using Python in 2024 (Easy Guide)","description":"Learn how to scrape Crunchbase using Crawlee for Python","date":"2025-01-03T00:00:00.000Z","tags":[{"inline":true,"label":"community","permalink":"/blog/tags/community"}],"readingTime":10.79,"hasTruncateMarker":true,"authors":[{"name":"Max","title":"Community Member of Crawlee and web scraping expert","url":"https://github.com/Mantisus","socials":{"github":"https://github.com/Mantisus"},"imageURL":"https://avatars.githubusercontent.com/u/34358312?v=4","key":"MaxB","page":null}],"frontMatter":{"slug":"scrape-crunchbase-python","title":"How to scrape Crunchbase using Python in 2024 (Easy Guide)","tags":["community"],"description":"Learn how to scrape Crunchbase using Crawlee for Python","image":"./img/scrape_crunchbase.webp","authors":["MaxB"]},"unlisted":false,"prevItem":{"title":"Crawlee for Python v0.5","permalink":"/blog/crawlee-for-python-v05"},"nextItem":{"title":"How to scrape Google Maps data using Python","permalink":"/blog/scrape-google-maps"}}')}}]);