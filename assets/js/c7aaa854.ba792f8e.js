"use strict";(self.webpackChunk=self.webpackChunk||[]).push([["59557"],{11679:function(e,t,n){n.r(t),n.d(t,{default:()=>w,frontMatter:()=>d,metadata:()=>a,assets:()=>p,toc:()=>u,contentTitle:()=>c});var a=JSON.parse('{"id":"examples/file-download","title":"Download a file","description":"When webcrawling, you sometimes need to download files such as images, PDFs, or other binary files. This example demonstrates how to download files using Crawlee and save them to the default key-value store.","source":"@site/versioned_docs/version-3.13/examples/file_download.mdx","sourceDirName":"examples","slug":"/examples/file-download","permalink":"/docs/examples/file-download","draft":false,"unlisted":false,"editUrl":"https://github.com/apify/crawlee/edit/master/website/versioned_docs/version-3.13/examples/file_download.mdx","tags":[],"version":"3.13","lastUpdatedBy":"Martin Ad\xe1mek","lastUpdatedAt":1741081496000,"frontMatter":{"id":"file-download","title":"Download a file"},"sidebar":"docs","previous":{"title":"Export entire dataset to one file","permalink":"/docs/examples/export-entire-dataset"},"next":{"title":"Download a file with Node.js streams","permalink":"/docs/examples/file-download-stream"}}'),o=n("85893"),s=n("50065"),l=n("60643"),i=n("47927");let r={code:"import { FileDownload } from 'crawlee';\n\n// Create a FileDownload - a custom crawler instance that will download files from URLs.\nconst crawler = new FileDownload({\n    async requestHandler({ body, request, contentType, getKeyValueStore }) {\n        const url = new URL(request.url);\n        const kvs = await getKeyValueStore();\n\n        await kvs.setValue(url.pathname.replace(/\\//g, '_'), body, { contentType: contentType.type });\n    },\n});\n\n// The initial list of URLs to crawl. Here we use just a few hard-coded URLs.\nawait crawler.addRequests([\n    'https://pdfobject.com/pdf/sample.pdf',\n    'https://download.blender.org/peach/bigbuckbunny_movies/BigBuckBunny_320x180.mp4',\n    'https://upload.wikimedia.org/wikipedia/commons/c/c8/Example.ogg',\n]);\n\n// Run the downloader and wait for it to finish.\nawait crawler.run();\n\nconsole.log('Crawler finished.');\n",hash:"invalid-token"},d={id:"file-download",title:"Download a file"},c=void 0,p={},u=[];function h(e){let t={code:"code",p:"p",...(0,s.a)(),...e.components};return(0,o.jsxs)(o.Fragment,{children:[(0,o.jsx)(t.p,{children:"When webcrawling, you sometimes need to download files such as images, PDFs, or other binary files. This example demonstrates how to download files using Crawlee and save them to the default key-value store."}),"\n",(0,o.jsxs)(t.p,{children:["The script simply downloads several files with plain HTTP requests using the custom ",(0,o.jsx)(i.Z,{to:"http-crawler/class/FileDownload",children:(0,o.jsx)(t.code,{children:"FileDownload"})})," crawler class and stores their contents in the default key-value store.\nIn local configuration, the data will be stored as files in ",(0,o.jsx)(t.code,{children:"./storage/key_value_stores/default"}),"."]}),"\n",(0,o.jsx)(l.Z,{className:"language-js",type:"cheerio",children:r})]})}function w(e={}){let{wrapper:t}={...(0,s.a)(),...e.components};return t?(0,o.jsx)(t,{...e,children:(0,o.jsx)(h,{...e})}):h(e)}},47927:function(e,t,n){n.d(t,{Z:function(){return c}});var a=n(85893);n(67294);var o=n(53367),s=n(89873),l=n(87262);let[i,r]=n(99074).version.split("."),d=[i,r].join("."),c=e=>{let{to:t,children:n}=e,i=(0,s.E)(),{siteConfig:r}=(0,l.default)();return r.presets[0][1].docs.disableVersioning||i.version===d?(0,a.jsx)(o.default,{to:`/api/${t}`,children:n}):(0,a.jsx)(o.default,{to:`/api/${"current"===i.version?"next":i.version}/${t}`,children:n})}},60643:function(e,t,n){n.d(t,{Z:()=>d});var a=n("85893");n("67294");var o=n("67026"),s=n("96199"),l=n("53367");let i={button:"button_YBBj",container:"container_TGAW"},r={playwright:"6i5QsHBMtm3hKph70",puppeteer:"7tWSD8hrYzuc9Lte7",cheerio:"kk67IcZkKSSBTslXI"},d=e=>{let{children:t,actor:n,hash:d,type:c,...p}=e;if(d=d??t.hash,!t.code)throw Error(`RunnableCodeBlock requires "code" and "hash" props
Make sure you are importing the code block contents with the roa-loader.`);if(!d)return(0,a.jsx)(s.default,{...p,children:t.code});let u=`https://console.apify.com/actors/${n??r[c??"playwright"]}?runConfig=${d}&asrc=run_on_apify`;return(0,a.jsxs)("div",{className:(0,o.Z)(i.container,"runnable-code-block"),children:[(0,a.jsxs)(l.default,{href:u,className:i.button,rel:"follow",children:["Run on",(0,a.jsxs)("svg",{width:"91",height:"25",viewBox:"0 0 91 25",fill:"none",xmlns:"http://www.w3.org/2000/svg",className:"apify-logo-light alignMiddle_src-theme-Footer-index-module",children:[(0,a.jsx)("path",{d:"M3.135 2.85A3.409 3.409 0 0 0 .227 6.699l2.016 14.398 8.483-19.304-7.59 1.059Z",fill:"#97D700"}),(0,a.jsx)("path",{d:"M23.604 14.847 22.811 3.78a3.414 3.414 0 0 0-3.64-3.154c-.077 0-.153.014-.228.025l-3.274.452 7.192 16.124c.54-.67.805-1.52.743-2.379Z",fill:"#71C5E8"}),(0,a.jsx)("path",{d:"M5.336 24.595c.58.066 1.169-.02 1.706-.248l12.35-5.211L13.514 5.97 5.336 24.595Z",fill:"#FF9013"}),(0,a.jsx)("path",{d:"M33.83 5.304h3.903l5.448 14.623h-3.494l-1.022-2.994h-5.877l-1.025 2.994h-3.384L33.83 5.304Zm-.177 9.032h4.14l-2-5.994h-.086l-2.054 5.994ZM58.842 5.304h3.302v14.623h-3.302V5.304ZM64.634 5.304h10.71v2.7h-7.4v4.101h5.962v2.632h-5.963v5.186h-3.309V5.303ZM82.116 14.38l-5.498-9.076h3.748l3.428 6.016h.085l3.599-6.016H91l-5.56 9.054v5.569h-3.324v-5.548ZM51.75 5.304h-7.292v14.623h3.3v-4.634h3.993a4.995 4.995 0 1 0 0-9.99Zm-.364 7.417h-3.628V7.875h3.627a2.423 2.423 0 0 1 0 4.846Z",className:"apify-logo",fill:"#000"})]})]}),(0,a.jsx)(s.default,{...p,className:(0,o.Z)(i.codeBlock,"code-block",null!=p.title?"has-title":"no-title"),children:t.code})]})}},50065:function(e,t,n){n.d(t,{Z:function(){return i},a:function(){return l}});var a=n(67294);let o={},s=a.createContext(o);function l(e){let t=a.useContext(s);return a.useMemo(function(){return"function"==typeof e?e(t):{...t,...e}},[t,e])}function i(e){let t;return t=e.disableParentContext?"function"==typeof e.components?e.components(o):e.components||o:l(e.components),a.createElement(s.Provider,{value:t},e.children)}},99074:function(e){e.exports=JSON.parse('{"name":"crawlee","version":"3.13.0","description":"The scalable web crawling and scraping library for JavaScript/Node.js. Enables development of data extraction and web automation jobs (not only) with headless Chrome and Puppeteer.","engines":{"node":">=16.0.0"},"bin":"./src/cli.ts","main":"./dist/index.js","module":"./dist/index.mjs","types":"./dist/index.d.ts","exports":{".":{"import":"./dist/index.mjs","require":"./dist/index.js","types":"./dist/index.d.ts"},"./package.json":"./package.json"},"keywords":["apify","headless","chrome","puppeteer","crawler","scraper"],"author":{"name":"Apify","email":"support@apify.com","url":"https://apify.com"},"contributors":["Jan Curn <jan@apify.com>","Marek Trunkat <marek@apify.com>","Ondra Urban <ondra@apify.com>"],"license":"Apache-2.0","repository":{"type":"git","url":"git+https://github.com/apify/crawlee"},"bugs":{"url":"https://github.com/apify/crawlee/issues"},"homepage":"https://crawlee.dev","scripts":{"build":"yarn clean && yarn compile && yarn copy","clean":"rimraf ./dist","compile":"tsc -p tsconfig.build.json && gen-esm-wrapper ./dist/index.js ./dist/index.mjs","copy":"tsx ../../scripts/copy.ts"},"publishConfig":{"access":"public"},"dependencies":{"@crawlee/basic":"3.13.0","@crawlee/browser":"3.13.0","@crawlee/browser-pool":"3.13.0","@crawlee/cheerio":"3.13.0","@crawlee/cli":"3.13.0","@crawlee/core":"3.13.0","@crawlee/http":"3.13.0","@crawlee/jsdom":"3.13.0","@crawlee/linkedom":"3.13.0","@crawlee/playwright":"3.13.0","@crawlee/puppeteer":"3.13.0","@crawlee/utils":"3.13.0","import-local":"^3.1.0","tslib":"^2.4.0"},"peerDependencies":{"playwright":"*","puppeteer":"*"},"peerDependenciesMeta":{"playwright":{"optional":true},"puppeteer":{"optional":true}}}')}}]);