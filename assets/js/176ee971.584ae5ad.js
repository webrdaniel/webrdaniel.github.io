"use strict";(self.webpackChunk=self.webpackChunk||[]).push([["66282"],{83734:function(e,n,t){t.r(n),t.d(n,{assets:function(){return o},contentTitle:function(){return l},default:function(){return h},frontMatter:function(){return i},metadata:function(){return a},toc:function(){return c}});var a=t(2267),s=t(85893),r=t(50065);let i={slug:"scrape-google-maps",title:"How to scrape Google Maps data using Python",tags:["community"],description:"Learn how to scrape google maps data using Crawlee for Python",image:"./img/google-maps.webp",authors:["SatyamT"]},l=void 0,o={image:t(48795).Z,authorsImageUrls:[void 0]},c=[{value:"What data will we extract from Google Maps?",id:"what-data-will-we-extract-from-google-maps",level:2},{value:"Building a Google Maps scraper",id:"building-a-google-maps-scraper",level:2},{value:"1. Setting up your environment",id:"1-setting-up-your-environment",level:3},{value:"2. Connecting to Google Maps",id:"2-connecting-to-google-maps",level:3},{value:"3. Import dependencies and defining Scraper Class",id:"3-import-dependencies-and-defining-scraper-class",level:3},{value:"4. Understanding Google Maps internal code structure",id:"4-understanding-google-maps-internal-code-structure",level:3},{value:"5. Scraping Google Maps data using identified selectors",id:"5-scraping-google-maps-data-using-identified-selectors",level:3},{value:"6. Managing Infinite Scrolling",id:"6-managing-infinite-scrolling",level:3},{value:"7. Scrape Listings",id:"7-scrape-listings",level:3},{value:"8. Running the Scraper",id:"8-running-the-scraper",level:3},{value:"9. Using proxies for Google Maps scraping",id:"9-using-proxies-for-google-maps-scraping",level:3},{value:"10. Project: Interactive hotel analysis dashboard",id:"10-project-interactive-hotel-analysis-dashboard",level:3},{value:"11. Now you\u2019re ready to put everything into action!",id:"11-now-youre-ready-to-put-everything-into-action",level:3},{value:"Wrapping up and next steps",id:"wrapping-up-and-next-steps",level:2}];function d(e){let n={a:"a",admonition:"admonition",code:"code",em:"em",h2:"h2",h3:"h3",img:"img",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,r.a)(),...e.components};return(0,s.jsxs)(s.Fragment,{children:[(0,s.jsx)(n.p,{children:"Millions of people use Google Maps daily, leaving behind a goldmine of data just waiting to be analyzed. In this guide, I'll show you how to build a reliable scraper using Crawlee and Python to extract locations, ratings, and reviews from Google Maps, all while handling its dynamic content challenges."}),"\n",(0,s.jsx)(n.admonition,{type:"note",children:(0,s.jsxs)(n.p,{children:["One of our community members wrote this blog as a contribution to the Crawlee Blog. If you would like to contribute blogs like these to Crawlee Blog, please reach out to us on our ",(0,s.jsx)(n.a,{href:"https://apify.com/discord",children:"discord channel"}),"."]})}),"\n",(0,s.jsx)(n.h2,{id:"what-data-will-we-extract-from-google-maps",children:"What data will we extract from Google Maps?"}),"\n",(0,s.jsx)(n.p,{children:'We\u2019ll collect information about hotels in a specific city. You can also customize your search to meet your requirements. For example, you might search for "hotels near me", "5-star hotels in Bombay", or other similar queries.'}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.img,{alt:"Google Maps Data Screenshot",src:t(20595).Z+"",width:"1906",height:"879"})}),"\n",(0,s.jsx)(n.p,{children:"We\u2019ll extract important data, including the hotel name, rating, review count, price, a link to the hotel page on Google Maps, and all available amenities. Here\u2019s an example of what the extracted data will look like:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-json",children:'{\n    "name": "Vividus Hotels, Bangalore",\n    "rating": "4.3",\n    "reviews": "633",\n    "price": "\u20B93,667",\n    "amenities": [\n        "Pool available",\n        "Free breakfast available",\n        "Free Wi-Fi available",\n        "Free parking available"\n    ],\n    "link": "https://www.google.com/maps/place/Vividus+Hotels+,+Bangalore/..."\n}\n'})}),"\n",(0,s.jsx)(n.h2,{id:"building-a-google-maps-scraper",children:"Building a Google Maps scraper"}),"\n",(0,s.jsx)(n.p,{children:"Let's build a Google Maps scraper step-by-step."}),"\n",(0,s.jsx)(n.admonition,{type:"note",children:(0,s.jsx)(n.p,{children:"Crawlee requires Python 3.9 or later."})}),"\n",(0,s.jsx)(n.h3,{id:"1-setting-up-your-environment",children:"1. Setting up your environment"}),"\n",(0,s.jsx)(n.p,{children:"First, let's set up everything you\u2019ll need to run the scraper. Open your terminal and run these commands:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:'# Create and activate a virtual environment\npython -m venv google-maps-scraper\n\n# Windows:\n.\\google-maps-scraper\\Scripts\\activate\n\n# Mac/Linux:\nsource google-maps-scraper/bin/activate\n\n# We plan to use Playwright with Crawlee, so we need to install both:\npip install crawlee "crawlee[playwright]"\nplaywright install\n'})}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsxs)(n.em,{children:["If you're new to ",(0,s.jsx)(n.strong,{children:"Crawlee"}),", check out its easy-to-follow documentation. It\u2019s available for both ",(0,s.jsx)(n.a,{href:"https://www.crawlee.dev/docs/quick-start",children:"Node.js"})," and ",(0,s.jsx)(n.a,{href:"https://www.crawlee.dev/python/docs/quick-start",children:"Python"}),"."]})}),"\n",(0,s.jsx)(n.admonition,{type:"note",children:(0,s.jsxs)(n.p,{children:["Before going ahead with the project, I'd like to ask you to star Crawlee for Python on ",(0,s.jsx)(n.a,{href:"https://github.com/apify/crawlee-python/",children:"GitHub"}),", it helps us to spread the word to fellow scraper developers."]})}),"\n",(0,s.jsx)(n.h3,{id:"2-connecting-to-google-maps",children:"2. Connecting to Google Maps"}),"\n",(0,s.jsx)(n.p,{children:"Let's see the steps to connect to Google Maps."}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"Step 1: Setting up the crawler"})}),"\n",(0,s.jsxs)(n.p,{children:["The first step is to configure the crawler. We're using ",(0,s.jsx)(n.a,{href:"https://www.crawlee.dev/python/api/class/PlaywrightCrawler",children:(0,s.jsx)(n.code,{children:"PlaywrightCrawler"})})," from Crawlee, which gives us powerful tools for automated browsing. We set ",(0,s.jsx)(n.code,{children:"headless=False"})," to make the browser visible during scraping and allow 5 minutes for the pages to load."]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:"from crawlee.playwright_crawler import PlaywrightCrawler\nfrom datetime import timedelta\n\n# Initialize crawler with browser visibility and timeout settings\ncrawler = PlaywrightCrawler(\n    headless=False,  # Shows the browser window while scraping\n    request_handler_timeout=timedelta(\n        minutes=5\n    ),  # Allows plenty of time for page loading\n)\n"})}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"Step 2: Handling each page"})}),"\n",(0,s.jsxs)(n.p,{children:["This function defines how each page is handled when the crawler visits it. It uses ",(0,s.jsx)(n.code,{children:"context.page"})," to navigate to the target URL."]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'async def scrape_google_maps(context):\n    """\n    Establishes connection to Google Maps and handles the initial page load\n    """\n    page = context.page\n    await page.goto(context.request.url)\n    context.log.info(f"Processing: {context.request.url}")\n'})}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"Step 3: Launching the crawler"})}),"\n",(0,s.jsx)(n.p,{children:"Finally, the main function brings everything together. It creates a search URL, sets up the crawler, and starts the scraping process."}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'import asyncio\n\nasync def main():\n    # Prepare the search URL\n    search_query = "hotels in bengaluru"\n    start_url = f"https://www.google.com/maps/search/{search_query.replace(\' \', \'+\')}"\n\n    # Tell the crawler how to handle each page it visits\n    crawler.router.default_handler(scrape_google_maps)\n\n    # Start the scraping process\n    await crawler.run([start_url])\n\nif __name__ == "__main__":\n    asyncio.run(main())\n'})}),"\n",(0,s.jsxs)(n.p,{children:["Let\u2019s combine the above code snippets and save them in a file named ",(0,s.jsx)(n.code,{children:"gmap_scraper.py"}),":"]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'from crawlee.playwright_crawler import PlaywrightCrawler\nfrom datetime import timedelta\nimport asyncio\n\nasync def scrape_google_maps(context):\n    """\n    Establishes connection to Google Maps and handles the initial page load\n    """\n    page = context.page\n    await page.goto(context.request.url)\n    context.log.info(f"Processing: {context.request.url}")\n\nasync def main():\n    """\n    Configures and launches the crawler with custom settings\n    """\n    # Initialize crawler with browser visibility and timeout settings\n    crawler = PlaywrightCrawler(\n        headless=False,  # Shows the browser window while scraping\n        request_handler_timeout=timedelta(\n            minutes=5\n        ),  # Allows plenty of time for page loading\n    )\n\n    # Tell the crawler how to handle each page it visits\n    crawler.router.default_handler(scrape_google_maps)\n\n    # Prepare the search URL\n    search_query = "hotels in bengaluru"\n    start_url = f"https://www.google.com/maps/search/{search_query.replace(\' \', \'+\')}"\n\n    # Start the scraping process\n    await crawler.run([start_url])\n\nif __name__ == "__main__":\n    asyncio.run(main())\n'})}),"\n",(0,s.jsx)(n.p,{children:"Run the code using:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"$ python3 gmap_scraper.py\n"})}),"\n",(0,s.jsx)(n.p,{children:"When everything works correctly, you'll see the output like this:"}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.img,{alt:"Connect to page",src:t(63709).Z+"",width:"1280",height:"720"})}),"\n",(0,s.jsx)(n.h3,{id:"3-import-dependencies-and-defining-scraper-class",children:"3. Import dependencies and defining Scraper Class"}),"\n",(0,s.jsx)(n.p,{children:"Let's start with the basic structure and necessary imports:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:"import asyncio\nfrom datetime import timedelta\nfrom typing import Dict, Optional, Set\nfrom crawlee.playwright_crawler import PlaywrightCrawler\nfrom playwright.async_api import Page, ElementHandle\n"})}),"\n",(0,s.jsxs)(n.p,{children:["The ",(0,s.jsx)(n.code,{children:"GoogleMapsScraper"})," class serves as the main scraper engine:"]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:"class GoogleMapsScraper:\n    def __init__(self, headless: bool = True, timeout_minutes: int = 5):\n        self.crawler = PlaywrightCrawler(\n            headless=headless,\n            request_handler_timeout=timedelta(minutes=timeout_minutes),\n        )\n        self.processed_names: Set[str] = set()\n\n    async def setup_crawler(self) -> None:\n        self.crawler.router.default_handler(self._scrape_listings)\n"})}),"\n",(0,s.jsx)(n.p,{children:"This initialization code sets up two crucial components:"}),"\n",(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsxs)(n.li,{children:["A ",(0,s.jsx)(n.code,{children:"PlaywrightCrawler"})," instance configured to run either headlessly (without a visible browser window) or with a visible browser"]}),"\n",(0,s.jsx)(n.li,{children:"A set to track processed business names, preventing duplicate entries"}),"\n"]}),"\n",(0,s.jsxs)(n.p,{children:["The ",(0,s.jsx)(n.code,{children:"setup_crawler"})," method configures the crawler to use our main scraping function as the default handler for all requests."]}),"\n",(0,s.jsx)(n.h3,{id:"4-understanding-google-maps-internal-code-structure",children:"4. Understanding Google Maps internal code structure"}),"\n",(0,s.jsx)(n.p,{children:"Before we dive into scraping, let's understand exactly what elements we need to target. When you search for hotels in Bengaluru, Google Maps organizes hotel information in a specific structure. Here's a detailed breakdown of how to locate each piece of information."}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"Hotel name:"})}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.img,{alt:"Hotel name",src:t(39919).Z+"",width:"1906",height:"705"})}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"Hotel rating:"})}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.img,{alt:"Hotel rating",src:t(3367).Z+"",width:"1908",height:"706"})}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"Hotel review count:"})}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.img,{alt:"Hotel Review Count",src:t(38405).Z+"",width:"1908",height:"709"})}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"Hotel URL:"})}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.img,{alt:"Hotel URL",src:t(88356).Z+"",width:"1905",height:"679"})}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"Hotel Price:"})}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.img,{alt:"Hotel Price",src:t(77169).Z+"",width:"1894",height:"751"})}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"Hotel amenities:"})}),"\n",(0,s.jsx)(n.p,{children:"This returns multiple elements as each hotel has several amenities. We'll need to iterate through these."}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.img,{alt:"Hotel amenities",src:t(7144).Z+"",width:"1731",height:"772"})}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"Quick tips:"})}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Always verify these selectors before scraping, as Google might update them."}),"\n",(0,s.jsx)(n.li,{children:"Use Chrome DevTools (F12) to inspect elements and confirm selectors."}),"\n",(0,s.jsx)(n.li,{children:"Some elements might not be present for all hotels (like prices during the off-season)."}),"\n"]}),"\n",(0,s.jsx)(n.h3,{id:"5-scraping-google-maps-data-using-identified-selectors",children:"5. Scraping Google Maps data using identified selectors"}),"\n",(0,s.jsx)(n.p,{children:"Let's build a scraper to extract detailed hotel information from Google Maps. First, create the core scraping function to handle data extraction."}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.em,{children:"gmap_scraper.py:"})}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'async def _extract_listing_data(self, listing: ElementHandle) -> Optional[Dict]:\n    """Extract structured data from a single listing element."""\n    try:\n        name_el = await listing.query_selector(".qBF1Pd")\n        if not name_el:\n            return None\n        name = await name_el.inner_text()\n        if name in self.processed_names:\n            return None\n\n        elements = {\n            "rating": await listing.query_selector(".MW4etd"),\n            "reviews": await listing.query_selector(".UY7F9"),\n            "price": await listing.query_selector(".wcldff"),\n            "link": await listing.query_selector("a.hfpxzc"),\n            "address": await listing.query_selector(".W4Efsd:nth-child(2)"),\n            "category": await listing.query_selector(".W4Efsd:nth-child(1)"),\n        }\n\n        amenities = []\n        amenities_els = await listing.query_selector_all(".dc6iWb")\n        for amenity in amenities_els:\n            amenity_text = await amenity.get_attribute("aria-label")\n            if amenity_text:\n                amenities.append(amenity_text)\n\n        place_data = {\n            "name": name,\n            "rating": await elements["rating"].inner_text() if elements["rating"] else None,\n            "reviews": (await elements["reviews"].inner_text()).strip("()") if elements["reviews"] else None,\n            "price": await elements["price"].inner_text() if elements["price"] else None,\n            "address": await elements["address"].inner_text() if elements["address"] else None,\n            "category": await elements["category"].inner_text() if elements["category"] else None,\n            "amenities": amenities if amenities else None,\n            "link": await elements["link"].get_attribute("href") if elements["link"] else None,\n        }\n\n        self.processed_names.add(name)\n        return place_data\n    except Exception as e:\n        context.log.exception("Error extracting listing data")\n        return None\n'})}),"\n",(0,s.jsx)(n.p,{children:"In the code:"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.code,{children:"query_selector"}),": Returns first DOM element matching CSS selector, useful for single items like a name or rating"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.code,{children:"query_selector_all"}),": Returns all matching elements, ideal for multiple items like amenities"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.code,{children:"inner_text()"}),": Extracts text content"]}),"\n",(0,s.jsx)(n.li,{children:"Some hotels might not have all the information available - we handle this with 'N/A\u2019"}),"\n"]}),"\n",(0,s.jsx)(n.p,{children:"When you run this script, you'll see output similar to this:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-json",children:'{\n    "name": "GRAND KALINGA HOTEL",\n    "rating": "4.2",\n    "reviews": "1,171",\n    "price": "\\u20b91,760",\n    "link": "https://www.google.com/maps/place/GRAND+KALINGA+HOTEL/data=!4m10!3m9!1s0x3bae160e0ce07789:0xb15bf736f4238e6a!5m2!4m1!1i2!8m2!3d12.9762259!4d77.5786043!16s%2Fg%2F11sp32pz28!19sChIJiXfgDA4WrjsRao4j9Db3W7E?authuser=0&hl=en&rclk=1",\n    "amenities": [\n        "Pool available",\n        "Free breakfast available",\n        "Free Wi-Fi available",\n        "Free parking available"\n    ]\n}\n'})}),"\n",(0,s.jsx)(n.h3,{id:"6-managing-infinite-scrolling",children:"6. Managing Infinite Scrolling"}),"\n",(0,s.jsx)(n.p,{children:"Google Maps uses infinite scrolling to load more results as users scroll down. We handle this with a dedicated method:"}),"\n",(0,s.jsxs)(n.p,{children:["First, we need a function that can handle the scrolling and detect when we've hit the bottom. Copy-paste this new function in the ",(0,s.jsx)(n.code,{children:"gmap_scraper.py"})," file:"]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'async def _load_more_items(self, page: Page) -> bool:\n        """Scroll down to load more items."""\n        try:\n            feed = await page.query_selector(\'div[role="feed"]\')\n            if not feed:\n                return False\n            prev_scroll = await feed.evaluate("(element) => element.scrollTop")\n            await feed.evaluate("(element) => element.scrollTop += 800")\n            await page.wait_for_timeout(2000)\n\n            new_scroll = await feed.evaluate("(element) => element.scrollTop")\n            if new_scroll <= prev_scroll:\n                return False\n            await page.wait_for_timeout(1000)\n            return True\n        except Exception as e:\n            context.log.exception("Error during scroll")\n            return False\n'})}),"\n",(0,s.jsx)(n.p,{children:"Run this code using:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"$ python3 gmap_scraper.py\n"})}),"\n",(0,s.jsx)(n.p,{children:"You should see an output like this:"}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.img,{alt:"scrape-google-maps-with-crawlee-screenshot-handle-pagination",src:t(57507).Z+"",width:"1125",height:"120"})}),"\n",(0,s.jsx)(n.h3,{id:"7-scrape-listings",children:"7. Scrape Listings"}),"\n",(0,s.jsx)(n.p,{children:"The main scraping function ties everything together. It scrapes listings from the page by repeatedly extracting data and scrolling."}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'async def _scrape_listings(self, context) -> None:\n    """Main scraping function to process all listings"""\n    try:\n        page = context.page\n        print(f"\\nProcessing URL: {context.request.url}\\n")\n\n        await page.wait_for_selector(".Nv2PK", timeout=30000)\n        await page.wait_for_timeout(2000)\n\n        while True:\n            listings = await page.query_selector_all(".Nv2PK")\n            new_items = 0\n\n            for listing in listings:\n                place_data = await self._extract_listing_data(listing)\n                if place_data:\n                    await context.push_data(place_data)\n                    new_items += 1\n                    print(f"Processed: {place_data[\'name\']}")\n\n            if new_items == 0 and not await self._load_more_items(page):\n                break\n            if new_items > 0:\n                await self._load_more_items(page)\n                \n        print(f"\\nFinished processing! Total items: {len(self.processed_names)}")\n    except Exception as e:\n        print(f"Error in scraping: {str(e)}")\n'})}),"\n",(0,s.jsxs)(n.p,{children:["The scraper uses Crawlee's built-in storage system to manage scraped data. When you run the scraper, it creates a ",(0,s.jsx)(n.code,{children:"storage"})," directory in your project with several key components:"]}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.code,{children:"datasets/"}),": Contains the scraped results in JSON format"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.code,{children:"key_value_stores/"}),": Stores crawler state and metadata"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.code,{children:"request_queues/"}),": Manages URLs to be processed"]}),"\n"]}),"\n",(0,s.jsxs)(n.p,{children:["The ",(0,s.jsx)(n.code,{children:"push_data()"})," method we use in our scraper sends the data to Crawlee's dataset storage as you can see below:"]}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.img,{alt:"Crawlee push_data",src:t(29178).Z+"",width:"1192",height:"542"})}),"\n",(0,s.jsx)(n.h3,{id:"8-running-the-scraper",children:"8. Running the Scraper"}),"\n",(0,s.jsx)(n.p,{children:"Finally, we need functions to execute our scraper:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'async def run(self, search_query: str) -> None:\n    """Execute the scraper with a search query"""\n    try:\n        await self.setup_crawler()\n        start_url = f"https://www.google.com/maps/search/{search_query.replace(\' \', \'+\')}"\n        await self.crawler.run([start_url])\n        await self.crawler.export_data_json(\'gmap_data.json\')\n    except Exception as e:\n        print(f"Error running scraper: {str(e)}")\n\nasync def main():\n    """Entry point of the script"""\n    scraper = GoogleMapsScraper(headless=True)\n    search_query = "hotels in bengaluru"\n    await scraper.run(search_query)\n\nif __name__ == "__main__":\n    asyncio.run(main())\n'})}),"\n",(0,s.jsx)(n.p,{children:"This data is automatically stored and can later be exported to a JSON file using:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:"await self.crawler.export_data_json('gmap_data.json')\n"})}),"\n",(0,s.jsx)(n.p,{children:"Here's what your exported JSON file will look like:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-json",children:'[\n  {\n    "name": "Vividus Hotels, Bangalore",\n    "rating": "4.3",\n    "reviews": "633",\n    "price": "\u20B93,667",\n    "amenities": [\n      "Pool available",\n      "Free breakfast available",\n      "Free Wi-Fi available",\n      "Free parking available"\n    ],\n    "link": "https://www.google.com/maps/place/Vividus+Hotels+,+Bangalore/..."\n  }\n]\n'})}),"\n",(0,s.jsx)(n.h3,{id:"9-using-proxies-for-google-maps-scraping",children:"9. Using proxies for Google Maps scraping"}),"\n",(0,s.jsx)(n.p,{children:"When scraping Google Maps at scale, using proxies is very helpful. Here are a few key reasons why:"}),"\n",(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Avoid IP blocks"}),": Google Maps can detect and block IP addresses that make an excessive number of requests in a short time. Using proxies helps you stay under the radar."]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Bypass rate limits"}),": Google implements strict limits on the number of requests per IP address. By rotating through multiple IPs, you can maintain a consistent scraping pace without hitting these limits."]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Access location-specific data"}),": Different regions may display different data on Google Maps. Proxies allow you to view listings as if you are browsing from any specific location."]}),"\n"]}),"\n",(0,s.jsx)(n.p,{children:"Here's a simple implementation using Crawlee's built-in proxy management. Update your previous code with this to use proxy settings."}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'from crawlee.playwright_crawler import PlaywrightCrawler\nfrom crawlee.proxy_configuration import ProxyConfiguration\n\n# Configure your proxy settings\nproxy_configuration = ProxyConfiguration(\n    proxy_urls=[\n        "http://username:password@proxy.provider.com:12345",\n        # Add more proxy URLs as needed\n    ]\n)\n\n# Initialize crawler with proxy support\ncrawler = PlaywrightCrawler(\n    headless=True,\n    request_handler_timeout=timedelta(minutes=5),\n    proxy_configuration=proxy_configuration,\n)\n'})}),"\n",(0,s.jsx)(n.p,{children:"Here, I use a proxy to scrape hotel data in New York City."}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.img,{alt:"Using a proxy",src:t(69601).Z+"",width:"1791",height:"833"})}),"\n",(0,s.jsx)(n.p,{children:"Here's an example of data scraped from New York City hotels using proxies:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-json",children:'{\n  "name": "The Manhattan at Times Square Hotel",\n  "rating": "3.1",\n  "reviews": "8,591",\n  "price": "$120",\n  "amenities": [\n    "Free parking available",\n    "Free Wi-Fi available",\n    "Air-conditioned available",\n    "Breakfast available"\n  ],\n  "link": "https://www.google.com/maps/place/..."\n}\n'})}),"\n",(0,s.jsx)(n.h3,{id:"10-project-interactive-hotel-analysis-dashboard",children:"10. Project: Interactive hotel analysis dashboard"}),"\n",(0,s.jsx)(n.p,{children:"After scraping hotel data from Google Maps, you can build an interactive dashboard that helps analyze hotel trends. Here\u2019s a preview of how the dashboard works:"}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.img,{alt:"Final dashboard",src:t(87104).Z+"",width:"1905",height:"833"})}),"\n",(0,s.jsxs)(n.p,{children:["Find the complete info for this dashboard on GitHub: ",(0,s.jsx)(n.a,{href:"https://github.com/triposat/Hotel-Analytics-Dashboard",children:"Hotel Analysis Dashboard"}),"."]}),"\n",(0,s.jsx)(n.h3,{id:"11-now-youre-ready-to-put-everything-into-action",children:"11. Now you\u2019re ready to put everything into action!"}),"\n",(0,s.jsx)(n.p,{children:"Take a look at the complete scripts in my GitHub Gist:"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:(0,s.jsx)(n.a,{href:"https://gist.github.com/triposat/9a6fb03130f3c4332bab71b72a973940",children:"Basic Scraper"})}),"\n",(0,s.jsx)(n.li,{children:(0,s.jsx)(n.a,{href:"https://gist.github.com/triposat/6c554b13c787a55348b48b6bfc5459c0",children:"Code with Proxy Integration"})}),"\n",(0,s.jsx)(n.li,{children:(0,s.jsx)(n.a,{href:"https://gist.github.com/triposat/13ce4b05c36512e69b5602833e781a6c",children:"Hotel Analysis Dashboard"})}),"\n"]}),"\n",(0,s.jsx)(n.p,{children:"To make it all work:"}),"\n",(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Run the basic scraper or proxy-integrated scraper"}),": This will collect the hotel data and store it in a JSON file."]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Run the dashboard script"}),": Load your JSON data and view it interactively in the dashboard."]}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"wrapping-up-and-next-steps",children:"Wrapping up and next steps"}),"\n",(0,s.jsx)(n.p,{children:"You've successfully built a comprehensive Google Maps scraper that collects and processes hotel data, presenting it through an interactive dashboard. Now you\u2019ve learned about:"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Using Crawlee with Playwright to navigate and extract data from Google Maps"}),"\n",(0,s.jsx)(n.li,{children:"Using proxies to scale up scraping without getting blocked"}),"\n",(0,s.jsx)(n.li,{children:"Storing the extracted data in JSON format"}),"\n",(0,s.jsx)(n.li,{children:"Creating an interactive dashboard to analyze hotel data"}),"\n"]}),"\n",(0,s.jsx)(n.p,{children:"We\u2019ve handpicked some great resources to help you further explore web scraping:"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:(0,s.jsx)(n.a,{href:"https://www.crawlee.dev/blog/scrapy-vs-crawlee",children:"Scrapy vs. Crawlee: Choosing the right tool"})}),"\n",(0,s.jsx)(n.li,{children:(0,s.jsx)(n.a,{href:"https://wwww.crawlee.dev/blog/proxy-management-in-crawlee",children:"Mastering proxy management with Crawlee"})}),"\n",(0,s.jsx)(n.li,{children:(0,s.jsx)(n.a,{href:"https://www.crawlee.dev/blog/web-scraping-tips",children:"Think like a web scraping expert: 12 pro tips"})}),"\n",(0,s.jsx)(n.li,{children:(0,s.jsx)(n.a,{href:"https://www.crawlee.dev/blog/linkedin-job-scraper-python",children:"Building a LinkedIn job scraper"})}),"\n"]})]})}function h(e={}){let{wrapper:n}={...(0,r.a)(),...e.components};return n?(0,s.jsx)(n,{...e,children:(0,s.jsx)(d,{...e})}):d(e)}},48795:function(e,n,t){t.d(n,{Z:function(){return a}});let a=t.p+"assets/images/google-maps-fb70c1ab3b5c3032b0291db3392eb32d.webp"},29178:function(e,n,t){t.d(n,{Z:function(){return a}});let a=t.p+"assets/images/How-to-scrape-Google-Maps-data-using-Python-and-Crawlee-metadata-a27257a5ffffad0fdcc598064445fe57.webp"},7144:function(e,n,t){t.d(n,{Z:function(){return a}});let a=t.p+"assets/images/scrape-google-maps-with-crawlee-screenshot-amenities-8a138b2fc9d7c4fad6a81bec55ee5db7.webp"},63709:function(e,n,t){t.d(n,{Z:function(){return a}});let a=t.p+"assets/images/scrape-google-maps-with-crawlee-screenshot-connect-to-page-6d6391022d64446a161825935a307d8d.png"},20595:function(e,n,t){t.d(n,{Z:function(){return a}});let a=t.p+"assets/images/scrape-google-maps-with-crawlee-screenshot-data-to-scrape-00e7e4e3498679b8a7611eafd0a1bfbe.webp"},57507:function(e,n,t){t.d(n,{Z:function(){return a}});let a=t.p+"assets/images/scrape-google-maps-with-crawlee-screenshot-handle-pagination-319232595ced535f175346ae0003e32f.webp"},87104:function(e,n,t){t.d(n,{Z:function(){return a}});let a=t.p+"assets/images/scrape-google-maps-with-crawlee-screenshot-hotel-analysis-dashboard-c14806409a7c1db63943f58d855aa07e.webp"},39919:function(e,n,t){t.d(n,{Z:function(){return a}});let a=t.p+"assets/images/scrape-google-maps-with-crawlee-screenshot-name-d1fcc59eb4e3eec109fcbf5be0237fbc.webp"},77169:function(e,n,t){t.d(n,{Z:function(){return a}});let a=t.p+"assets/images/scrape-google-maps-with-crawlee-screenshot-price-a2ab8516020bfcbfd6054d889f871743.webp"},69601:function(e,n,t){t.d(n,{Z:function(){return a}});let a=t.p+"assets/images/scrape-google-maps-with-crawlee-screenshot-proxies-5c4dece0247a87e7d338328c472cea74.webp"},3367:function(e,n,t){t.d(n,{Z:function(){return a}});let a=t.p+"assets/images/scrape-google-maps-with-crawlee-screenshot-ratings-7748ca46b1e14126de728add8313d286.webp"},38405:function(e,n,t){t.d(n,{Z:function(){return a}});let a=t.p+"assets/images/scrape-google-maps-with-crawlee-screenshot-reviews-521c92ebf7eeefb615659e0cd9cce6eb.webp"},88356:function(e,n,t){t.d(n,{Z:function(){return a}});let a=t.p+"assets/images/scrape-google-maps-with-crawlee-screenshot-url-ef8f37822fe579765ece5c37c1f8fdeb.webp"},50065:function(e,n,t){t.d(n,{Z:function(){return l},a:function(){return i}});var a=t(67294);let s={},r=a.createContext(s);function i(e){let n=a.useContext(r);return a.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function l(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(s):e.components||s:i(e.components),a.createElement(r.Provider,{value:n},e.children)}},2267:function(e){e.exports=JSON.parse('{"permalink":"/blog/scrape-google-maps","source":"@site/blog/2024/12-13-scrape-google-maps-using-python/index.md","title":"How to scrape Google Maps data using Python","description":"Learn how to scrape google maps data using Crawlee for Python","date":"2024-12-13T00:00:00.000Z","tags":[{"inline":true,"label":"community","permalink":"/blog/tags/community"}],"readingTime":10.865,"hasTruncateMarker":true,"authors":[{"name":"Satyam Tripathi","title":"Community Member of Crawlee","url":"https://github.com/triposat","socials":{"github":"https://github.com/triposat"},"imageURL":"https://avatars.githubusercontent.com/u/69134468?v=4","key":"SatyamT","page":null}],"frontMatter":{"slug":"scrape-google-maps","title":"How to scrape Google Maps data using Python","tags":["community"],"description":"Learn how to scrape google maps data using Crawlee for Python","image":"./img/google-maps.webp","authors":["SatyamT"]},"unlisted":false,"prevItem":{"title":"How to scrape Crunchbase using Python in 2024 (Easy Guide)","permalink":"/blog/scrape-crunchbase-python"},"nextItem":{"title":"How to scrape Google search results with Python","permalink":"/blog/scrape-google-search"}}')}}]);