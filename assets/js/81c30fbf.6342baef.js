"use strict";(self.webpackChunk=self.webpackChunk||[]).push([["17885"],{67947:function(e,t,n){n.r(t),n.d(t,{assets:function(){return l},contentTitle:function(){return o},default:function(){return d},frontMatter:function(){return i},metadata:function(){return r},toc:function(){return c}});var r=n(351),a=n(85893),s=n(50065);let i={slug:"scrape-google-search",title:"How to scrape Google search results with Python",tags:["community"],description:"Learn how to scrape google search results using Crawlee for Python",image:"./img/google-search.webp",authors:["MaxB"]},o=void 0,l={image:n(2086).Z,authorsImageUrls:[void 0]},c=[{value:"Prerequisites",id:"prerequisites",level:2},{value:"Project setup",id:"project-setup",level:3},{value:"Development of the Google Search scraper in Python",id:"development-of-the-google-search-scraper-in-python",level:2},{value:"1. Defining data for extraction",id:"1-defining-data-for-extraction",level:3},{value:"2. Configure the crawler",id:"2-configure-the-crawler",level:3},{value:"3. Implementing data extraction",id:"3-implementing-data-extraction",level:3},{value:"4. Handling pagination",id:"4-handling-pagination",level:3},{value:"5. Exporting data to CSV format",id:"5-exporting-data-to-csv-format",level:3},{value:"6. Finalizing the Google Search scraper",id:"6-finalizing-the-google-search-scraper",level:3},{value:"Scrape Google Search results with Apify",id:"scrape-google-search-results-with-apify",level:2},{value:"What will you scrape?",id:"what-will-you-scrape",level:2}];function h(e){let t={a:"a",admonition:"admonition",code:"code",em:"em",h2:"h2",h3:"h3",img:"img",li:"li",ol:"ol",p:"p",pre:"pre",ul:"ul",...(0,s.a)(),...e.components};return(0,a.jsxs)(a.Fragment,{children:[(0,a.jsxs)(t.p,{children:["Scraping ",(0,a.jsx)(t.code,{children:"Google Search"})," delivers essential ",(0,a.jsx)(t.code,{children:"SERP analysis"}),", SEO optimization, and data collection capabilities. Modern scraping tools make this process faster and more reliable."]}),"\n",(0,a.jsx)(t.admonition,{type:"note",children:(0,a.jsxs)(t.p,{children:["One of our community members wrote this blog as a contribution to the Crawlee Blog. If you would like to contribute blogs like these to Crawlee Blog, please reach out to us on our ",(0,a.jsx)(t.a,{href:"https://apify.com/discord",children:"discord channel"}),"."]})}),"\n",(0,a.jsxs)(t.p,{children:["In this guide, we'll create a Google Search scraper using ",(0,a.jsx)(t.a,{href:"https://github.com/apify/crawlee-python",children:(0,a.jsx)(t.code,{children:"Crawlee for Python"})})," that can handle result ranking and pagination."]}),"\n",(0,a.jsx)(t.p,{children:"We'll create a scraper that:"}),"\n",(0,a.jsxs)(t.ul,{children:["\n",(0,a.jsx)(t.li,{children:"Extracts titles, URLs, and descriptions from search results"}),"\n",(0,a.jsx)(t.li,{children:"Handles multiple search queries"}),"\n",(0,a.jsx)(t.li,{children:"Tracks ranking positions"}),"\n",(0,a.jsx)(t.li,{children:"Processes multiple result pages"}),"\n",(0,a.jsx)(t.li,{children:"Saves data in a structured format"}),"\n"]}),"\n",(0,a.jsx)(t.p,{children:(0,a.jsx)(t.img,{alt:"How to scrape Google search results with Python",src:n(35635).Z+"",width:"1152",height:"649"})}),"\n",(0,a.jsx)(t.h2,{id:"prerequisites",children:"Prerequisites"}),"\n",(0,a.jsxs)(t.ul,{children:["\n",(0,a.jsx)(t.li,{children:"Python 3.7 or higher"}),"\n",(0,a.jsx)(t.li,{children:"Basic understanding of HTML and CSS selectors"}),"\n",(0,a.jsx)(t.li,{children:"Familiarity with web scraping concepts"}),"\n",(0,a.jsx)(t.li,{children:"Crawlee for Python v0.4.2 or higher"}),"\n"]}),"\n",(0,a.jsx)(t.h3,{id:"project-setup",children:"Project setup"}),"\n",(0,a.jsxs)(t.ol,{children:["\n",(0,a.jsxs)(t.li,{children:["\n",(0,a.jsx)(t.p,{children:"Install Crawlee with required dependencies:"}),"\n",(0,a.jsx)(t.pre,{children:(0,a.jsx)(t.code,{className:"language-bash",children:"pipx install crawlee[beautifulsoup,curl-impersonate]\n"})}),"\n"]}),"\n",(0,a.jsxs)(t.li,{children:["\n",(0,a.jsx)(t.p,{children:"Create a new project using Crawlee CLI:"}),"\n",(0,a.jsx)(t.pre,{children:(0,a.jsx)(t.code,{className:"language-bash",children:"pipx run crawlee create crawlee-google-search\n"})}),"\n"]}),"\n",(0,a.jsxs)(t.li,{children:["\n",(0,a.jsxs)(t.p,{children:["When prompted, select ",(0,a.jsx)(t.code,{children:"Beautifulsoup"})," as your template type."]}),"\n"]}),"\n",(0,a.jsxs)(t.li,{children:["\n",(0,a.jsx)(t.p,{children:"Navigate to the project directory and complete installation:"}),"\n",(0,a.jsx)(t.pre,{children:(0,a.jsx)(t.code,{className:"language-bash",children:"cd crawlee-google-search\npoetry install\n"})}),"\n"]}),"\n"]}),"\n",(0,a.jsx)(t.h2,{id:"development-of-the-google-search-scraper-in-python",children:"Development of the Google Search scraper in Python"}),"\n",(0,a.jsx)(t.h3,{id:"1-defining-data-for-extraction",children:"1. Defining data for extraction"}),"\n",(0,a.jsx)(t.p,{children:"First, let's define our extraction scope. Google's search results now include maps, notable people, company details, videos, common questions, and many other elements. We'll focus on analyzing standard search results with rankings."}),"\n",(0,a.jsx)(t.p,{children:"Here's what we'll be extracting:"}),"\n",(0,a.jsx)(t.p,{children:(0,a.jsx)(t.img,{alt:"Search Example",src:n(16607).Z+"",width:"1873",height:"813"})}),"\n",(0,a.jsxs)(t.p,{children:["Let's verify whether we can extract the necessary data from the page's HTML code, or if we need deeper analysis or ",(0,a.jsx)(t.code,{children:"JS"})," rendering. Note that this verification is sensitive to HTML tags:"]}),"\n",(0,a.jsx)(t.p,{children:(0,a.jsx)(t.img,{alt:"Check Html",src:n(44095).Z+"",width:"1917",height:"951"})}),"\n",(0,a.jsxs)(t.p,{children:["Based on the data obtained from the page, all necessary information is present in the HTML code. Therefore, we can use ",(0,a.jsx)(t.a,{href:"https://www.crawlee.dev/python/docs/examples/beautifulsoup-crawler",children:(0,a.jsx)(t.code,{children:"beautifulsoup_crawler"})}),"."]}),"\n",(0,a.jsx)(t.p,{children:"The fields we'll extract:"}),"\n",(0,a.jsxs)(t.ul,{children:["\n",(0,a.jsx)(t.li,{children:"Search result titles"}),"\n",(0,a.jsx)(t.li,{children:"URLs"}),"\n",(0,a.jsx)(t.li,{children:"Description text"}),"\n",(0,a.jsx)(t.li,{children:"Ranking positions"}),"\n"]}),"\n",(0,a.jsx)(t.h3,{id:"2-configure-the-crawler",children:"2. Configure the crawler"}),"\n",(0,a.jsx)(t.p,{children:"First, let's create the crawler configuration."}),"\n",(0,a.jsxs)(t.p,{children:["We'll use ",(0,a.jsx)(t.a,{href:"https://www.crawlee.dev/python/api/class/CurlImpersonateHttpClient",children:(0,a.jsx)(t.code,{children:"CurlImpersonateHttpClient"})})," as our ",(0,a.jsx)(t.code,{children:"http_client"})," with preset ",(0,a.jsx)(t.code,{children:"headers"})," and ",(0,a.jsx)(t.code,{children:"impersonate"})," relevant to the ",(0,a.jsx)(t.a,{href:"https://www.google.com/intl/en/chrome/",children:(0,a.jsx)(t.code,{children:"Chrome"})})," browser."]}),"\n",(0,a.jsxs)(t.p,{children:["We'll also configure ",(0,a.jsx)(t.a,{href:"https://www.crawlee.dev/python/api/class/ConcurrencySettings",children:(0,a.jsx)(t.code,{children:"ConcurrencySettings"})})," to control scraping aggressiveness. This is crucial to avoid getting blocked by Google."]}),"\n",(0,a.jsxs)(t.p,{children:["If you need to extract data more intensively, consider setting up ",(0,a.jsx)(t.a,{href:"https://www.crawlee.dev/python/api/class/ProxyConfiguration",children:(0,a.jsx)(t.code,{children:"ProxyConfiguration"})}),"."]}),"\n",(0,a.jsx)(t.pre,{children:(0,a.jsx)(t.code,{className:"language-python",children:'from crawlee.beautifulsoup_crawler import BeautifulSoupCrawler\nfrom crawlee.http_clients.curl_impersonate import CurlImpersonateHttpClient\nfrom crawlee import ConcurrencySettings, HttpHeaders\n\nasync def main() -> None:\n    concurrency_settings = ConcurrencySettings(max_concurrency=5, max_tasks_per_minute=200)\n\n    http_client = CurlImpersonateHttpClient(impersonate="chrome124",\n                                            headers=HttpHeaders({"referer": "https://www.google.com/",\n                                                     "accept-language": "en",\n                                                     "accept-encoding": "gzip, deflate, br, zstd",\n                                                     "user-agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/131.0.0.0 Safari/537.36"\n                                            }))\n\n    crawler = BeautifulSoupCrawler(\n        max_request_retries=1,\n        concurrency_settings=concurrency_settings,\n        http_client=http_client,\n        max_requests_per_crawl=10,\n        max_crawl_depth=5\n    )\n\n    await crawler.run([\'https://www.google.com/search?q=Apify\'])\n'})}),"\n",(0,a.jsx)(t.h3,{id:"3-implementing-data-extraction",children:"3. Implementing data extraction"}),"\n",(0,a.jsx)(t.p,{children:"First, let's analyze the HTML code of the elements we need to extract:"}),"\n",(0,a.jsx)(t.p,{children:(0,a.jsx)(t.img,{alt:"Check Html",src:n(82976).Z+"",width:"1916",height:"931"})}),"\n",(0,a.jsxs)(t.p,{children:["There's an obvious distinction between ",(0,a.jsx)(t.em,{children:"readable"})," ID attributes and ",(0,a.jsx)(t.em,{children:"generated"})," class names and other attributes. When creating selectors for data extraction, you should ignore any generated attributes. Even if you've read that Google has been using a particular generated tag for N years, you shouldn't rely on it - this reflects your experience in writing robust code."]}),"\n",(0,a.jsxs)(t.p,{children:["Now that we understand the HTML structure, let's implement the extraction. As our crawler deals with only one type of page, we can use ",(0,a.jsx)(t.code,{children:"router.default_handler"})," for processing it. Within the handler, we'll use ",(0,a.jsx)(t.code,{children:"BeautifulSoup"})," to iterate through each search result, extracting data such as ",(0,a.jsx)(t.code,{children:"title"}),", ",(0,a.jsx)(t.code,{children:"url"}),", and ",(0,a.jsx)(t.code,{children:"text_widget"})," while saving the results."]}),"\n",(0,a.jsx)(t.pre,{children:(0,a.jsx)(t.code,{className:"language-python",children:'@crawler.router.default_handler\nasync def default_handler(context: BeautifulSoupCrawlingContext) -> None:\n    """Default request handler."""\n    context.log.info(f\'Processing {context.request} ...\')\n\n    for item in context.soup.select("div#search div#rso div[data-hveid][lang]"):\n        data = {\n            \'title\': item.select_one("h3").get_text(),\n            "url": item.select_one("a").get("href"),\n            "text_widget": item.select_one("div[style*=\'line\']").get_text(),\n        }\n        await context.push_data(data)\n'})}),"\n",(0,a.jsx)(t.h3,{id:"4-handling-pagination",children:"4. Handling pagination"}),"\n",(0,a.jsx)(t.p,{children:"Since Google results depend on the IP geolocation of the search request, we can't rely on link text for pagination. We need to create a more sophisticated CSS selector that works regardless of geolocation and language settings."}),"\n",(0,a.jsxs)(t.p,{children:["The ",(0,a.jsx)(t.code,{children:"max_crawl_depth"})," parameter controls how many pages our crawler should scan. Once we have our robust selector, we simply need to get the next page link and add it to the crawler's queue."]}),"\n",(0,a.jsxs)(t.p,{children:["To write more efficient selectors, learn the basics of ",(0,a.jsx)(t.a,{href:"https://www.w3schools.com/cssref/css_selectors.php",children:"CSS"})," and ",(0,a.jsx)(t.a,{href:"https://www.w3schools.com/xml/xpath_syntax.asp",children:"XPath"})," syntax."]}),"\n",(0,a.jsx)(t.pre,{children:(0,a.jsx)(t.code,{className:"language-python",children:"    await context.enqueue_links(selector=\"div[role='navigation'] td[role='heading']:last-of-type > a\")\n"})}),"\n",(0,a.jsx)(t.h3,{id:"5-exporting-data-to-csv-format",children:"5. Exporting data to CSV format"}),"\n",(0,a.jsx)(t.p,{children:"Since we want to save all search result data in a convenient tabular format like CSV, we can simply add the export_data method call right after running the crawler:"}),"\n",(0,a.jsx)(t.pre,{children:(0,a.jsx)(t.code,{className:"language-python",children:'await crawler.export_data_csv("google_search.csv")\n'})}),"\n",(0,a.jsx)(t.h3,{id:"6-finalizing-the-google-search-scraper",children:"6. Finalizing the Google Search scraper"}),"\n",(0,a.jsxs)(t.p,{children:["While our core crawler logic works, you might have noticed that our results currently lack ranking position information. To complete our scraper, we need to implement proper ranking position tracking by passing data between requests using ",(0,a.jsx)(t.code,{children:"user_data"})," in ",(0,a.jsx)(t.a,{href:"https://www.crawlee.dev/python/api/class/Request",children:(0,a.jsx)(t.code,{children:"Request"})}),"."]}),"\n",(0,a.jsxs)(t.p,{children:["Let's modify the script to handle multiple queries and track ranking positions for search results analysis. We'll also set the crawling depth as a top-level variable. Let's move the ",(0,a.jsx)(t.code,{children:"router.default_handler"})," to ",(0,a.jsx)(t.code,{children:"routes.py"})," to match the project structure:"]}),"\n",(0,a.jsx)(t.pre,{children:(0,a.jsx)(t.code,{className:"language-python",children:'# crawlee-google-search.main\n\nfrom crawlee.beautifulsoup_crawler import BeautifulSoupCrawler, BeautifulSoupCrawlingContext\nfrom crawlee.http_clients.curl_impersonate import CurlImpersonateHttpClient\nfrom crawlee import Request, ConcurrencySettings, HttpHeaders\n\nfrom .routes import router\n\nQUERIES = ["Apify", "Crawlee"]\n\nCRAWL_DEPTH = 2\n\n\nasync def main() -> None:\n    """The crawler entry point."""\n\n    concurrency_settings = ConcurrencySettings(max_concurrency=5, max_tasks_per_minute=200)\n\n    http_client = CurlImpersonateHttpClient(impersonate="chrome124",\n                                            headers=HttpHeaders({"referer": "https://www.google.com/",\n                                                     "accept-language": "en",\n                                                     "accept-encoding": "gzip, deflate, br, zstd",\n                                                     "user-agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/131.0.0.0 Safari/537.36"\n                                            }))\n    crawler = BeautifulSoupCrawler(\n        request_handler=router,\n        max_request_retries=1,\n        concurrency_settings=concurrency_settings,\n        http_client=http_client,\n        max_requests_per_crawl=100,\n        max_crawl_depth=CRAWL_DEPTH\n    )\n\n    requests_lists = [Request.from_url(f"https://www.google.com/search?q={query}", user_data = {"query": query}) for query in QUERIES]\n\n    await crawler.run(requests_lists)\n\n    await crawler.export_data_csv("google_ranked.csv")\n'})}),"\n",(0,a.jsxs)(t.p,{children:["Let's also modify the handler to add ",(0,a.jsx)(t.code,{children:"query"})," and ",(0,a.jsx)(t.code,{children:"order_no"})," fields and basic error handling:"]}),"\n",(0,a.jsx)(t.pre,{children:(0,a.jsx)(t.code,{className:"language-python",children:'# crawlee-google-search.routes\n\nfrom crawlee.beautifulsoup_crawler import BeautifulSoupCrawlingContext\nfrom crawlee.router import Router\n\nrouter = Router[BeautifulSoupCrawlingContext]()\n\n\n@router.default_handler\nasync def default_handler(context: BeautifulSoupCrawlingContext) -> None:\n    """Default request handler."""\n    context.log.info(f\'Processing {context.request.url} ...\')\n\n    order = context.request.user_data.get("last_order", 1)\n    query = context.request.user_data.get("query")\n    for item in context.soup.select("div#search div#rso div[data-hveid][lang]"):\n        try:\n            data = {\n                "query": query,\n                "order_no": order,\n                \'title\': item.select_one("h3").get_text(),\n                "url": item.select_one("a").get("href"),\n                "text_widget": item.select_one("div[style*=\'line\']").get_text(),\n            }\n            await context.push_data(data)\n            order += 1\n        except AttributeError as e:\n            context.log.warning(f\'Attribute error for query "{query}": {str(e)}\')\n        except Exception as e:\n            context.log.error(f\'Unexpected error for query "{query}": {str(e)}\')\n\n    await context.enqueue_links(selector="div[role=\'navigation\'] td[role=\'heading\']:last-of-type > a",\n                                user_data={"last_order": order, "query": query})\n'})}),"\n",(0,a.jsx)(t.p,{children:"And we're done!"}),"\n",(0,a.jsxs)(t.p,{children:["Our Google Search crawler is ready. Let's look at the results in the ",(0,a.jsx)(t.code,{children:"google_ranked.csv"})," file:"]}),"\n",(0,a.jsx)(t.p,{children:(0,a.jsx)(t.img,{alt:"Results CSV",src:n(87316).Z+"",width:"1319",height:"588"})}),"\n",(0,a.jsxs)(t.p,{children:["The code repository is available on ",(0,a.jsx)(t.a,{href:"https://github.com/Mantisus/crawlee-google-search",children:(0,a.jsx)(t.code,{children:"GitHub"})})]}),"\n",(0,a.jsx)(t.h2,{id:"scrape-google-search-results-with-apify",children:"Scrape Google Search results with Apify"}),"\n",(0,a.jsxs)(t.p,{children:["If you're working on a large-scale project requiring millions of data points, like the project featured in this ",(0,a.jsx)(t.a,{href:"https://backlinko.com/search-engine-ranking",children:"article about Google ranking analysis"})," - you might need a ready-made solution."]}),"\n",(0,a.jsxs)(t.p,{children:["Consider using ",(0,a.jsx)(t.a,{href:"https://www.apify.com/apify/google-search-scraper",children:(0,a.jsx)(t.code,{children:"Google Search Results Scraper"})})," by the Apify team."]}),"\n",(0,a.jsx)(t.p,{children:"It offers important features such as:"}),"\n",(0,a.jsxs)(t.ul,{children:["\n",(0,a.jsx)(t.li,{children:"Proxy support"}),"\n",(0,a.jsx)(t.li,{children:"Scalability for large-scale data extraction"}),"\n",(0,a.jsx)(t.li,{children:"Geolocation control"}),"\n",(0,a.jsxs)(t.li,{children:["Integration with external services like ",(0,a.jsx)(t.a,{href:"https://zapier.com/",children:(0,a.jsx)(t.code,{children:"Zapier"})}),", ",(0,a.jsx)(t.a,{href:"https://www.make.com/",children:(0,a.jsx)(t.code,{children:"Make"})}),", ",(0,a.jsx)(t.a,{href:"https://airbyte.com/",children:(0,a.jsx)(t.code,{children:"Airbyte"})}),", ",(0,a.jsx)(t.a,{href:"https://www.langchain.com/",children:(0,a.jsx)(t.code,{children:"LangChain"})})," and others"]}),"\n"]}),"\n",(0,a.jsxs)(t.p,{children:["You can learn more in the Apify ",(0,a.jsx)(t.a,{href:"https://blog.apify.com/unofficial-google-search-api-from-apify-22a20537a951/",children:"blog"})]}),"\n",(0,a.jsx)(t.h2,{id:"what-will-you-scrape",children:"What will you scrape?"}),"\n",(0,a.jsx)(t.p,{children:"In this blog, we've explored step-by-step how to create a Google Search crawler that collects ranking data. How you analyze this dataset is up to you!"}),"\n",(0,a.jsxs)(t.p,{children:["As a reminder, you can find the full project code on ",(0,a.jsx)(t.a,{href:"https://github.com/Mantisus/crawlee-google-search",children:(0,a.jsx)(t.code,{children:"GitHub"})}),"."]}),"\n",(0,a.jsx)(t.p,{children:"I'd like to think that in 5 years I'll need to write an article on \"How to extract data from the best search engine for LLMs\", but I suspect that in 5 years this article will still be relevant."})]})}function d(e={}){let{wrapper:t}={...(0,s.a)(),...e.components};return t?(0,a.jsx)(t,{...e,children:(0,a.jsx)(h,{...e})}):h(e)}},2086:function(e,t,n){n.d(t,{Z:function(){return r}});let r=n.p+"assets/images/google-search-a91bfdf17a4c2860798444b1be56f625.webp"},44095:function(e,t,n){n.d(t,{Z:function(){return r}});let r=n.p+"assets/images/check_html-e243b1a0eff6d4404b9034863969bedc.webp"},35635:function(e,t,n){n.d(t,{Z:function(){return r}});let r=n.p+"assets/images/google-search-a91bfdf17a4c2860798444b1be56f625.webp"},82976:function(e,t,n){n.d(t,{Z:function(){return r}});let r=n.p+"assets/images/html_example-ccefa4ed63c38812ac5b8ca7b5122c8c.webp"},87316:function(e,t,n){n.d(t,{Z:function(){return r}});let r=n.p+"assets/images/results-03c51354b4347837a24ec6977a442ce8.webp"},16607:function(e,t,n){n.d(t,{Z:function(){return r}});let r=n.p+"assets/images/search_example-53f4fdf556178b9478a8d4f3e3816669.webp"},50065:function(e,t,n){n.d(t,{Z:function(){return o},a:function(){return i}});var r=n(67294);let a={},s=r.createContext(a);function i(e){let t=r.useContext(s);return r.useMemo(function(){return"function"==typeof e?e(t):{...t,...e}},[t,e])}function o(e){let t;return t=e.disableParentContext?"function"==typeof e.components?e.components(a):e.components||a:i(e.components),r.createElement(s.Provider,{value:t},e.children)}},351:function(e){e.exports=JSON.parse('{"permalink":"/blog/scrape-google-search","source":"@site/blog/2024/12-02-scrape-google-search/index.md","title":"How to scrape Google search results with Python","description":"Learn how to scrape google search results using Crawlee for Python","date":"2024-12-02T00:00:00.000Z","tags":[{"inline":true,"label":"community","permalink":"/blog/tags/community"}],"readingTime":6.115,"hasTruncateMarker":true,"authors":[{"name":"Max","title":"Community Member of Crawlee and web scraping expert","url":"https://github.com/Mantisus","socials":{"github":"https://github.com/Mantisus"},"imageURL":"https://avatars.githubusercontent.com/u/34358312?v=4","key":"MaxB","page":null}],"frontMatter":{"slug":"scrape-google-search","title":"How to scrape Google search results with Python","tags":["community"],"description":"Learn how to scrape google search results using Crawlee for Python","image":"./img/google-search.webp","authors":["MaxB"]},"unlisted":false,"prevItem":{"title":"How to scrape Google Maps data using Python","permalink":"/blog/scrape-google-maps"},"nextItem":{"title":"Reverse engineering GraphQL persistedQuery extension","permalink":"/blog/graphql-persisted-query"}}')}}]);