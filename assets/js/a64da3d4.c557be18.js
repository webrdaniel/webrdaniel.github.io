"use strict";(self.webpackChunk=self.webpackChunk||[]).push([["78490"],{77795:function(e,r,n){n.r(r),n.d(r,{assets:function(){return c},contentTitle:function(){return o},default:function(){return p},frontMatter:function(){return i},metadata:function(){return s},toc:function(){return l}});var s=n(77970),t=n(85893),a=n(50065);let i={slug:"superscraper-with-crawlee",title:"Inside implementing SuperScraper with Crawlee",description:"This article explains how SuperScraper works, highlights its implementation details, and provides code snippets to demonstrate its core functionality.",image:"./img/superscraper.webp",authors:["SauravJ","RadoC"]},o=void 0,c={image:n(37263).Z,authorsImageUrls:[void 0,void 0]},l=[{value:"What is SuperScraper?",id:"what-is-superscraper",level:3},{value:"How to enable standby mode",id:"how-to-enable-standby-mode",level:3},{value:"Server setup",id:"server-setup",level:3},{value:"Handling multiple crawlers",id:"handling-multiple-crawlers",level:3},{value:"Mapping standby HTTP requests to Crawlee requests",id:"mapping-standby-http-requests-to-crawlee-requests",level:3},{value:"Managing migrations",id:"managing-migrations",level:3},{value:"Build your own",id:"build-your-own",level:3}];function d(e){let r={a:"a",code:"code",h3:"h3",img:"img",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",...(0,a.a)(),...e.components};return(0,t.jsxs)(t.Fragment,{children:[(0,t.jsxs)(r.p,{children:[(0,t.jsx)(r.a,{href:"https://github.com/apify/super-scraper",children:"SuperScraper"})," is an open-source ",(0,t.jsx)(r.a,{href:"https://docs.apify.com/platform/actors",children:"Actor"})," that combines features from various web scraping services, including ",(0,t.jsx)(r.a,{href:"https://www.scrapingbee.com/",children:"ScrapingBee"}),", ",(0,t.jsx)(r.a,{href:"https://scrapingant.com/",children:"ScrapingAnt"}),", and ",(0,t.jsx)(r.a,{href:"https://www.scraperapi.com/",children:"ScraperAPI"}),"."]}),"\n",(0,t.jsx)(r.p,{children:"A key capability is its standby mode, which runs the Actor as a persistent API server. This removes the usual start-up times - a common pain point in many systems - and lets users make direct API calls to interact with the system immediately."}),"\n",(0,t.jsx)(r.p,{children:"This blog explains how SuperScraper works, highlights its implementation details, and provides code snippets to demonstrate its core functionality."}),"\n",(0,t.jsx)(r.p,{children:(0,t.jsx)(r.img,{alt:"Google Maps Data Screenshot",src:n(19014).Z+"",width:"1152",height:"649"})}),"\n",(0,t.jsx)(r.h3,{id:"what-is-superscraper",children:"What is SuperScraper?"}),"\n",(0,t.jsx)(r.p,{children:"SuperScraper transforms a traditional scraper into an API server. Instead of running with static inputs and waiting for completion, it starts only once, stays active, and listens for incoming requests."}),"\n",(0,t.jsx)(r.h3,{id:"how-to-enable-standby-mode",children:"How to enable standby mode"}),"\n",(0,t.jsx)(r.p,{children:"To activate standby mode, you must configure the settings so it listens for incoming requests."}),"\n",(0,t.jsx)(r.p,{children:(0,t.jsx)(r.img,{alt:"Activating Actor standby mode",src:n(79773).Z+"",width:"2504",height:"984"})}),"\n",(0,t.jsx)(r.h3,{id:"server-setup",children:"Server setup"}),"\n",(0,t.jsxs)(r.p,{children:["The project uses Node.js ",(0,t.jsx)(r.code,{children:"http"})," module to create a server that listens on the desired port. After the server starts, a check ensures users are interacting with it correctly by sending requests instead of running it traditionally. This keeps SuperScraper operating as a persistent server."]}),"\n",(0,t.jsx)(r.h3,{id:"handling-multiple-crawlers",children:"Handling multiple crawlers"}),"\n",(0,t.jsxs)(r.p,{children:["SuperScraper processes user requests using multiple instances of Crawlee\u2019s ",(0,t.jsx)(r.a,{href:"https://crawlee.dev/api/playwright-crawler/class/PlaywrightCrawler",children:(0,t.jsx)(r.code,{children:"PlaywrightCrawler"})}),". Since each ",(0,t.jsx)(r.code,{children:"PlaywrightCrawler"})," instance can only handle one proxy configuration, a separate crawler is created for each unique proxy setting."]}),"\n",(0,t.jsx)(r.p,{children:"For example, if the user sends one request for \u201Cnormal\u201D proxies and one request with residential US proxies, a separate crawler needs to be created for each proxy configuration. Hence, to solve this, we store the crawlers in a key-value map, where the key is a stringified proxy configuration."}),"\n",(0,t.jsx)(r.pre,{children:(0,t.jsx)(r.code,{className:"language-ts",children:"const crawlers = new Map<string, PlaywrightCrawler>();\n"})}),"\n",(0,t.jsx)(r.p,{children:"Here\u2019s a part of the code that gets executed when a new request from the user arrives; if the crawler for this proxy configuration exists in the map, it will be used. Otherwise, a new crawler gets created. Then, we add the request to the crawler\u2019s queue so it can be processed."}),"\n",(0,t.jsx)(r.pre,{children:(0,t.jsx)(r.code,{className:"language-ts",children:"const key = JSON.stringify(crawlerOptions); \nconst crawler = crawlers.has(key) ? crawlers.get(key)! : await createAndStartCrawler(crawlerOptions);\n\nawait crawler.addRequests([request]);\n"})}),"\n",(0,t.jsxs)(r.p,{children:["The function below initializes new crawlers with predefined settings and behaviors. Each crawler utilizes its own in-memory queue created with the ",(0,t.jsx)(r.code,{children:"MemoryStorage"})," client. This approach is used for two key reasons:"]}),"\n",(0,t.jsxs)(r.ol,{children:["\n",(0,t.jsxs)(r.li,{children:[(0,t.jsx)(r.strong,{children:"Performance"}),": In-memory queues are faster, and there's no need to persist them when SuperScraper migrates."]}),"\n",(0,t.jsxs)(r.li,{children:[(0,t.jsx)(r.strong,{children:"Isolation"}),": Using a separate queue prevents interference with the shared default queue of the SuperScraper Actor, avoiding potential bugs when multiple crawlers use it simultaneously."]}),"\n"]}),"\n",(0,t.jsx)(r.pre,{children:(0,t.jsx)(r.code,{className:"language-ts",children:"export const createAndStartCrawler = async (crawlerOptions: CrawlerOptions = DEFAULT_CRAWLER_OPTIONS) => {\n    const client = new MemoryStorage({ persistStorage: false });\n    const queue = await RequestQueue.open(undefined, { storageClient: client });\n\n    const proxyConfig = await Actor.createProxyConfiguration(crawlerOptions.proxyConfigurationOptions);\n\n    const crawler = new PlaywrightCrawler({\n        keepAlive: true,\n        proxyConfiguration: proxyConfig,\n        maxRequestRetries: 4,\n        requestQueue: queue,\n    });\n};\n"})}),"\n",(0,t.jsx)(r.p,{children:"At the end of the function, we start the crawler and log a message if it terminates for any reason. Next, we add the newly created crawler to the key-value map containing all crawlers, and finally, we return the crawler."}),"\n",(0,t.jsx)(r.pre,{children:(0,t.jsx)(r.code,{className:"language-ts",children:"crawler.run().then(\n    () => log.warning(`Crawler ended`, crawlerOptions),\n    () => { }\n);\n\ncrawlers.set(JSON.stringify(crawlerOptions), crawler);\n\nlog.info('Crawler ready \uD83D\uDE80', crawlerOptions);\n\nreturn crawler;\n"})}),"\n",(0,t.jsx)(r.h3,{id:"mapping-standby-http-requests-to-crawlee-requests",children:"Mapping standby HTTP requests to Crawlee requests"}),"\n",(0,t.jsxs)(r.p,{children:["When creating the server, it accepts a request listener function that takes two arguments: the user\u2019s request and a response object. The response object is used to send scraped data back to the user. These response objects are stored in a key-value map to so they can be accessed later in the code. The key is a randomly generated string shared between the request and its corresponding response object, it is used as ",(0,t.jsx)(r.code,{children:"request.uniqueKey"}),"."]}),"\n",(0,t.jsx)(r.pre,{children:(0,t.jsx)(r.code,{className:"language-ts",children:"const responses = new Map<string, ServerResponse>();\n"})}),"\n",(0,t.jsx)(r.p,{children:(0,t.jsx)(r.strong,{children:"Saving response objects"})}),"\n",(0,t.jsx)(r.p,{children:"The following function stores a response object in the key-value map:"}),"\n",(0,t.jsx)(r.pre,{children:(0,t.jsx)(r.code,{className:"language-ts",children:"export function addResponse(responseId: string, response: ServerResponse) {\n    responses.set(responseId, response);\n}\n"})}),"\n",(0,t.jsx)(r.p,{children:(0,t.jsx)(r.strong,{children:"Updating crawler logic to store responses"})}),"\n",(0,t.jsx)(r.p,{children:"Here\u2019s the updated logic for fetching/creating the corresponding crawler for a given proxy configuration, with a call to store the response object:"}),"\n",(0,t.jsx)(r.pre,{children:(0,t.jsx)(r.code,{className:"language-ts",children:"const key = JSON.stringify(crawlerOptions); \nconst crawler = crawlers.has(key) ? crawlers.get(key)! : await createAndStartCrawler(crawlerOptions);\n\naddResponse(request.uniqueKey!, res);\n\nawait crawler.requestQueue!.addRequest(request);\n"})}),"\n",(0,t.jsx)(r.p,{children:(0,t.jsx)(r.strong,{children:"Sending scraped data back"})}),"\n",(0,t.jsx)(r.p,{children:"Once a crawler finishes processing a request, it retrieves the corresponding response object using the key and sends the scraped data back to the user:"}),"\n",(0,t.jsx)(r.pre,{children:(0,t.jsx)(r.code,{className:"language-ts",children:"export const sendSuccResponseById = (responseId: string, result: unknown, contentType: string) => {\n    const res = responses.get(responseId);\n    if (!res) {\n        log.info(`Response for request ${responseId} not found`);\n        return;\n    }\n\n    res.writeHead(200, { 'Content-Type': contentType });\n    res.end(result);\n    responses.delete(responseId);\n};\n"})}),"\n",(0,t.jsx)(r.p,{children:(0,t.jsx)(r.strong,{children:"Error handling"})}),"\n",(0,t.jsx)(r.p,{children:"There is similar logic to send a response back if an error occurs during scraping:"}),"\n",(0,t.jsx)(r.pre,{children:(0,t.jsx)(r.code,{className:"language-ts",children:"export const sendErrorResponseById = (responseId: string, result: string, statusCode: number = 500) => {\n    const res = responses.get(responseId);\n    if (!res) {\n        log.info(`Response for request ${responseId} not found`);\n        return;\n    }\n\n    res.writeHead(statusCode, { 'Content-Type': 'application/json' });\n    res.end(result);\n    responses.delete(responseId);\n};\n"})}),"\n",(0,t.jsx)(r.p,{children:(0,t.jsx)(r.strong,{children:"Adding timeouts during migrations"})}),"\n",(0,t.jsx)(r.p,{children:"During migration, SuperScraper adds timeouts to pending responses to handle termination cleanly."}),"\n",(0,t.jsx)(r.pre,{children:(0,t.jsx)(r.code,{className:"language-ts",children:"export const addTimeoutToAllResponses = (timeoutInSeconds: number = 60) => {\n    const migrationErrorMessage = {\n        errorMessage: 'Actor had to migrate to another server. Please, retry your request.',\n    };\n\n    const responseKeys = Object.keys(responses);\n\n    for (const key of responseKeys) {\n        setTimeout(() => {\n            sendErrorResponseById(key, JSON.stringify(migrationErrorMessage));\n        }, timeoutInSeconds * 1000);\n    }\n};\n"})}),"\n",(0,t.jsx)(r.h3,{id:"managing-migrations",children:"Managing migrations"}),"\n",(0,t.jsx)(r.p,{children:"SuperScraper handles migrations by timing out active responses to prevent lingering requests during server transitions."}),"\n",(0,t.jsx)(r.pre,{children:(0,t.jsx)(r.code,{className:"language-ts",children:"Actor.on('migrating', ()=>{\n    addTimeoutToAllResponses(60);\n});\n"})}),"\n",(0,t.jsx)(r.p,{children:"Users receive clear feedback during server migrations, maintaining stable operation."}),"\n",(0,t.jsx)(r.h3,{id:"build-your-own",children:"Build your own"}),"\n",(0,t.jsxs)(r.p,{children:["This guide showed how to build and manage a standby web scraper using Apify\u2019s platform and Crawlee. The implementation handles multiple proxy configurations through ",(0,t.jsx)(r.code,{children:"PlaywrightCrawler"})," instances while managing request-response cycles efficiently to support diverse scraping needs."]}),"\n",(0,t.jsx)(r.p,{children:"Standby mode transforms SuperScraper into a persistent API server, eliminating start-up delays. The migration handling system keeps operations stable during server transitions. You can build on this foundation to create web scraping tools tailored to your requirements."}),"\n",(0,t.jsxs)(r.p,{children:["To get started, explore the project on ",(0,t.jsx)(r.a,{href:"https://github.com/apify/super-scraper",children:"GitHub"})," or learn more about ",(0,t.jsx)(r.a,{href:"https://crawlee.dev/",children:"Crawlee"})," to build your own scalable web scraping tools."]})]})}function p(e={}){let{wrapper:r}={...(0,a.a)(),...e.components};return r?(0,t.jsx)(r,{...e,children:(0,t.jsx)(d,{...e})}):d(e)}},37263:function(e,r,n){n.d(r,{Z:function(){return s}});let s=n.p+"assets/images/superscraper-8d24da63227f97df70998e8900b3a901.webp"},79773:function(e,r,n){n.d(r,{Z:function(){return s}});let s=n.p+"assets/images/actor-standby-9b094dde2615b70afb82685d56c8d74e.webp"},19014:function(e,r,n){n.d(r,{Z:function(){return s}});let s=n.p+"assets/images/superscraper-8d24da63227f97df70998e8900b3a901.webp"},50065:function(e,r,n){n.d(r,{Z:function(){return o},a:function(){return i}});var s=n(67294);let t={},a=s.createContext(t);function i(e){let r=s.useContext(a);return s.useMemo(function(){return"function"==typeof e?e(r):{...r,...e}},[r,e])}function o(e){let r;return r=e.disableParentContext?"function"==typeof e.components?e.components(t):e.components||t:i(e.components),s.createElement(a.Provider,{value:r},e.children)}},77970:function(e){e.exports=JSON.parse('{"permalink":"/blog/superscraper-with-crawlee","source":"@site/blog/2025/03-05-superscraper/index.md","title":"Inside implementing SuperScraper with Crawlee","description":"This article explains how SuperScraper works, highlights its implementation details, and provides code snippets to demonstrate its core functionality.","date":"2025-03-05T00:00:00.000Z","tags":[],"readingTime":5.11,"hasTruncateMarker":true,"authors":[{"name":"Saurav Jain","title":"Developer Community Manager","url":"https://github.com/souravjain540","socials":{"x":"https://x.com/sauain","github":"https://github.com/souravjain540"},"imageURL":"https://avatars.githubusercontent.com/u/53312820?v=4","key":"SauravJ","page":null},{"name":"Radoslav Chudovsk\xfd","title":"Web Automation Engineer","url":"https://github.com/chudovskyr","socials":{"github":"https://github.com/chudovskyr"},"imageURL":"https://ca.slack-edge.com/T0KRMEKK6-U04MGU11VUK-7f59c4a9343b-512","key":"RadoC","page":null}],"frontMatter":{"slug":"superscraper-with-crawlee","title":"Inside implementing SuperScraper with Crawlee","description":"This article explains how SuperScraper works, highlights its implementation details, and provides code snippets to demonstrate its core functionality.","image":"./img/superscraper.webp","authors":["SauravJ","RadoC"]},"unlisted":false,"nextItem":{"title":"Crawlee for Python v0.5","permalink":"/blog/crawlee-for-python-v05"}}')}}]);