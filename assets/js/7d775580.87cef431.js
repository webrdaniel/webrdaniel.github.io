"use strict";(self.webpackChunk=self.webpackChunk||[]).push([["89929"],{20218:function(e,r,t){t.r(r),t.d(r,{default:()=>m,frontMatter:()=>u,metadata:()=>a,assets:()=>w,toc:()=>g,contentTitle:()=>p});var a=JSON.parse('{"id":"quick-start/quick-start","title":"Quick Start","description":"With this short tutorial you can start scraping with Crawlee in a minute or two. To learn more, read the Introduction.","source":"@site/versioned_docs/version-3.2/quick-start/index.mdx","sourceDirName":"quick-start","slug":"/quick-start/","permalink":"/docs/3.2/quick-start/","draft":false,"unlisted":false,"editUrl":"https://github.com/apify/crawlee/edit/master/website/versioned_docs/version-3.2/quick-start/index.mdx","tags":[],"version":"3.2","lastUpdatedBy":"Martin Ad\xe1mek","lastUpdatedAt":1675760121000,"frontMatter":{"id":"quick-start","title":"Quick Start","description":"With this short tutorial you can start scraping with Crawlee in a minute or two. To learn more, read the Introduction."},"sidebar":"docs","next":{"title":"Introduction","permalink":"/docs/3.2/introduction/"}}'),n=t("85893"),l=t("50065"),s=t("47927"),i=t("23349"),o=t("58168"),d=t("97645"),c=t("96199"),h=t("54198");let u={id:"quick-start",title:"Quick Start",description:"With this short tutorial you can start scraping with Crawlee in a minute or two. To learn more, read the Introduction."},p=void 0,w={},g=[{value:"Choose your crawler",id:"choose-your-crawler",level:2},{value:"CheerioCrawler",id:"cheeriocrawler",level:3},{value:"PuppeteerCrawler",id:"puppeteercrawler",level:3},{value:"PlaywrightCrawler",id:"playwrightcrawler",level:3},{value:"Installation with Crawlee CLI",id:"installation-with-crawlee-cli",level:2},{value:"Manual installation",id:"manual-installation",level:2},{value:"Crawling",id:"crawling",level:2},{value:"Running headful browsers",id:"running-headful-browsers",level:3},{value:"Results",id:"results",level:2},{value:"Examples and further reading",id:"examples-and-further-reading",level:2}];function f(e){let r={a:"a",admonition:"admonition",code:"code",h2:"h2",h3:"h3",li:"li",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,l.a)(),...e.components};return(0,n.jsxs)(n.Fragment,{children:[(0,n.jsxs)(r.p,{children:["With this short tutorial you can start scraping with Crawlee in a minute or two. To learn in-depth how Crawlee works, read the ",(0,n.jsx)(r.a,{href:"./introduction",children:"Introduction"}),", which is a comprehensive step-by-step guide for creating your first scraper."]}),"\n",(0,n.jsx)(r.h2,{id:"choose-your-crawler",children:"Choose your crawler"}),"\n",(0,n.jsxs)(r.p,{children:["Crawlee comes with three main crawler classes: ",(0,n.jsx)(s.Z,{to:"cheerio-crawler/class/CheerioCrawler",children:(0,n.jsx)(r.code,{children:"CheerioCrawler"})}),", ",(0,n.jsx)(s.Z,{to:"puppeteer-crawler/class/PuppeteerCrawler",children:(0,n.jsx)(r.code,{children:"PuppeteerCrawler"})})," and ",(0,n.jsx)(s.Z,{to:"playwright-crawler/class/PlaywrightCrawler",children:(0,n.jsx)(r.code,{children:"PlaywrightCrawler"})}),". All classes share the same interface for maximum flexibility when switching between them."]}),"\n",(0,n.jsx)(r.h3,{id:"cheeriocrawler",children:"CheerioCrawler"}),"\n",(0,n.jsxs)(r.p,{children:["This is a plain HTTP crawler. It parses HTML using the ",(0,n.jsx)(r.a,{href:"https://github.com/cheeriojs/cheerio",target:"_blank",rel:"noopener",children:"Cheerio"})," library and crawls the web using the specialized ",(0,n.jsx)(r.a,{href:"https://github.com/apify/got-scraping",target:"_blank",rel:"noopener",children:"got-scraping"})," HTTP client which masks as a browser. It's very fast and efficient, but can't handle JavaScript rendering."]}),"\n",(0,n.jsx)(r.h3,{id:"puppeteercrawler",children:"PuppeteerCrawler"}),"\n",(0,n.jsxs)(r.p,{children:["This crawler uses a headless browser to crawl, controlled by the ",(0,n.jsx)(r.a,{href:"https://github.com/puppeteer/puppeteer",target:"_blank",rel:"noopener",children:"Puppeteer"})," library. It can control Chromium or Chrome. Puppeteer is the de-facto standard in headless browser automation."]}),"\n",(0,n.jsx)(r.h3,{id:"playwrightcrawler",children:"PlaywrightCrawler"}),"\n",(0,n.jsxs)(r.p,{children:[(0,n.jsx)(r.a,{href:"https://github.com/microsoft/playwright",target:"_blank",rel:"noopener",children:"Playwright"})," is a more powerful and full-featured successor to Puppeteer. It can control Chromium, Chrome, Firefox, Webkit and many other browsers. If you're not familiar with Puppeteer already, and you need a headless browser, go with Playwright."]}),"\n",(0,n.jsx)(r.admonition,{title:"before you start",type:"caution",children:(0,n.jsxs)(r.p,{children:["Crawlee requires ",(0,n.jsx)(r.a,{href:"https://nodejs.org/en/",target:"_blank",rel:"noopener",children:"Node.js 16 or later"}),"."]})}),"\n",(0,n.jsx)(r.h2,{id:"installation-with-crawlee-cli",children:"Installation with Crawlee CLI"}),"\n",(0,n.jsxs)(r.p,{children:["The fastest way to try Crawlee out is to use the ",(0,n.jsx)(r.strong,{children:"Crawlee CLI"})," and choose the ",(0,n.jsx)(r.strong,{children:"Getting started example"}),".\nThe CLI will install all the necessary dependencies and add boilerplate code for you to play with."]}),"\n",(0,n.jsx)(r.pre,{children:(0,n.jsx)(r.code,{className:"language-bash",children:"npx crawlee create my-crawler\n"})}),"\n",(0,n.jsx)(r.p,{children:"After the installation is complete you can start the crawler like this:"}),"\n",(0,n.jsx)(r.pre,{children:(0,n.jsx)(r.code,{className:"language-bash",children:"cd my-crawler && npm start\n"})}),"\n",(0,n.jsx)(r.h2,{id:"manual-installation",children:"Manual installation"}),"\n",(0,n.jsx)(r.p,{children:"You can add Crawlee to any Node.js project by running:"}),"\n",(0,n.jsxs)(o.Z,{groupId:"quick_start",children:[(0,n.jsx)(d.Z,{value:"cheerio",label:"CheerioCrawler",default:!0,children:(0,n.jsx)(c.default,{language:"bash",children:"npm install crawlee"})}),(0,n.jsxs)(d.Z,{value:"playwright",label:"PlaywrightCrawler",children:[(0,n.jsx)(r.admonition,{type:"caution",children:(0,n.jsxs)(r.p,{children:[(0,n.jsx)(r.code,{children:"playwright"})," is not bundled with Crawlee to reduce install size and allow greater flexibility. You need to explicitly install it with NPM. \uD83D\uDC47"]})}),(0,n.jsx)(c.default,{language:"bash",children:"npm install crawlee playwright"})]}),(0,n.jsxs)(d.Z,{value:"puppeteer",label:"PuppeteerCrawler",children:[(0,n.jsx)(r.admonition,{type:"caution",children:(0,n.jsxs)(r.p,{children:[(0,n.jsx)(r.code,{children:"puppeteer"})," is not bundled with Crawlee to reduce install size and allow greater flexibility. You need to explicitly install it with NPM. \uD83D\uDC47"]})}),(0,n.jsx)(c.default,{language:"bash",children:"npm install crawlee puppeteer"})]})]}),"\n",(0,n.jsx)(r.h2,{id:"crawling",children:"Crawling"}),"\n",(0,n.jsx)(r.p,{children:"Run the following example to perform a recursive crawl of the Crawlee website using the selected crawler."}),"\n",(0,n.jsx)(i.Z,{type:"caution",title:"Don't forget about module imports",children:(0,n.jsxs)(r.p,{children:["To run the example, add a ",(0,n.jsx)("code",{children:'"type": "module"'})," clause into your ",(0,n.jsx)("code",{children:"package.json"})," or\ncopy it into a file with an ",(0,n.jsx)("code",{children:".mjs"})," suffix. This enables ",(0,n.jsx)("code",{children:"import"})," statements in Node.js.\nSee ",(0,n.jsx)("a",{href:"https://nodejs.org/dist/latest-v16.x/docs/api/esm.html#enabling",target:"_blank",rel:"noreferrer",children:"Node.js docs"})," for\nmore information."]})}),"\n",(0,n.jsxs)(o.Z,{groupId:"quick_start",children:[(0,n.jsx)(d.Z,{value:"cheerio",label:"CheerioCrawler",default:!0,children:(0,n.jsx)(c.default,{language:"js",children:"import { CheerioCrawler, Dataset } from 'crawlee';\n\n// CheerioCrawler crawls the web using HTTP requests\n// and parses HTML using the Cheerio library.\nconst crawler = new CheerioCrawler({\n    // Use the requestHandler to process each of the crawled pages.\n    async requestHandler({ request, $, enqueueLinks, log }) {\n        const title = $('title').text();\n        log.info(`Title of ${request.loadedUrl} is '${title}'`);\n\n        // Save results as JSON to ./storage/datasets/default\n        await Dataset.pushData({ title, url: request.loadedUrl });\n\n        // Extract links from the current page\n        // and add them to the crawling queue.\n        await enqueueLinks();\n    },\n});\n\n// Add first URL to the queue and start the crawl.\nawait crawler.run(['https://crawlee.dev']);\n"})}),(0,n.jsx)(d.Z,{value:"playwright",label:"PlaywrightCrawler",children:(0,n.jsx)(c.default,{language:"js",children:"import { PlaywrightCrawler, Dataset } from 'crawlee';\n\n// PlaywrightCrawler crawls the web using a headless\n// browser controlled by the Playwright library.\nconst crawler = new PlaywrightCrawler({\n    // Use the requestHandler to process each of the crawled pages.\n    async requestHandler({ request, page, enqueueLinks, log }) {\n        const title = await page.title();\n        log.info(`Title of ${request.loadedUrl} is '${title}'`);\n\n        // Save results as JSON to ./storage/datasets/default\n        await Dataset.pushData({ title, url: request.loadedUrl });\n\n        // Extract links from the current page\n        // and add them to the crawling queue.\n        await enqueueLinks();\n    },\n    // Uncomment this option to see the browser window.\n    // headless: false,\n});\n\n// Add first URL to the queue and start the crawl.\nawait crawler.run(['https://crawlee.dev']);\n"})}),(0,n.jsx)(d.Z,{value:"puppeteer",label:"PuppeteerCrawler",children:(0,n.jsx)(c.default,{language:"js",children:"import { PuppeteerCrawler, Dataset } from 'crawlee';\n\n// PuppeteerCrawler crawls the web using a headless\n// browser controlled by the Puppeteer library.\nconst crawler = new PuppeteerCrawler({\n    // Use the requestHandler to process each of the crawled pages.\n    async requestHandler({ request, page, enqueueLinks, log }) {\n        const title = await page.title();\n        log.info(`Title of ${request.loadedUrl} is '${title}'`);\n\n        // Save results as JSON to ./storage/datasets/default\n        await Dataset.pushData({ title, url: request.loadedUrl });\n\n        // Extract links from the current page\n        // and add them to the crawling queue.\n        await enqueueLinks();\n    },\n    // Uncomment this option to see the browser window.\n    // headless: false,\n});\n\n// Add first URL to the queue and start the crawl.\nawait crawler.run(['https://crawlee.dev']);\n"})})]}),"\n",(0,n.jsx)(r.p,{children:"When you run the example, you will see Crawlee automating the data extraction process in your terminal."}),"\n",(0,n.jsx)(c.default,{language:"log",children:"INFO  CheerioCrawler: Starting the crawl\nINFO  CheerioCrawler: Title of https://crawlee.dev/ is 'Crawlee \xb7 Build reliable crawlers. Fast. | Crawlee'\nINFO  CheerioCrawler: Title of https://crawlee.dev/docs/examples is 'Examples | Crawlee'\nINFO  CheerioCrawler: Title of https://crawlee.dev/docs/quick-start is 'Quick Start | Crawlee'\nINFO  CheerioCrawler: Title of https://crawlee.dev/docs/guides is 'Guides | Crawlee'\n"}),"\n",(0,n.jsx)(r.h3,{id:"running-headful-browsers",children:"Running headful browsers"}),"\n",(0,n.jsxs)(r.p,{children:["Browsers controlled by Puppeteer and Playwright run headless (without a visible window). You can switch to headful by adding the ",(0,n.jsx)(r.code,{children:"headless: false"})," option to the crawlers' constructor. This is useful in the development phase when you want to see what's going on in the browser."]}),"\n",(0,n.jsxs)(o.Z,{groupId:"quick_start",children:[(0,n.jsx)(d.Z,{value:"playwright",label:"PlaywrightCrawler",children:(0,n.jsx)(c.default,{language:"js",children:"import { PlaywrightCrawler, Dataset } from 'crawlee';\n\nconst crawler = new PlaywrightCrawler({\n    async requestHandler({ request, page, enqueueLinks, log }) {\n        const title = await page.title();\n        log.info(`Title of ${request.loadedUrl} is '${title}'`);\n        await Dataset.pushData({ title, url: request.loadedUrl });\n        await enqueueLinks();\n    },\n    // When you turn off headless mode, the crawler\n    // will run with a visible browser window.\n    headless: false,\n});\n\n// Add first URL to the queue and start the crawl.\nawait crawler.run(['https://crawlee.dev']);\n"})}),(0,n.jsx)(d.Z,{value:"puppeteer",label:"PuppeteerCrawler",children:(0,n.jsx)(c.default,{language:"js",children:"import { PuppeteerCrawler, Dataset } from 'crawlee';\n\nconst crawler = new PuppeteerCrawler({\n    async requestHandler({ request, page, enqueueLinks, log }) {\n        const title = await page.title();\n        log.info(`Title of ${request.loadedUrl} is '${title}'`);\n        await Dataset.pushData({ title, url: request.loadedUrl });\n        await enqueueLinks();\n    },\n    // When you turn off headless mode, the crawler\n    // will run with a visible browser window.\n    headless: false,\n});\n\n// Add first URL to the queue and start the crawl.\nawait crawler.run(['https://crawlee.dev']);\n"})})]}),"\n",(0,n.jsx)(r.p,{children:"When you run the example code, you'll see an automated browser blaze through the Crawlee website."}),"\n",(0,n.jsx)(r.admonition,{type:"note",children:(0,n.jsx)(r.p,{children:"For the sake of this show off, we've slowed down the crawler, but rest assured, it's blazing fast in real world usage."})}),"\n",(0,n.jsx)(h.Z,{alt:"An image showing off Crawlee scraping the Crawlee website using Puppeteer/Playwright and Chromium",sources:{light:"/img/chrome-scrape-light.gif",dark:"/img/chrome-scrape-dark.gif"}}),"\n",(0,n.jsx)(r.h2,{id:"results",children:"Results"}),"\n",(0,n.jsxs)(r.p,{children:["Crawlee stores data to the ",(0,n.jsx)(r.code,{children:"./storage"})," directory in your current working directory. The results of your crawl will be available under ",(0,n.jsx)(r.code,{children:"./storage/datasets/default/*.json"})," as JSON files."]}),"\n",(0,n.jsx)(r.pre,{children:(0,n.jsx)(r.code,{className:"language-json",metastring:'title="./storage/datasets/default/000000001.json"',children:'{\n    "url": "https://crawlee.dev/",\n    "title": "Crawlee \xb7 The scalable web crawling, scraping and automation library for JavaScript/Node.js | Crawlee"\n}\n'})}),"\n",(0,n.jsx)(r.admonition,{type:"tip",children:(0,n.jsxs)(r.p,{children:["You can override the storage directory by setting the ",(0,n.jsx)(r.code,{children:"CRAWLEE_STORAGE_DIR"})," environment variable."]})}),"\n",(0,n.jsx)(r.h2,{id:"examples-and-further-reading",children:"Examples and further reading"}),"\n",(0,n.jsxs)(r.p,{children:["You can find more examples showcasing various features of Crawlee in the ",(0,n.jsx)(r.a,{href:"./examples",children:"Examples"})," section of the documentation. To better understand Crawlee and its components you should read the ",(0,n.jsx)(r.a,{href:"./introduction",children:"Introduction"})," step-by-step guide."]}),"\n",(0,n.jsx)(r.p,{children:(0,n.jsx)(r.strong,{children:"Related links"})}),"\n",(0,n.jsxs)(r.ul,{children:["\n",(0,n.jsx)(r.li,{children:(0,n.jsx)(r.a,{href:"./guides/configuration",children:"Configuration"})}),"\n",(0,n.jsx)(r.li,{children:(0,n.jsx)(r.a,{href:"./guides/request-storage",children:"Request storage"})}),"\n",(0,n.jsx)(r.li,{children:(0,n.jsx)(r.a,{href:"./guides/result-storage",children:"Result storage"})}),"\n"]})]})}function m(e={}){let{wrapper:r}={...(0,l.a)(),...e.components};return r?(0,n.jsx)(r,{...e,children:(0,n.jsx)(f,{...e})}):f(e)}},47927:function(e,r,t){t.d(r,{Z:function(){return c}});var a=t(85893);t(67294);var n=t(53367),l=t(89873),s=t(87262);let[i,o]=t(99074).version.split("."),d=[i,o].join("."),c=e=>{let{to:r,children:t}=e,i=(0,l.E)(),{siteConfig:o}=(0,s.default)();return o.presets[0][1].docs.disableVersioning||i.version===d?(0,a.jsx)(n.default,{to:`/api/${r}`,children:t}):(0,a.jsx)(n.default,{to:`/api/${"current"===i.version?"next":i.version}/${r}`,children:t})}},99074:function(e){e.exports=JSON.parse('{"name":"crawlee","version":"3.13.0","description":"The scalable web crawling and scraping library for JavaScript/Node.js. Enables development of data extraction and web automation jobs (not only) with headless Chrome and Puppeteer.","engines":{"node":">=16.0.0"},"bin":"./src/cli.ts","main":"./dist/index.js","module":"./dist/index.mjs","types":"./dist/index.d.ts","exports":{".":{"import":"./dist/index.mjs","require":"./dist/index.js","types":"./dist/index.d.ts"},"./package.json":"./package.json"},"keywords":["apify","headless","chrome","puppeteer","crawler","scraper"],"author":{"name":"Apify","email":"support@apify.com","url":"https://apify.com"},"contributors":["Jan Curn <jan@apify.com>","Marek Trunkat <marek@apify.com>","Ondra Urban <ondra@apify.com>"],"license":"Apache-2.0","repository":{"type":"git","url":"git+https://github.com/apify/crawlee"},"bugs":{"url":"https://github.com/apify/crawlee/issues"},"homepage":"https://crawlee.dev","scripts":{"build":"yarn clean && yarn compile && yarn copy","clean":"rimraf ./dist","compile":"tsc -p tsconfig.build.json && gen-esm-wrapper ./dist/index.js ./dist/index.mjs","copy":"tsx ../../scripts/copy.ts"},"publishConfig":{"access":"public"},"dependencies":{"@crawlee/basic":"3.13.0","@crawlee/browser":"3.13.0","@crawlee/browser-pool":"3.13.0","@crawlee/cheerio":"3.13.0","@crawlee/cli":"3.13.0","@crawlee/core":"3.13.0","@crawlee/http":"3.13.0","@crawlee/jsdom":"3.13.0","@crawlee/linkedom":"3.13.0","@crawlee/playwright":"3.13.0","@crawlee/puppeteer":"3.13.0","@crawlee/utils":"3.13.0","import-local":"^3.1.0","tslib":"^2.4.0"},"peerDependencies":{"playwright":"*","puppeteer":"*"},"peerDependenciesMeta":{"playwright":{"optional":true},"puppeteer":{"optional":true}}}')}}]);