"use strict";(self.webpackChunk=self.webpackChunk||[]).push([["84702"],{61418:function(e,t,n){n.r(t),n.d(t,{default:()=>m,frontMatter:()=>c,metadata:()=>r,assets:()=>d,toc:()=>p,contentTitle:()=>l});var r=JSON.parse('{"id":"guides/custom-http-client/custom-http-client","title":"Using a custom HTTP client (Experimental)","description":"Use a custom HTTP client for `sendRequest` and plain-HTTP crawling","source":"@site/../docs/guides/custom-http-client/custom-http-client.mdx","sourceDirName":"guides/custom-http-client","slug":"/guides/custom-http-client/","permalink":"/docs/next/guides/custom-http-client/","draft":false,"unlisted":false,"editUrl":"https://github.com/apify/crawlee/edit/master/website/../docs/guides/custom-http-client/custom-http-client.mdx","tags":[],"version":"current","lastUpdatedBy":"Jan Buchar","lastUpdatedAt":1729682043000,"frontMatter":{"id":"custom-http-client","title":"Using a custom HTTP client (Experimental)","description":"Use a custom HTTP client for `sendRequest` and plain-HTTP crawling"},"sidebar":"docs","previous":{"title":"Parallel Scraping","permalink":"/docs/next/guides/parallel-scraping/"},"next":{"title":"Deployment","permalink":"/docs/next/deployment"}}'),s=n("85893"),a=n("50065"),o=n("47927"),i=n("96199");let c={id:"custom-http-client",title:"Using a custom HTTP client (Experimental)",description:"Use a custom HTTP client for `sendRequest` and plain-HTTP crawling"},l=void 0,d={},p=[];function u(e){let t={code:"code",p:"p",...(0,a.a)(),...e.components};return(0,s.jsxs)(s.Fragment,{children:[(0,s.jsxs)(t.p,{children:["The ",(0,s.jsx)(o.Z,{to:"basic-crawler/class/BasicCrawler",children:(0,s.jsx)(t.code,{children:"BasicCrawler"})})," class allows you to configure the HTTP client implementation using the ",(0,s.jsx)(t.code,{children:"httpClient"})," constructor option. This might be useful for testing or if you need to swap out the default implementation based on ",(0,s.jsx)(t.code,{children:"got-scraping"})," for something else, such as ",(0,s.jsx)(t.code,{children:"curl-impersonate"})," or ",(0,s.jsx)(t.code,{children:"axios"}),"."]}),"\n",(0,s.jsxs)(t.p,{children:["The HTTP client implementation needs to conform to the ",(0,s.jsx)(o.Z,{to:"core/interface/BaseHttpClient",children:(0,s.jsx)(t.code,{children:"BaseHttpClient"})})," interface. For a rough idea on how it might look, see a skeleton implementation that uses the standard ",(0,s.jsx)(t.code,{children:"fetch"})," interface:"]}),"\n",(0,s.jsx)(i.default,{language:"ts",children:"import {\n    BaseHttpClient,\n    HttpRequest,\n    HttpResponse,\n    RedirectHandler,\n    ResponseTypes,\n    StreamingHttpResponse,\n} from '@crawlee/core';\nimport { Readable } from 'node:stream';\n\nclass CustomHttpClient implements BaseHttpClient {\n    async sendRequest<TResponseType extends keyof ResponseTypes = 'text'>(\n        request: HttpRequest<TResponseType>,\n    ): Promise<HttpResponse<TResponseType>> {\n        const requestHeaders = new Headers();\n        for (let [headerName, headerValues] of Object.entries(request.headers ?? {})) {\n            if (headerValues === undefined) {\n                continue;\n            }\n\n            if (!Array.isArray(headerValues)) {\n                headerValues = [headerValues];\n            }\n\n            for (const value of headerValues) {\n                requestHeaders.append(headerName, value);\n            }\n        }\n\n        const response = await fetch(request.url, {\n            method: request.method,\n            headers: requestHeaders,\n            body: request.body as string, // TODO implement stream/generator handling\n            signal: request.signal,\n            // TODO implement the rest of request parameters (e.g., timeout, proxyUrl, cookieJar, ...)\n        });\n\n        const headers: Record<string, string> = {};\n\n        response.headers.forEach((value, headerName) => {\n            headers[headerName] = value;\n        });\n\n        return {\n            complete: true,\n            request,\n            url: response.url,\n            statusCode: response.status,\n            redirectUrls: [], // TODO you need to handle redirects manually to track them\n            headers,\n            trailers: {}, // TODO not supported by fetch\n            ip: undefined,\n            body:\n                request.responseType === 'text'\n                    ? await response.text()\n                    : request.responseType === 'json'\n                      ? await response.json()\n                      : Buffer.from(await response.text()),\n        };\n    }\n\n    async stream(request: HttpRequest, onRedirect?: RedirectHandler): Promise<StreamingHttpResponse> {\n        const fetchResponse = await fetch(request.url, {\n            method: request.method,\n            headers: new Headers(),\n            body: request.body as string, // TODO implement stream/generator handling\n            signal: request.signal,\n            // TODO implement the rest of request parameters (e.g., timeout, proxyUrl, cookieJar, ...)\n        });\n\n        const headers: Record<string, string> = {}; // TODO same as in sendRequest()\n\n        async function* read() {\n            const reader = fetchResponse.body?.getReader();\n\n            const stream = new ReadableStream({\n                start(controller) {\n                    if (!reader) {\n                        return null;\n                    }\n                    return pump();\n                    function pump() {\n                        return reader!.read().then(({ done, value }) => {\n                            // When no more data needs to be consumed, close the stream\n                            if (done) {\n                                controller.close();\n                                return;\n                            }\n                            // Enqueue the next data chunk into our target stream\n                            controller.enqueue(value);\n                            return pump();\n                        });\n                    }\n                },\n            });\n\n            for await (const chunk of stream) {\n                yield chunk;\n            }\n        }\n\n        const response = {\n            complete: false,\n            request,\n            url: fetchResponse.url,\n            statusCode: fetchResponse.status,\n            redirectUrls: [], // TODO you need to handle redirects manually to track them\n            headers,\n            trailers: {}, // TODO not supported by fetch\n            ip: undefined,\n            stream: Readable.from(read()),\n            get downloadProgress() {\n                return { percent: 0, transferred: 0 }; // TODO track this\n            },\n            get uploadProgress() {\n                return { percent: 0, transferred: 0 }; // TODO track this\n            },\n        };\n\n        return response;\n    }\n}\n"}),"\n",(0,s.jsx)(t.p,{children:"You may then instantiate it and pass to a crawler constructor:"}),"\n",(0,s.jsx)(i.default,{language:"ts",children:"const crawler = new HttpCrawler({\n    httpClient: new CustomHttpClient(),\n    async requestHandler() {\n        /* ... */\n    },\n});\n"}),"\n",(0,s.jsx)(t.p,{children:"Please note that the interface is experimental and it will likely change with Crawlee version 4."})]})}function m(e={}){let{wrapper:t}={...(0,a.a)(),...e.components};return t?(0,s.jsx)(t,{...e,children:(0,s.jsx)(u,{...e})}):u(e)}},47927:function(e,t,n){n.d(t,{Z:function(){return d}});var r=n(85893);n(67294);var s=n(53367),a=n(89873),o=n(87262);let[i,c]=n(99074).version.split("."),l=[i,c].join("."),d=e=>{let{to:t,children:n}=e,i=(0,a.E)(),{siteConfig:c}=(0,o.default)();return c.presets[0][1].docs.disableVersioning||i.version===l?(0,r.jsx)(s.default,{to:`/api/${t}`,children:n}):(0,r.jsx)(s.default,{to:`/api/${"current"===i.version?"next":i.version}/${t}`,children:n})}},50065:function(e,t,n){n.d(t,{Z:function(){return i},a:function(){return o}});var r=n(67294);let s={},a=r.createContext(s);function o(e){let t=r.useContext(a);return r.useMemo(function(){return"function"==typeof e?e(t):{...t,...e}},[t,e])}function i(e){let t;return t=e.disableParentContext?"function"==typeof e.components?e.components(s):e.components||s:o(e.components),r.createElement(a.Provider,{value:t},e.children)}},99074:function(e){e.exports=JSON.parse('{"name":"crawlee","version":"3.13.0","description":"The scalable web crawling and scraping library for JavaScript/Node.js. Enables development of data extraction and web automation jobs (not only) with headless Chrome and Puppeteer.","engines":{"node":">=16.0.0"},"bin":"./src/cli.ts","main":"./dist/index.js","module":"./dist/index.mjs","types":"./dist/index.d.ts","exports":{".":{"import":"./dist/index.mjs","require":"./dist/index.js","types":"./dist/index.d.ts"},"./package.json":"./package.json"},"keywords":["apify","headless","chrome","puppeteer","crawler","scraper"],"author":{"name":"Apify","email":"support@apify.com","url":"https://apify.com"},"contributors":["Jan Curn <jan@apify.com>","Marek Trunkat <marek@apify.com>","Ondra Urban <ondra@apify.com>"],"license":"Apache-2.0","repository":{"type":"git","url":"git+https://github.com/apify/crawlee"},"bugs":{"url":"https://github.com/apify/crawlee/issues"},"homepage":"https://crawlee.dev","scripts":{"build":"yarn clean && yarn compile && yarn copy","clean":"rimraf ./dist","compile":"tsc -p tsconfig.build.json && gen-esm-wrapper ./dist/index.js ./dist/index.mjs","copy":"tsx ../../scripts/copy.ts"},"publishConfig":{"access":"public"},"dependencies":{"@crawlee/basic":"3.13.0","@crawlee/browser":"3.13.0","@crawlee/browser-pool":"3.13.0","@crawlee/cheerio":"3.13.0","@crawlee/cli":"3.13.0","@crawlee/core":"3.13.0","@crawlee/http":"3.13.0","@crawlee/jsdom":"3.13.0","@crawlee/linkedom":"3.13.0","@crawlee/playwright":"3.13.0","@crawlee/puppeteer":"3.13.0","@crawlee/utils":"3.13.0","import-local":"^3.1.0","tslib":"^2.4.0"},"peerDependencies":{"playwright":"*","puppeteer":"*"},"peerDependenciesMeta":{"playwright":{"optional":true},"puppeteer":{"optional":true}}}')}}]);